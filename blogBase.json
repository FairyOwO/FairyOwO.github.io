{"singlePage": [], "startSite": "11/22/2024", "filingNum": "", "onePageListNum": 15, "commentLabelColor": "#006b75", "yearColorList": ["#bc4c00", "#0969da", "#1f883d", "#A333D0"], "i18n": "CN", "themeMode": "manual", "dayTheme": "light", "nightTheme": "dark_colorblind", "urlMode": "pinyin", "script": "", "style": "", "head": "", "indexScript": "", "indexStyle": "", "bottomText": "\u8f6c\u8f7d\u65e0\u9700\u6ce8\u660e\u51fa\u5904", "showPostSource": 1, "iconList": {}, "UTC": 8, "rssSplit": "sentence", "exlink": {}, "needComment": 1, "allHead": "", "title": "FairyOwO \u7684 Blog", "subTitle": "\u8bb0\u5f55\u4e00\u4e9b\u7410\u4e8b", "avatarUrl": "https://github.githubassets.com/favicons/favicon.svg", "GMEEK_VERSION": "last", "email": "740614599@qq.com", "postListJson": {"P1": {"htmlDir": "docs/post/Test.html", "labels": ["blog"], "postTitle": "Test", "postUrl": "post/Test.html", "postSourceUrl": "https://github.com/FairyOwO/FairyOwO.github.io/issues/1", "commentNum": 2, "wordCount": 24, "description": "\u8fd9\u662f\u4e00\u4e2a\u6d4b\u8bd5!\r\nThis is a test!\u3002", "top": 0, "createdAt": 1732245994, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2024-11-22", "dateLabelColor": "#bc4c00"}, "P2": {"htmlDir": "docs/post/dai-yue-du.html", "labels": ["idea"], "postTitle": "\u5f85\u9605\u8bfb", "postUrl": "post/dai-yue-du.html", "postSourceUrl": "https://github.com/FairyOwO/FairyOwO.github.io/issues/2", "commentNum": 0, "wordCount": 315, "description": "## \u5bf9\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u7cfb\u7edf\u6027\u7684\u5b66\u4e60\r\n> \u6b64\u524d\u56e0\u4e3a\u5bf9\u4e00\u4e9b\u5f3a\u5316\u5b66\u4e60\u9879\u76ee\u611f\u5174\u8da3, \u800c\u8349\u8349\u5b66\u4e60\u4e86\u4e00\u90e8\u5206(\u611f\u8c22 ai, \u89e3\u91ca\u4e86\u5927\u90e8\u5206\u5185\u5bb9), \u8fd9\u91cc\u51c6\u5907\u7cfb\u7edf\u6027\u5b66\u4e60\u4e00\u4efd\r\n1. [Reinforcement Learning: An Introduction](https://rl.qiwihui.com/zh-cn/latest/) \u5f3a\u5316\u5b66\u4e60\u5bfc\u8bba\r\n2. [Openai Spinning Up](https://spinningup.qiwihui.com/zh-cn/latest/) OpenAI \u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\r\n\r\n## \u77e5\u4e4e \u5927\u6a21\u578b\u76f8\u5173\r\n\r\n## \u82cf\u5251\u6797\u535a\u5ba2\r\n\r\n## \u56fe\u50cf\u751f\u6210, llm\u5bf9prompt\u4f18\u5316\r\n> demo\u9700\u8981\u3002", "top": 0, "createdAt": 1732265067, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2024-11-22", "dateLabelColor": "#bc4c00"}, "P3": {"htmlDir": "docs/post/shi-yong- k3s -da-jian- k8s -ji-qun-(-shi-yong-guo-nei-jing-xiang-).html", "labels": ["blog"], "postTitle": "\u4f7f\u7528 k3s \u642d\u5efa k8s \u96c6\u7fa4(\u4f7f\u7528\u56fd\u5185\u955c\u50cf)", "postUrl": "post/shi-yong-%20k3s%20-da-jian-%20k8s%20-ji-qun-%28-shi-yong-guo-nei-jing-xiang-%29.html", "postSourceUrl": "https://github.com/FairyOwO/FairyOwO.github.io/issues/3", "commentNum": 0, "wordCount": 8564, "description": "> \u5386\u53f2\u6587\u7ae0\u642c\u8fd0\r\n\r\n> \u6ce8: \u6b64\u5904k3s\u4e3a\u5177\u4f53\u7684 Kubernetes \u53d1\u884c\u7248, \u540e\u9762\u7684k8s\u4e3a Kubernetes \u7684\u7f29\u5199, Kubernetes \u662f\u5f00\u6e90\u5bb9\u5668\u7f16\u6392\u5e73\u53f0\r\n\r\n\u9700\u6c42\u4e4b\u521d\u662f\u60f3\u5bf9\u5e74\u629b\u673a, \u6708\u629b\u673a\u8fdb\u884c\u7edf\u4e00\u7684\u7ba1\u7406, \u65b9\u4fbf\u90e8\u7f72\u76f8\u5173\u955c\u50cf, \u7c7b\u4f3c\u4e8e\u53f2\u83b1\u59c6\u7684\u7ed3\u6784(\u62ff\u5230\u65b0\u7684\u673a\u5668, \u52a0\u5165\u96c6\u7fa4, \u673a\u5668\u65f6\u95f4\u8fc7\u671f, \u81ea\u52a8\u79bb\u7ebf, \u4f38\u7f29\u91cd\u542f\u5206\u914d\u5168\u7531\u96c6\u7fa4\u672c\u8eab\u7ba1\u7406)\r\n\r\n\u4f7f\u7528\u7cfb\u7edf\u4e3a Debian\r\n\r\n## \u670d\u52a1\u5668\u642d\u5efa\r\n\r\n### \u642d\u5efa\u96c6\u7fa4\r\n\r\n\u4e3b server sh\u811a\u672c\r\n<details><summary>Details</summary>\r\n<p>\r\n\r\n```sh\r\necho 'deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware\r\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware\r\n\r\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware\r\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware\r\n\r\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware\r\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware\r\n\r\n# \u4ee5\u4e0b\u5b89\u5168\u66f4\u65b0\u8f6f\u4ef6\u6e90\u5305\u542b\u4e86\u5b98\u65b9\u6e90\u4e0e\u955c\u50cf\u7ad9\u914d\u7f6e\uff0c\u5982\u6709\u9700\u8981\u53ef\u81ea\u884c\u4fee\u6539\u6ce8\u91ca\u5207\u6362\r\ndeb https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware\r\ndeb-src https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware' > /etc/apt/sources.list\r\n\r\napt update\r\n\r\ncurl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \\\r\n  INSTALL_K3S_MIRROR=cn \\\r\n  sh -s - server \\\r\n  --cluster-init \\\r\n  --system-default-registry=registry.cn-hangzhou.aliyuncs.com\r\n\r\ncat /var/lib/rancher/k3s/server/token\r\n\r\ncat >> /etc/rancher/k3s/registries.yaml << EOF\r\nmirrors:\r\n  docker.io:\r\n    endpoint:\r\n      - 'https://dockerproxy.net'\r\n      - 'https://registry.cn-hangzhou.aliyuncs.com/'\r\n      - 'https://mirror.ccs.tencentyun.com'\r\n  k8s.gcr.io:\r\n    endpoint:\r\n      - 'https://k8s.dockerproxy.net'\r\n      - 'https://registry.aliyuncs.com/google_containers'\r\n  ghcr.io:\r\n    endpoint:\r\n      - 'https://ghcr.dockerproxy.net'\r\n      - 'https://ghcr.m.daocloud.io/'\r\n  gcr.io:\r\n    endpoint:\r\n      - 'https://gcr.dockerproxy.net'\r\n      - 'https://gcr.m.daocloud.io/'\r\n  quay.io:\r\n    endpoint:\r\n      - 'https://quay.dockerproxy.net'\r\n      - 'https://quay.tencentcloudcr.com/'\r\n  registry.k8s.io:\r\n    endpoint:\r\n      - 'https://k8s.dockerproxy.net'\r\n      - 'https://registry.aliyuncs.com/v2/google_containers'\r\nEOF\r\nsystemctl restart k3s\r\n```\r\n\r\n</p>\r\n</details> \r\n\r\n\u526f server sh\u811a\u672c\r\n\r\n<details><summary>Details</summary>\r\n<p>\r\n\r\n```sh\r\necho 'deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware\r\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware\r\n\r\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware\r\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware\r\n\r\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware\r\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware\r\n\r\n# \u4ee5\u4e0b\u5b89\u5168\u66f4\u65b0\u8f6f\u4ef6\u6e90\u5305\u542b\u4e86\u5b98\u65b9\u6e90\u4e0e\u955c\u50cf\u7ad9\u914d\u7f6e\uff0c\u5982\u6709\u9700\u8981\u53ef\u81ea\u884c\u4fee\u6539\u6ce8\u91ca\u5207\u6362\r\ndeb https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware\r\ndeb-src https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware' > /etc/apt/sources.list\r\n\r\napt update\r\n\r\ncurl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \\\r\n  INSTALL_K3S_MIRROR=cn \\\r\n  sh -s - server \\\r\n  --cluster-init \\\r\n  --system-default-registry=registry.cn-hangzhou.aliyuncs.com\r\n\r\ncat /var/lib/rancher/k3s/server/token\r\n\r\ncat >> /etc/rancher/k3s/registries.yaml << EOF\r\nmirrors:\r\n  docker.io:\r\n    endpoint:\r\n      - 'https://dockerproxy.net'\r\n      - 'https://registry.cn-hangzhou.aliyuncs.com/'\r\n      - 'https://mirror.ccs.tencentyun.com'\r\n  k8s.gcr.io:\r\n    endpoint:\r\n      - 'https://k8s.dockerproxy.net'\r\n      - 'https://registry.aliyuncs.com/google_containers'\r\n  ghcr.io:\r\n    endpoint:\r\n      - 'https://ghcr.dockerproxy.net'\r\n      - 'https://ghcr.m.daocloud.io/'\r\n  gcr.io:\r\n    endpoint:\r\n      - 'https://gcr.dockerproxy.net'\r\n      - 'https://gcr.m.daocloud.io/'\r\n  quay.io:\r\n    endpoint:\r\n      - 'https://quay.dockerproxy.net'\r\n      - 'https://quay.tencentcloudcr.com/'\r\n  registry.k8s.io:\r\n    endpoint:\r\n      - 'https://k8s.dockerproxy.net'\r\n      - 'https://registry.aliyuncs.com/v2/google_containers'\r\nEOF\r\nsystemctl restart k3s\r\n```\r\n\r\n</p>\r\n</details> \r\n\r\nclient sh\u811a\u672c\r\n\r\n<details><summary>Details</summary>\r\n<p>\r\n\r\n```sh\r\necho 'deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware\r\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware\r\n\r\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware\r\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware\r\n\r\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware\r\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware\r\n\r\n# \u4ee5\u4e0b\u5b89\u5168\u66f4\u65b0\u8f6f\u4ef6\u6e90\u5305\u542b\u4e86\u5b98\u65b9\u6e90\u4e0e\u955c\u50cf\u7ad9\u914d\u7f6e\uff0c\u5982\u6709\u9700\u8981\u53ef\u81ea\u884c\u4fee\u6539\u6ce8\u91ca\u5207\u6362\r\ndeb https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware\r\ndeb-src https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware' > /etc/apt/sources.list\r\n\r\napt update\r\n\r\ncurl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \\\r\n  INSTALL_K3S_MIRROR=cn \\\r\n  K3S_URL=https://ip:6443 \\\r\n  K3S_TOKEN=your_token \\\r\n  sh -\r\n\r\nmkdir -p /etc/rancher/k3s\r\ncat >> /etc/rancher/k3s/registries.yaml << EOF\r\nmirrors:\r\n  docker.io:\r\n    endpoint:\r\n      - 'https://dockerproxy.net'\r\n      - 'https://registry.cn-hangzhou.aliyuncs.com/'\r\n      - 'https://mirror.ccs.tencentyun.com'\r\n  k8s.gcr.io:\r\n    endpoint:\r\n      - 'https://k8s.dockerproxy.net'\r\n      - 'https://registry.aliyuncs.com/google_containers'\r\n  ghcr.io:\r\n    endpoint:\r\n      - 'https://ghcr.dockerproxy.net'\r\n      - 'https://ghcr.m.daocloud.io/'\r\n  gcr.io:\r\n    endpoint:\r\n      - 'https://gcr.dockerproxy.net'\r\n      - 'https://gcr.m.daocloud.io/'\r\n  quay.io:\r\n    endpoint:\r\n      - 'https://quay.dockerproxy.net'\r\n      - 'https://quay.tencentcloudcr.com/'\r\n  registry.k8s.io:\r\n    endpoint:\r\n      - 'https://k8s.dockerproxy.net'\r\n      - 'https://registry.aliyuncs.com/v2/google_containers'\r\nEOF\r\nsystemctl restart k3s-agent\r\n```\r\n\r\n</p>\r\n</details> \r\n\r\n> \u6ce8: k3s \u642d\u5efa\u96c6\u7fa4\u7684\u65b9\u6848\u9700\u8981\u4fdd\u8bc1\u4e3b\u670d\u52a1\u5668\u4e0d\u79bb\u7ebf, \u5426\u5219\u6574\u4e2a\u96c6\u7fa4\u4f1a\u79bb\u7ebf, \u8003\u8651\u5230k3s\u5360\u7528\u4f4e, \u673a\u5668\u4e00\u822c\u662f\u6027\u80fd\u4e0d\u9ad8\u7684\u7c7b\u578b, \u6211\u4e5f\u6709\u957f\u671f\u7eed\u8d39\u7684\u670d\u52a1\u5668, \u6545\u4f7f\u7528\u8fd9\u4e2a\u65b9\u6848\r\n\r\n\u5728\u4e3bserver\u670d\u52a1\u5668\u4f7f\u7528\r\n\r\n```sh\r\nkubectl get nodes -A\r\n```\r\n\u51fa\u73b0\u6bcf\u53f0\u673a\u5b50\u7684\u4fe1\u606f, \u4ee3\u8868\u96c6\u7fa4\u5185\u90e8\u7f51\u7edc\u901a\u4fe1\u6ca1\u95ee\u9898\r\n\r\n\u5728\u4e3bserver\u670d\u52a1\u5668\u4f7f\u7528\r\n```sh\r\nkubectl get pods --all-namespaces\r\n```\r\n\u5728\u6240\u6709\u670d\u52a1\u5728 `RUNNING` \u72b6\u6001\u65f6, \u4e3a\u5b89\u88c5\u6210\u529f (\u8fd9\u4e9b\u670d\u52a1\u90fd\u662f\u5185\u90e8\u901a\u4fe1\u4e0e\u5747\u8861\u8d1f\u8f7d\u7684\u955c\u50cf), \u5982\u679c\u662f\u5361\u5728 `container creating`, \u5219\u5b89\u88c5\u5931\u8d25, \u539f\u56e0\u662f\u955c\u50cf\u6ca1\u6b63\u786e\u914d\u7f6e\r\n\r\n### \u5b89\u88c5helm (\u867d\u7136\u4e0d\u77e5\u9053\u5e72\u4ec0\u4e48\u7528, \u96c6\u7fa4\u5185\u4e5f\u81ea\u5e26\u4e00\u4e2ahelm)\r\n\r\n1. \u624b\u52a8\u5b89\u88c5\r\n    1. \u4e0b\u8f7d\u9700\u8981\u7684\u7248\u672c [\u4e0b\u8f7d\u5730\u5740](https://github.com/helm/helm/releases)\r\n    2. \u89e3\u538b, \u4e0a\u4f20\u5230\u670d\u52a1\u5668, chmod\u7ed9\u6267\u884c\u6743\u9650\r\n    3. \u79fb\u52a8\u5230\u73af\u5883\u53d8\u91cf\u7684\u76ee\u5f55\u4e2d\r\n        ```sh\r\n        mv helm /usr/local/bin/helm\r\n        ```\r\n2. \u4f7f\u7528\u811a\u672c\u5b89\u88c5\r\n    ```sh\r\n    https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash\r\n    ```\r\n\r\n## \u9762\u677f\u5b89\u88c5\r\n\r\n\u4e3a\u4e86\u7b80\u5355, \u9762\u677f\u9009\u62e9\u7684\u662f kubepi\r\n\r\n[\u6587\u6863](https://github.com/1Panel-dev/KubePi/wiki/2%E3%80%81%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2#kubernetes)\r\n\r\n\u8fd9\u91cc\u9009\u62e9\u7684\u662f\u975e\u6301\u4e45\u5316\u90e8\u7f72, \u5728\u76f4\u63a5\u90e8\u7f72\u5728\u521a\u521a\u5efa\u597d\u7684\u96c6\u7fa4\u4e4b\u4e2d\r\n\r\n> \u6301\u4e45\u5316\u90e8\u7f72\u4f1a\u6709\u83ab\u540d\u5176\u5999\u7684\u5206\u914d\u95ee\u9898, \u5e94\u8be5\u662f\u8ddf\u5206\u914d\u672c\u5730\u7a7a\u95f4\u6709\u5173\u7cfb, \u6211\u4e5f\u4e0d\u9700\u8981\u6301\u4e45\u5316\u96c6\u7fa4\u4fe1\u606f(\u56e0\u4e3a\u53ea\u6709\u4e00\u4e2a\u96c6\u7fa4), \u6240\u4ee5\u6ca1\u4ec0\u4e48\u5173\u7cfb\r\n\r\n```sh\r\n# \u5b89\u88c5\r\nsudo kubectl apply -f https://raw.githubusercontent.com/1Panel-dev/KubePi/master/docs/deploy/kubectl/kubepi.yaml\r\n```\r\n\r\n\u5b89\u88c5\u5b8c\u6210\u540e, \u6839\u636e\u5b89\u88c5\u6559\u7a0b, \u83b7\u53d6\u8bbf\u95ee\u5730\u5740\r\n\r\n```sh\r\n# \u83b7\u53d6 NodeIp\r\nexport NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[0].address}')\r\n# \u83b7\u53d6 NodePort\r\nexport NODE_PORT=$(kubectl -n kube-system get services kubepi -o jsonpath='{.spec.ports[0].nodePort}')\r\n# \u83b7\u53d6 Address\r\necho http://$NODE_IP:$NODE_PORT\r\n```\r\n\r\n> \u6ce8: \u5185\u7f51\u7ec4\u673a\u5b50\u7684\u65f6\u5019\u8fd9\u91cc\u4f1a\u662f\u5185\u7f51\u5730\u5740, \u9700\u8981\u4f7f\u7528\u7aef\u53e3\u8f6c\u53d1\u8f6c\u53d1\u5230 `0.0.0.0` \u4e4b\u540e\u624d\u80fd\u5916\u7f51\u8bbf\u95ee\r\n> ```sh\r\n> kubectl port-forward --address 0.0.0.0 kubepi-d8477f9d8-drthz -n kube-system 2999:80\r\n> ```\r\n> \u6b64\u547d\u4ee4\u4e0d\u4f1a\u4e2d\u65ad, \u4f1a\u6301\u7eed\u8fd0\u884c, \u9700\u8981\u628a\u8fd9\u6761\u547d\u4ee4\u4e2d\u7684 `kubepi-d8477f9d8-drthz` \u6362\u6210\u5b9e\u9645\u540d\u5b57\r\n\r\n\u767b\u9646\u7cfb\u7edf\r\n\r\n```text\r\n\u5730\u5740: http://$NODE_IP:$NODE_PORT\r\n\u7528\u6237\u540d: admin\r\n\u5bc6\u7801: kubepi\r\n```\r\n\r\n\u767b\u9646\u540e\u8bb0\u5f97\u4fee\u6539\u5bc6\u7801\r\n\r\n\u5bfc\u5165\u96c6\u7fa4\r\n\r\n\u5728\u4e3b\u670d\u52a1\u5668, \u83b7\u53d6\r\n\r\n```sh\r\ncd /etc/rancher/k3s\r\ncat k3s.yaml\r\n```\r\n\u5728 kubepi \u5bfc\u5165\u96c6\u7fa4, \u8ba4\u8bc1\u6a21\u5f0f\u9009\u62e9 kubeconfig\u6587\u4ef6, \u628a\u8fd9\u4e2a\u6587\u4ef6\u590d\u5236\u8fdb\u53bb\r\n\r\n\u5728\u96c6\u7fa4\u914d\u7f6e\u4e2d, \u914d\u7f6e\u4e00\u4e0b\u7f51\u7edc, \u4f7f\u4e4b\u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7\u5916\u7f51\u7aef\u53e3\u8bbf\u95ee\r\n\r\n\u5177\u4f53\u914d\u7f6e\u6d41\u7a0b\u5fd8\u4e86, \u6b64\u65b9\u6cd5\u7531\u540c\u4e8b\u6307\u70b9\r\n\r\n## \u90e8\u7f72\u9879\u76ee\r\n\r\n\u5728 kubepi \u4e2d \u9009\u62e9\u96c6\u7fa4, \u5e94\u7528\u5e02\u573a, chart \u4ed3\u5e93, \u586b\u5165\u76f8\u5173\u4fe1\u606f, \u8fd9\u91cc\u6211\u4f7f\u7528\u7684\u662f:\r\n```text\r\n\u5f00\u6e90\u793e: http://mirror.kaiyuanshe.cn/kubernetes/charts/\r\n\u5f00\u6e90\u5e94\u7528\u5e02\u573a: https://charts.grapps.cn\r\n```\r\n\r\n\u70b9\u5f00\u5e94\u7528\u5c31\u6709\u5f88\u591a\u9879\u76ee\u8df3\u51fa\u6765\u53ef\u4ee5\u90e8\u7f72\u3002", "top": 0, "createdAt": 1732268091, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2024-11-22", "dateLabelColor": "#bc4c00"}, "P4": {"htmlDir": "docs/post/nailong-shu-ju-ji-, -jian-ce-nailong-de-mo-xing-, -xun-lian-yu-tui-li- (-yi-).html", "labels": ["blog"], "postTitle": "nailong\u6570\u636e\u96c6, \u68c0\u6d4bnailong\u7684\u6a21\u578b, \u8bad\u7ec3\u4e0e\u63a8\u7406 (\u4e00)", "postUrl": "post/nailong-shu-ju-ji-%2C%20-jian-ce-nailong-de-mo-xing-%2C%20-xun-lian-yu-tui-li-%20%28-yi-%29.html", "postSourceUrl": "https://github.com/FairyOwO/FairyOwO.github.io/issues/4", "commentNum": 0, "wordCount": 48336, "description": "> \u56de\u5f52 \u8001\u672c\u884c\r\n\r\n\u5076\u7136\u5f97\u5230 nailong \u6570\u636e\u96c6, \u5206\u4e3a\u4e24\u5757, \u4e00\u79cd\u662f\u7ed9[\u5206\u7c7b\u6a21\u578b\u4f7f\u7528\u7684\u6570\u636e\u96c6](https://huggingface.co/datasets/refoundd/NailongClassification), \u53e6\u4e00\u79cd\u662f\u7ed9[\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u4f7f\u7528\u7684\u6570\u636e\u96c6](https://huggingface.co/datasets/refoundd/NailongDetection)\r\n\r\n> \u540e\u8005\u7684\u6570\u636e\u91cf\u4e0d\u662f\u975e\u5e38\u591a(6\u5f20), \u7b49\u5230\u6709\u8db3\u591f\u591a\u7684\u6570\u636e\u6216\u8005\u6211\u4e00\u65f6\u5174\u8d77\u624b\u52a8\u6807\u6ce8\u5728\u8fdb\u884c\u7814\u7a76\r\n\r\n## \u521d\u7248\u65b9\u6848\r\n\r\n### \u6570\u636e\u96c6\u9009\u62e9\r\n\r\n\u5728\u6211\u521a\u63a5\u89e6\u8fd9\u4e2a\u6570\u636e\u96c6\u7684\u65f6\u5019, \u6570\u636e\u96c6\u662f\u53ea\u6709\u5976\u9f99\u7684(\u65e0\u5176\u4ed6\u6807\u7b7e\u7684\u6570\u636e), \u8fd9\u4e2a\u65f6\u5019\u7b2c\u4e00\u4e2a\u60f3\u6cd5\u5c31\u662f\u5f15\u5165\u5176\u4ed6\u5206\u7c7b, \u8fd9\u91cc\u91c7\u7528cifer10\u6570\u636e\u96c6\u7684\u6570\u636e, \u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u589e\u5e7f\r\n\r\n\u7136\u800c, cifer10 \u7684\u6570\u636e\u5206\u5e03\u6bd5\u7adf\u4e0e\u5e38\u89c1\u7fa4\u804a\u5185\u53d1\u9001\u7684\u56fe\u7247\u4e0d\u540c, \u6211\u89c9\u5f97\u4f1a\u5f71\u54cd\u6700\u7ec8\u80fd\u529b, \u5e94\u8be5\u6709\u9009\u62e9\u6027\u800c\u4e0d\u662f\u968f\u610f\u6dfb\u52a0\u5176\u4ed6\u7c7b\u578b\u7684\u56fe\u7247, \u5728\u4e00\u756a\u641c\u7d22\u4e4b\u540e, \u9009\u4e2d\u4e86 [\u8868\u60c5\u5305\u6570\u636e\u96c6](https://github.com/LLM-Red-Team/emo-visual-data)\r\n\r\n\u867d\u7136\u8fd9\u4e2a\u6570\u636e\u96c6\u7684\u539f\u8ba1\u5212\u662f\u7528\u6765\u68c0\u6d4b VLLM \u7684\u80fd\u529b, \u4f46\u6211\u8ba4\u4e3a\u5728\u6211\u4eec\u8fd9\u4e2a\u4efb\u52a1\u4e2d\u4e5f\u53ef\u4ee5\u4f7f\u7528\r\n\r\n### \u6a21\u578b\r\n\r\n\u5728\u6572\u5b9a\u6570\u636e\u96c6\u4e4b\u540e, \u5c31\u5f00\u59cb\u6311\u9009\u6a21\u578b\u4e86, \u56e0\u4e3a\u662f\u4e2a\u4eba\u5c0f\u9879\u76ee, \u8fd9\u91cc\u91c7\u7528\u6211\u4e2a\u4eba\u559c\u597d\u7684\u6a21\u578b\u9009\u62e9, \u4f7f\u7528\u4e86 [convnext \u7cfb\u6a21\u578b](https://github.com/facebookresearch/ConvNeXt)\r\n\r\n\u8fd9\u4e2a\u6a21\u578b\u7684\u8bba\u6587\u662f\u4e00\u7bc7\u975e\u5e38\u7ecf\u5178\u7684\u5b9e\u9a8c\u6587, \u91cc\u9762\u5927\u91cf\u63a2\u7d22\u4e86\u4e00\u4e9b\u6280\u5de7\u5bf9\u6a21\u578b\u80fd\u529b\u7684\u5f71\u54cd (\u5404\u7c7b\u6d88\u878d\u5b9e\u9a8c), \u867d\u7136\u4ed6\u662f 2020 \u5e74\u63a8\u51fa, \u4f46\u4ed6\u5bf9\u73b0\u5728\u7684\u5377\u79ef\u7f51\u7edc\u7684\u8bad\u7ec3\u6280\u5de7\u7684\u6307\u5f15\u5f88\u5927\r\n\r\n\u5177\u4f53\u7ec6\u8282\u53ef\u4ee5\u641c\u7d22\u76f8\u5173\u7684\u6a21\u578b\u89e3\u6790, \u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\r\n\r\n<details><summary>model.py</summary>\r\n<p>\r\n\r\n```python\r\n# copy from facebook/ConvNeXt\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom timm.models.layers import trunc_normal_, DropPath\r\nfrom timm.models.registry import register_model\r\n\r\nclass Block(nn.Module):\r\n    r''' ConvNeXt Block. There are two equivalent implementations:\r\n    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\r\n    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\r\n    We use (2) as we find it slightly faster in PyTorch\r\n    \r\n    Args:\r\n        dim (int): Number of input channels.\r\n        drop_path (float): Stochastic depth rate. Default: 0.0\r\n        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\r\n    '''\r\n    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\r\n        super().__init__()\r\n        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\r\n        self.norm = LayerNorm(dim, eps=1e-6)\r\n        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\r\n        self.act = nn.GELU()\r\n        self.pwconv2 = nn.Linear(4 * dim, dim)\r\n        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \r\n                                    requires_grad=True) if layer_scale_init_value > 0 else None\r\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\r\n\r\n    def forward(self, x):\r\n        input = x\r\n        x = self.dwconv(x)\r\n        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\r\n        x = self.norm(x)\r\n        x = self.pwconv1(x)\r\n        x = self.act(x)\r\n        x = self.pwconv2(x)\r\n        if self.gamma is not None:\r\n            x = self.gamma * x\r\n        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\r\n\r\n        x = input + self.drop_path(x)\r\n        return x\r\n\r\nclass ConvNeXt(nn.Module):\r\n    r''' ConvNeXt\r\n        A PyTorch impl of : `A ConvNet for the 2020s`  -\r\n          https://arxiv.org/pdf/2201.03545.pdf\r\n\r\n    Args:\r\n        in_chans (int): Number of input image channels. Default: 3\r\n        num_classes (int): Number of classes for classification head. Default: 1000\r\n        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\r\n        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\r\n        drop_path_rate (float): Stochastic depth rate. Default: 0.\r\n        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\r\n        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\r\n    '''\r\n    def __init__(self, in_chans=3, num_classes=1000, \r\n                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0., \r\n                 layer_scale_init_value=1e-6, head_init_scale=1.,\r\n                 ):\r\n        super().__init__()\r\n\r\n        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers\r\n        stem = nn.Sequential(\r\n            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\r\n            LayerNorm(dims[0], eps=1e-6, data_format='channels_first')\r\n        )\r\n        self.downsample_layers.append(stem)\r\n        for i in range(3):\r\n            downsample_layer = nn.Sequential(\r\n                    LayerNorm(dims[i], eps=1e-6, data_format='channels_first'),\r\n                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\r\n            )\r\n            self.downsample_layers.append(downsample_layer)\r\n\r\n        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\r\n        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \r\n        cur = 0\r\n        for i in range(4):\r\n            stage = nn.Sequential(\r\n                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], \r\n                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\r\n            )\r\n            self.stages.append(stage)\r\n            cur += depths[i]\r\n\r\n        self.norm = nn.LayerNorm(dims[-1], eps=1e-6) # final norm layer\r\n        self.head = nn.Linear(dims[-1], num_classes)\r\n\r\n        self.apply(self._init_weights)\r\n        self.head.weight.data.mul_(head_init_scale)\r\n        self.head.bias.data.mul_(head_init_scale)\r\n\r\n    def _init_weights(self, m):\r\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\r\n            trunc_normal_(m.weight, std=.02)\r\n            nn.init.constant_(m.bias, 0)\r\n\r\n    def forward_features(self, x):\r\n        for i in range(4):\r\n            x = self.downsample_layers[i](x)\r\n            x = self.stages[i](x)\r\n        return self.norm(x.mean([-2, -1])) # global average pooling, (N, C, H, W) -> (N, C)\r\n\r\n    def forward(self, x):\r\n        x = self.forward_features(x)\r\n        x = self.head(x)\r\n        return x\r\n\r\nclass LayerNorm(nn.Module):\r\n    r''' LayerNorm that supports two data formats: channels_last (default) or channels_first. \r\n    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \r\n    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \r\n    with shape (batch_size, channels, height, width).\r\n    '''\r\n    def __init__(self, normalized_shape, eps=1e-6, data_format='channels_last'):\r\n        super().__init__()\r\n        self.weight = nn.Parameter(torch.ones(normalized_shape))\r\n        self.bias = nn.Parameter(torch.zeros(normalized_shape))\r\n        self.eps = eps\r\n        self.data_format = data_format\r\n        if self.data_format not in ['channels_last', 'channels_first']:\r\n            raise NotImplementedError \r\n        self.normalized_shape = (normalized_shape, )\r\n    \r\n    def forward(self, x):\r\n        if self.data_format == 'channels_last':\r\n            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\r\n        elif self.data_format == 'channels_first':\r\n            u = x.mean(1, keepdim=True)\r\n            s = (x - u).pow(2).mean(1, keepdim=True)\r\n            x = (x - u) / torch.sqrt(s + self.eps)\r\n            x = self.weight[:, None, None] * x + self.bias[:, None, None]\r\n            return x\r\n\r\n\r\nmodel_urls = {\r\n    'convnext_tiny_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth',\r\n    'convnext_small_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth',\r\n    'convnext_base_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth',\r\n    'convnext_large_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth',\r\n    'convnext_tiny_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth',\r\n    'convnext_small_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth',\r\n    'convnext_base_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth',\r\n    'convnext_large_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth',\r\n    'convnext_xlarge_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth',\r\n}\r\n\r\n@register_model\r\ndef convnext_tiny(pretrained=False,in_22k=False, **kwargs):\r\n    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)\r\n    if pretrained:\r\n        url = model_urls['convnext_tiny_22k'] if in_22k else model_urls['convnext_tiny_1k']\r\n        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu', check_hash=True)\r\n        model.load_state_dict(checkpoint['model'])\r\n    return model\r\n\r\n@register_model\r\ndef convnext_small(pretrained=False,in_22k=False, **kwargs):\r\n    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], **kwargs)\r\n    if pretrained:\r\n        url = model_urls['convnext_small_22k'] if in_22k else model_urls['convnext_small_1k']\r\n        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')\r\n        model.load_state_dict(checkpoint['model'])\r\n    return model\r\n\r\n@register_model\r\ndef convnext_base(pretrained=False, in_22k=False, **kwargs):\r\n    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], **kwargs)\r\n    if pretrained:\r\n        url = model_urls['convnext_base_22k'] if in_22k else model_urls['convnext_base_1k']\r\n        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')\r\n        model.load_state_dict(checkpoint['model'])\r\n    return model\r\n\r\n@register_model\r\ndef convnext_large(pretrained=False, in_22k=False, **kwargs):\r\n    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], **kwargs)\r\n    if pretrained:\r\n        url = model_urls['convnext_large_22k'] if in_22k else model_urls['convnext_large_1k']\r\n        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')\r\n        model.load_state_dict(checkpoint['model'])\r\n    return model\r\n\r\n@register_model\r\ndef convnext_xlarge(pretrained=False, in_22k=False, **kwargs):\r\n    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048], **kwargs)\r\n    if pretrained:\r\n        assert in_22k, 'only ImageNet-22K pre-trained ConvNeXt-XL is available; please set in_22k=True'\r\n        url = model_urls['convnext_xlarge_22k']\r\n        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')\r\n        model.load_state_dict(checkpoint['model'])\r\n    return model\r\n```\r\n\r\n</p>\r\n</details> \r\n\r\n\r\n### \u4ee3\u7801\r\n\r\n\u8bad\u7ec3\u4ee3\u7801\u5927\u90e8\u5206\u90fd\u662f\u6a21\u677f, \u4e0d\u8fc7, \u6211\u53d1\u73b0\u6211\u8fd8\u6ca1\u6709\u4e00\u4e2a\u5c5e\u4e8e\u81ea\u5df1\u7684 trainer, \u8d81\u7740\u8fd9\u6b21\u8bad\u7ec3\u6a21\u578b\u7684\u65f6\u5019\u8865\u5145\u4e00\u4e2a\r\n\r\n\u4f7f\u7528\u522b\u4eba\u7684 trainer \u96be\u514d\u4f1a\u9047\u5230 debug, \u800c\u4ee3\u7801\u4e0d\u719f\u7684\u60c5\u51b5, \u81ea\u5df1\u5199\u7684 trainer \u53ef\u4ee5\u638c\u63e1\u5404\u79cd\u7ec6\u8282\r\n\r\n\u5728\u6574\u7406\u4e86\u4e00\u4e9b\u4ee5\u524d\u4ee3\u7801\u540e, \u603b\u7ed3\u51fa\u4e86\u8986\u76d6\u8bb8\u591a\u8bad\u7ec3\u6a21\u578b\u60c5\u51b5\u7684\u6d41\u7a0b, \u8d81\u7740\u8fd9\u4e2a\u65f6\u5019\u6d4b\u8bd5\u4e00\u4e0b\u73b0\u5728ai\u7f16\u7801\u7684\u80fd\u529b, \u5c06\u6d41\u7a0b\u53d1\u7ed9 claude-sonnet \u540e, \u8f93\u51fa\u4e86\u4e00\u7248\u4ee3\u7801, \u5728\u6211\u7684\u4e00\u4e9b\u5c0f\u4fee\u5c0f\u8865(\u8865\u5145\u65e5\u5fd7)\u540e, \u5c31\u53ef\u4ee5\u8dd1\u8d77\u6765\u4e86\r\n\r\n<details><summary>trainer.py</summary>\r\n<p>\r\n\r\n```python\r\nimport gc\r\nimport json\r\nimport logging\r\nimport os\r\nimport shutil\r\n\r\nimport torch\r\nfrom torch import optim\r\nfrom torch.amp import GradScaler\r\nfrom torch.nn.utils import clip_grad_norm_\r\nfrom torch.utils.tensorboard import SummaryWriter\r\nfrom tqdm import tqdm\r\n\r\nexample_config = {\r\n    'model': 'model_name',\r\n    'checkpoint_dir': './checkpoints',\r\n    'tensorboard_dir': './tensorboard',\r\n    'device': 'cuda',\r\n    'enable_cudnn_benchmark': True,\r\n    'enable_amp': False,\r\n    'learning_rate': 1e-4,\r\n    'betas': [0.9, 0.999],\r\n    'eps': 1e-8,\r\n    'enable_compile': False,\r\n    'weight_decay': 0.05,\r\n    'max_steps': 100000,\r\n    'max_grad_norm': 1.0,\r\n    'save_every': 10000,\r\n    'gradient_accumulation_steps': 4\r\n}\r\n\r\n\r\nclass Trainer:\r\n    def __init__(self, config):\r\n        self.config = config\r\n        self.setup_logging()\r\n        self.setup_device()\r\n        self.setup_model()\r\n        self.setup_training()\r\n        \r\n    def setup_logging(self):\r\n        '''\u8bbe\u7f6e\u65e5\u5fd7'''\r\n        logging.basicConfig(\r\n            level=logging.INFO,\r\n            format='%(asctime)s %(levelname)s %(message)s'\r\n        )\r\n        self.logger = logging.getLogger(__name__)\r\n        self.writer = SummaryWriter(self.config['tensorboard_dir'])\r\n        \r\n    def setup_device(self):\r\n        '''\u8bbe\u7f6e\u8bbe\u5907'''\r\n        self.device = torch.device(self.config['device'])\r\n        torch.backends.cudnn.benchmark = self.config.get('enable_cudnn_benchmark', True)\r\n        if self.device.type == 'cuda':\r\n            self.logger.info(f'Using device: {self.device} ({torch.cuda.get_device_name()})')\r\n        else:\r\n            self.logger.info(f'Using device: {self.device}')\r\n            \r\n    def setup_model(self):\r\n        '''\u8bbe\u7f6e\u6a21\u578b\u3001\u635f\u5931\u51fd\u6570\u7b49'''\r\n        self.model = self.build_model().to(self.device)\r\n        if self.config.get('enable_compile', False):\r\n            self.model.compile()\r\n        self.criterion = self.build_criterion()\r\n        \r\n        # \u6253\u5370\u6a21\u578b\u4fe1\u606f\r\n        n_parameters = sum(p.numel() for p in self.model.parameters())\r\n        self.logger.info(f'Number of parameters: {n_parameters:,}')\r\n        \r\n    def setup_training(self):\r\n        '''\u8bbe\u7f6e\u8bad\u7ec3\u76f8\u5173\u7ec4\u4ef6'''\r\n        # \u4f18\u5316\u5668\r\n        self.optimizer = self.build_optimizer()\r\n        \r\n        # \u5b66\u4e60\u7387\u8c03\u5ea6\u5668\r\n        self.scheduler = self.build_scheduler()\r\n        \r\n        # \u68af\u5ea6\u7f29\u653e\u5668(\u7528\u4e8e\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3)\r\n        self.scaler = GradScaler(\r\n            enabled=self.config.get('enable_amp', False)\r\n        )\r\n        self.gradient_accumulation_steps = self.config.get('gradient_accumulation_steps', 1)\r\n        \r\n        # \u52a0\u8f7d\u68c0\u67e5\u70b9\r\n        self.steps = 0\r\n        self.best_metric = {}\r\n        self.load_checkpoint()\r\n        \r\n    def build_model(self):\r\n        '''\u6784\u5efa\u6a21\u578b(\u9700\u8981\u5b50\u7c7b\u5b9e\u73b0)'''\r\n        raise NotImplementedError\r\n        \r\n    def build_criterion(self):\r\n        '''\u6784\u5efa\u635f\u5931\u51fd\u6570(\u9700\u8981\u5b50\u7c7b\u5b9e\u73b0)'''\r\n        raise NotImplementedError\r\n        \r\n    def build_optimizer(self):\r\n        '''\u6784\u5efa\u4f18\u5316\u5668'''\r\n        # \u533a\u5206\u9700\u8981\u548c\u4e0d\u9700\u8981weight decay\u7684\u53c2\u6570\r\n        decay_params = []\r\n        no_decay_params = []\r\n        for name, param in self.model.named_parameters():\r\n            if 'bias' in name or 'norm' in name:\r\n                no_decay_params.append(param)\r\n            else:\r\n                decay_params.append(param)\r\n                \r\n        opt_params = [\r\n            {'params': decay_params, 'weight_decay': self.config['weight_decay']},\r\n            {'params': no_decay_params, 'weight_decay': 0.0}\r\n        ]\r\n        \r\n        return optim.AdamW(\r\n            opt_params,\r\n            lr=self.config['learning_rate'],\r\n            betas=self.config.get('betas', (0.9, 0.999)),\r\n            eps=self.config.get('eps', 1e-8)\r\n        )\r\n        \r\n    def build_scheduler(self):\r\n        '''\u6784\u5efa\u5b66\u4e60\u7387\u8c03\u5ea6\u5668(\u9700\u8981\u5b50\u7c7b\u5b9e\u73b0)'''\r\n        return NotImplementedError\r\n        \r\n    def build_dataloader(self):\r\n        '''\u6784\u5efa\u6570\u636e\u52a0\u8f7d\u5668(\u9700\u8981\u5b50\u7c7b\u5b9e\u73b0)'''\r\n        raise NotImplementedError\r\n        \r\n    def train_step(self, batch):\r\n        '''\u5355\u6b65\u8bad\u7ec3(\u9700\u8981\u5b50\u7c7b\u5b9e\u73b0)'''\r\n        raise NotImplementedError\r\n        \r\n    def validate(self):\r\n        '''\u9a8c\u8bc1(\u9700\u8981\u5b50\u7c7b\u5b9e\u73b0)'''\r\n        raise NotImplementedError\r\n        \r\n    def save_checkpoint(self, is_best=False):\r\n        '''\u4fdd\u5b58\u68c0\u67e5\u70b9'''\r\n        state = {\r\n            'model': self.model.state_dict(),\r\n            'optimizer': self.optimizer.state_dict(),\r\n            'scheduler': self.scheduler.state_dict(),\r\n            'scaler': self.scaler.state_dict(),\r\n            'steps': self.steps,\r\n            'best_metric': self.best_metric,\r\n            'config': self.config\r\n        }\r\n        \r\n        # \u4fdd\u5b58\u6700\u65b0\u68c0\u67e5\u70b9\r\n        torch.save(\r\n            state,\r\n            os.path.join(self.config['checkpoint_dir'], 'latest.pt')\r\n        )\r\n        \r\n        # \u4fdd\u5b58\u6700\u4f73\u68c0\u67e5\u70b9\r\n        if is_best:\r\n            shutil.copy(\r\n                os.path.join(self.config['checkpoint_dir'], 'latest.pt'),\r\n                os.path.join(self.config['checkpoint_dir'], 'best.pt')\r\n            )\r\n            \r\n    def load_checkpoint(self):\r\n        '''\u52a0\u8f7d\u68c0\u67e5\u70b9'''\r\n        checkpoint_path = os.path.join(\r\n            self.config['checkpoint_dir'],\r\n            'latest.pt'\r\n        )\r\n        \r\n        if os.path.exists(checkpoint_path):\r\n            checkpoint = torch.load(\r\n                checkpoint_path,\r\n                map_location=self.device\r\n            )\r\n            \r\n            self.model.load_state_dict(checkpoint['model'])\r\n            self.optimizer.load_state_dict(checkpoint['optimizer'])\r\n            self.scheduler.load_state_dict(checkpoint['scheduler'])\r\n            self.scaler.load_state_dict(checkpoint['scaler'])\r\n            self.steps = checkpoint['steps']\r\n            self.best_metric = checkpoint['best_metric']\r\n            \r\n            self.logger.info(f'Loaded checkpoint from {checkpoint_path}')\r\n            self.logger.info(f'Training will resume from step {self.steps}')\r\n    \r\n    @staticmethod\r\n    def is_better_performance(baseline_dict, compare_dict):\r\n        '''\r\n        \u5224\u65adcompare_dict\u4e2d\u7684\u6307\u6807\u662f\u5426\u5168\u9762\u8d85\u8fc7baseline_dict\r\n        \r\n        Args:\r\n            baseline_dict: \u57fa\u51c6\u5b57\u5178,\u683c\u5f0f\u4e3a {\u6307\u6807\u540d: \u503c}\r\n            compare_dict: \u6bd4\u8f83\u5b57\u5178,\u683c\u5f0f\u4e3a {\u6307\u6807\u540d: \u503c} \r\n        \r\n        Returns:\r\n            bool: \u5982\u679ccompare_dict\u4e2d\u6240\u6709\u6307\u6807\u90fd\u4e25\u683c\u5927\u4e8ebaseline_dict\u5219\u8fd4\u56deTrue,\u5426\u5219\u8fd4\u56deFalse\r\n        '''\r\n        if not baseline_dict:\r\n            return True\r\n        \r\n        # \u68c0\u67e5\u4e24\u4e2a\u5b57\u5178\u7684\u952e\u662f\u5426\u4e00\u81f4\r\n        if set(baseline_dict.keys()) != set(compare_dict.keys()):\r\n            return False\r\n            \r\n        # \u68c0\u67e5\u6bcf\u4e2a\u6307\u6807\u662f\u5426\u90fd\u6709\u63d0\u5347\r\n        for metric in baseline_dict:\r\n            if compare_dict[metric] <= baseline_dict[metric]:\r\n                return False\r\n                \r\n        return True\r\n            \r\n    def train(self):\r\n        '''\u8bad\u7ec3\u6d41\u7a0b'''\r\n        train_loader = self.build_dataloader()\r\n        self.model.train()\r\n        \r\n        self.logger.info('Start training...')\r\n        pbar = tqdm(total=self.config['max_steps'], initial=self.steps)\r\n        \r\n        while self.steps < self.config['max_steps']:\r\n            for batch in train_loader:\r\n                # \u8bad\u7ec3\u4e00\u6b65\r\n                with torch.autocast(device_type=self.config['device'], enabled=self.config.get('enable_amp', False)):\r\n                    loss = self.train_step(batch)\r\n                self.scaler.scale(loss / self.gradient_accumulation_steps).backward()\r\n                \r\n                if (self.steps + 1) % self.gradient_accumulation_steps == 0:\r\n                    # \u68af\u5ea6\u88c1\u526a\r\n                    if self.config.get('max_grad_norm', 0) > 0:\r\n                        self.scaler.unscale_(self.optimizer)\r\n                        clip_grad_norm_(\r\n                            self.model.parameters(),\r\n                            self.config['max_grad_norm']\r\n                        )\r\n\r\n                    # \u4f18\u5316\u5668\u6b65\u8fdb\r\n                    self.scaler.step(self.optimizer)\r\n                    self.scaler.update()\r\n                    self.optimizer.zero_grad(set_to_none=True)\r\n                self.scheduler.step()\r\n                \r\n                # \u8bb0\u5f55\r\n                self.writer.add_scalar('train/loss', loss, self.steps)\r\n                self.writer.add_scalar(\r\n                    'train/lr',\r\n                    self.scheduler.get_last_lr()[0],\r\n                    self.steps\r\n                )\r\n                \r\n                self.steps += 1\r\n                pbar.update(1)\r\n                \r\n                # \u9a8c\u8bc1\u548c\u4fdd\u5b58\r\n                if self.steps % self.config['save_every'] == 0:\r\n                    metric = self.validate()\r\n                    for i in metric:\r\n                        self.logger.info(f'Validation {i}: {metric[i]}')\r\n                        self.writer.add_scalar(f'val/{i}', metric[i], self.steps)\r\n                    \r\n                    is_best = self.is_better_performance(self.best_metric, metric)\r\n                    if is_best:\r\n                        self.best_metric = metric\r\n\r\n                    self.model.train()\r\n                    self.save_checkpoint(is_best)\r\n                    \r\n                if self.steps >= self.config['max_steps']:\r\n                    break\r\n                \r\n            gc.collect()\r\n            torch.cuda.empty_cache()\r\n                    \r\n        pbar.close()\r\n        self.logger.info('Training finished!')\r\n\r\n\r\ndef main():\r\n    '''\u4e3b\u51fd\u6570'''\r\n    # \u52a0\u8f7d\u914d\u7f6e\r\n    with open('config.json') as f:\r\n        config = json.load(f)\r\n        \r\n    # \u521b\u5efa\u8f93\u51fa\u76ee\u5f55\r\n    os.makedirs(config['checkpoint_dir'], exist_ok=True)\r\n    os.makedirs(config['tensorboard_dir'], exist_ok=True)\r\n    \r\n    # \u8bad\u7ec3\r\n    trainer = Trainer(config)\r\n    trainer.train()\r\n    \r\nif __name__ == '__main__':\r\n    try:\r\n        main()\r\n    except KeyboardInterrupt:\r\n        pass\r\n``` \r\n\r\n</p>\r\n</details> \r\n\r\n`trainer.py` \u7b80\u5355\u6574\u5408\u4e86\u51e0\u4e2a\u5e38\u7528\u7684\u8bad\u7ec3\u624b\u6bb5, \u6bd4\u5982\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3, \u68af\u5ea6\u88c1\u526a, \u68af\u5ea6\u7d2f\u8ba1, weight_decay(\u5199\u6b7b\u4e86), tensorboard\u7684\u8bb0\u5f55, \u65ad\u70b9\u7eed\u8bad\u7b49\u64cd\u4f5c, \u9700\u8981\u6ce8\u610f\u7684\u662f, `trainer.py` \u6ca1\u6709\u4f7f\u7528 epoch \u4f5c\u4e3a\u8bad\u7ec3\u8fdb\u5ea6, \u800c\u662f\u7528\u4e86\u66f4\u7cbe\u7ec6\u7684 step (\u6bcf\u6b21\u8fed\u4ee3\u53c2\u6570\u5373\u4e3a\u4e00\u4e2astep), \u4f7f\u7528\u7684\u65f6\u5019\u9700\u8981\u81ea\u884c\u5b9e\u73b0\u6a21\u578b\u6784\u5efa, \u635f\u5931\u51fd\u6570\u6784\u5efa \u5b66\u4e60\u7387\u8c03\u5ea6\u5668 \u6570\u636e\u96c6\u52a0\u8f7d\u5668, \u5355\u6b65\u8bad\u7ec3, \u9a8c\u8bc1\u7684\u6d41\u7a0b\u7684\u5b50\u7c7b\u5b9e\u73b0\r\n\r\n\u7136\u540e\u5c06\u4e00\u4e9b\u914d\u7f6e\u653e\u5230config\u4e2d\u4fbf\u4e8e\u8bfb\u53d6, \u5176\u4e2d\u6709\u4e00\u4e9b\u914d\u7f6e\u662f\u5fc5\u987b\u7684, \u5176\u4ed6\u5219\u662f\u5b50\u7c7b\u5b9e\u73b0\u7684\u65f6\u5019\u9700\u8981\u7684\r\n\r\n\u542c\u8d77\u6765\u53ef\u80fd\u6709\u70b9\u62bd\u8c61, \u4e0b\u9762\u662f\u4e00\u4e2a\u7b80\u5355\u7684trainer\u4f7f\u7528\u6848\u4f8b\r\n\r\n<details><summary>trainer\u4f7f\u7528\u6848\u4f8b</summary>\r\n<p>\r\n\r\nimport torchvision\r\nimport torch\r\nfrom trainer import Trainer\r\nfrom torchvision.models import resnet18\r\nfrom torch.optim.lr_scheduler import LambdaLR\r\n\r\n\r\n\r\ntransform = torchvision.transforms.Compose([\r\n    torchvision.transforms.ToTensor(),\r\n    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\r\n])\r\n\r\nclass ConstantLambdaLR(LambdaLR):\r\n    def __init__(self, optimizer, **kwargs):\r\n        kwargs['optimizer'] = optimizer\r\n        kwargs['lr_lambda'] = self._step_inner\r\n        super().__init__(**kwargs)\r\n\r\n    def _step_inner(self, steps):\r\n        return 1\r\n\r\n\r\nclass Cifer10Trainer(Trainer):\r\n    def __init__(self, config):\r\n        super().__init__(config)\r\n\r\n    def build_model(self):\r\n        model = resnet18()\r\n        model.fc = torch.nn.Linear(model.fc.in_features, 10)\r\n        return model\r\n    \r\n    def build_criterion(self):\r\n        return torch.nn.CrossEntropyLoss()\r\n    \r\n    def build_scheduler(self):\r\n        return ConstantLambdaLR(self.optimizer)\r\n    \r\n    def build_dataloader(self):\r\n        train_dataset = torchvision.datasets.CIFAR10(root='./temp', train=True, download=True, transform=transform)\r\n        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.config['batch_size'], shuffle=True, num_workers=1)\r\n        return train_loader\r\n    \r\n    def train_step(self, batch):\r\n        inputs, labels = batch\r\n        inputs, labels = inputs.to(self.device), labels.to(self.device)\r\n        outputs = self.model(inputs)\r\n        loss = self.criterion(outputs, labels)\r\n        return loss\r\n    \r\n    def validate(self):\r\n        self.model.eval()\r\n        test_dataset = torchvision.datasets.CIFAR10(root='./temp', train=False, download=True, transform=transform)\r\n        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=self.config['batch_size'], shuffle=False, num_workers=1)\r\n        acc = []\r\n        with torch.inference_mode():\r\n            for batch in test_loader:\r\n                inputs, labels = batch\r\n                inputs, labels = inputs.to(self.device), labels.to(self.device)\r\n                y_hat = self.model(inputs)\r\n                acc.append((y_hat.argmax(dim=1) == labels).sum().item() / labels.size(0))\r\n                \r\n        return {'acc': sum(acc) / len(acc)}\r\n                \r\n\r\ndef main():\r\n    config = {\r\n        'model': 'resnet18',\r\n        'checkpoint_dir': './checkpoints',\r\n        'tensorboard_dir': './tensorboard',\r\n        'device': 'cuda',\r\n        'enable_cudnn_benchmark': True,\r\n        'enable_amp': False,\r\n        'learning_rate': 1e-3,\r\n        'betas': [0.9, 0.999],\r\n        'eps': 1e-8,\r\n        'enable_compile': False,\r\n        'weight_decay': 0.05,\r\n        'max_steps': 500,\r\n        'max_grad_norm': 1.0,\r\n        'save_every': 100,\r\n        'gradient_accumulation_steps': 1,\r\n        'batch_size': 32\r\n    }\r\n    trainer = Cifer10Trainer(config)\r\n    trainer.train()\r\n    \r\nif __name__ == '__main__':\r\n    try:\r\n        main()\r\n    except KeyboardInterrupt:\r\n        pass\r\n\r\n</p>\r\n</details> \r\n\r\n> \u4ee3\u7801\u4f7f\u7528cifer10\u6570\u636e\u96c6, resnet18\u4f5c\u4e3a\u6a21\u578b\u8bad\u7ec3\u7684\u7b80\u5355\u7684\u6d41\u7a0b\r\n\r\n\u6709\u4e86\u6d41\u7a0b\u63a5\u4e0b\u6765\u7f16\u5199\u6211\u4eec\u7684\u8bad\u7ec3\u4ee3\u7801\r\n\r\n<details><summary>train.py(\u4ee3\u7801\u672a\u6574\u7406\u5b8c\u6bd5, \u975e\u521d\u7248\u4ee3\u7801, \u4ec5\u4f9b\u53c2\u8003)</summary>\r\n<p>\r\n\r\n```python\r\nimport os\r\nimport random\r\n\r\nimport cv2\r\nimport numpy as np\r\nfrom sklearn.metrics import f1_score\r\nimport torch\r\nfrom PIL import Image\r\nfrom torch.optim.lr_scheduler import LambdaLR\r\nfrom torch.utils.data import DataLoader, Dataset\r\nfrom datasets import load_dataset\r\nfrom torchvision import transforms\r\n\r\n# from torchvision.models import resnet18\r\nfrom model import convnext_base\r\nfrom trainer import Trainer\r\n\r\nimage_size = 224\r\nbatch_size = 32\r\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n\r\n\r\n# def get_color_from_image(image_path):\r\n#     '''\r\n#     \u4ece\u7eaf\u8272\u56fe\u7247\u4e2d\u83b7\u53d6RGB\u989c\u8272\u503c\r\n#     \u8fd4\u56de: (R, G, B)\u5143\u7ec4\r\n#     '''\r\n#     # \u8bfb\u53d6\u56fe\u7247\r\n#     image = Image.open(image_path).convert('RGB')\r\n#     # \u8f6c\u6362\u4e3anumpy\u6570\u7ec4\r\n#     img_array = np.array(image)\r\n    \r\n#     # \u83b7\u53d6\u56fe\u7247\u4e2d\u5fc3\u70b9\u7684\u989c\u8272\u503c\r\n#     h, w = img_array.shape[:2]\r\n#     center_color = img_array[h//2, w//2]\r\n    \r\n#     # \u6216\u8005\u8ba1\u7b97\u6574\u4e2a\u56fe\u7247\u7684\u5e73\u5747\u989c\u8272\r\n#     average_color = img_array.mean(axis=(0,1)).astype(int)\r\n    \r\n#     return tuple(average_color)  # \u6216\u8005 tuple(average_color)\r\n\r\n\r\n# class AugmentationUtils:\r\n#     @staticmethod\r\n#     def add_color_mask(image, is_positive):\r\n#         '''\u7ed9\u56fe\u7247\u6dfb\u52a0\u989c\u8272\u906e\u7f69'''\r\n#         # \u8f6c\u6362\u4e3anumpy\u6570\u7ec4\u5e76\u786e\u4fdd\u7c7b\u578b\u4e3auint8\r\n#         image = np.array(image, dtype=np.uint8)\r\n        \r\n#         # \u521b\u5efa\u4e0e\u56fe\u50cf\u76f8\u540c\u5927\u5c0f\u7684\u906e\u7f69\r\n#         mask = np.ones_like(image, dtype=np.uint8)\r\n        \r\n#         # \u968f\u673a\u751f\u6210\u989c\u8272\r\n#         if is_positive:\r\n#             color = [random.randint(0, 255) for _ in range(3)]\r\n#         else:\r\n#             color = get_color_from_image('22.png')\r\n        \r\n#         # \u4e3a\u906e\u7f69\u8d4b\u4e88\u989c\u8272    \r\n#         for i in range(3):\r\n#             mask[:, :, i] = color[i]\r\n        \r\n#         # \u786e\u4fddmask\u4e5f\u662fuint8\u7c7b\u578b\r\n#         mask = mask.astype(np.uint8)\r\n        \r\n#         # \u6dfb\u52a0\u906e\u7f69\r\n#         alpha = 0.5  # \u900f\u660e\u5ea6\r\n#         image = cv2.addWeighted(image, 1-alpha, mask, alpha, 0)\r\n        \r\n#         return Image.fromarray(image)\r\n\r\n#     @staticmethod\r\n#     def embed_positive_in_negative(positive_img, negative_img):\r\n#         '''\u5728\u8d1f\u6837\u672c\u4e2d\u5d4c\u5165\u6b63\u6837\u672c'''\r\n#         # \u8f6c\u6362\u4e3anumpy\u6570\u7ec4\r\n#         pos_img = np.array(positive_img)\r\n#         neg_img = np.array(negative_img)\r\n        \r\n#         # \u786e\u4fdd\u56fe\u50cf\u662f3\u901a\u9053\u7684\r\n#         if len(pos_img.shape) == 2:\r\n#             pos_img = cv2.cvtColor(pos_img, cv2.COLOR_GRAY2BGR)\r\n#         if len(neg_img.shape) == 2:\r\n#             neg_img = cv2.cvtColor(neg_img, cv2.COLOR_GRAY2BGR)\r\n        \r\n#         # \u83b7\u53d6\u8d1f\u6837\u672c\u5c3a\u5bf8\r\n#         h, w = neg_img.shape[:2]\r\n#         pos_h, pos_w = pos_img.shape[:2]\r\n        \r\n#         # \u8ba1\u7b97\u5408\u9002\u7684\u7f29\u653e\u6bd4\u4f8b\r\n#         scale = min(\r\n#             random.uniform(0.5, 0.8),\r\n#             (w * 0.8) / pos_w,\r\n#             (h * 0.8) / pos_h\r\n#         )\r\n        \r\n#         # \u7f29\u653e\u6b63\u6837\u672c\r\n#         new_size = (int(pos_w * scale), int(pos_h * scale))\r\n#         pos_img_resized = cv2.resize(pos_img, new_size)\r\n        \r\n#         # \u786e\u4fdd\u6709\u6548\u7684\u968f\u673a\u4f4d\u7f6e\u8303\u56f4\r\n#         max_x = max(0, w - new_size[0])\r\n#         max_y = max(0, h - new_size[1])\r\n        \r\n#         # \u968f\u673a\u9009\u62e9\u63d2\u5165\u4f4d\u7f6e\r\n#         x = random.randint(0, max_x) if max_x > 0 else 0\r\n#         y = random.randint(0, max_y) if max_y > 0 else 0\r\n        \r\n#         # \u83b7\u53d6ROI\u533a\u57df\u5e76\u786e\u4fdd\u4e0e\u7f29\u653e\u540e\u7684\u6b63\u6837\u672c\u5177\u6709\u76f8\u540c\u7684\u5f62\u72b6\r\n#         roi = neg_img[y:y+new_size[1], x:x+new_size[0]]\r\n        \r\n#         # \u786e\u4fddROI\u548cpos_img_resized\u5177\u6709\u76f8\u540c\u7684\u5f62\u72b6\u548c\u901a\u9053\u6570\r\n#         if roi.shape == pos_img_resized.shape:\r\n#             # \u6df7\u5408\u56fe\u50cf\r\n#             blended = cv2.addWeighted(roi, 0.3, pos_img_resized, 0.7, 0)\r\n#             neg_img[y:y+new_size[1], x:x+new_size[0]] = blended\r\n        \r\n#         return Image.fromarray(neg_img)\r\n    \r\n#     @staticmethod\r\n#     def embed_same(positive_img, negative_img):\r\n#         '''\u5728\u8d1f\u6837\u672c\u4e2d\u5d4c\u5165\u6b63\u6837\u672c'''\r\n#         # \u8f6c\u6362\u4e3anumpy\u6570\u7ec4\r\n#         pos_img = np.array(positive_img)\r\n#         neg_img = np.array(negative_img)\r\n        \r\n#         # \u786e\u4fdd\u56fe\u50cf\u662f3\u901a\u9053\u7684\r\n#         if len(pos_img.shape) == 2:\r\n#             pos_img = cv2.cvtColor(pos_img, cv2.COLOR_GRAY2BGR)\r\n#         if len(neg_img.shape) == 2:\r\n#             neg_img = cv2.cvtColor(neg_img, cv2.COLOR_GRAY2BGR)\r\n        \r\n#         # \u83b7\u53d6\u8d1f\u6837\u672c\u5c3a\u5bf8\r\n#         h, w = neg_img.shape[:2]\r\n#         pos_h, pos_w = pos_img.shape[:2]\r\n        \r\n#         # \u8ba1\u7b97\u5408\u9002\u7684\u7f29\u653e\u6bd4\u4f8b\r\n#         scale = min(\r\n#             random.uniform(0.5, 0.8),\r\n#             (w * 0.8) / pos_w,\r\n#             (h * 0.8) / pos_h\r\n#         )\r\n        \r\n#         # \u7f29\u653e\u6b63\u6837\u672c\r\n#         new_size = (int(pos_w * scale), int(pos_h * scale))\r\n#         pos_img_resized = cv2.resize(pos_img, new_size)\r\n        \r\n#         # \u786e\u4fdd\u6709\u6548\u7684\u968f\u673a\u4f4d\u7f6e\u8303\u56f4\r\n#         max_x = max(0, w - new_size[0])\r\n#         max_y = max(0, h - new_size[1])\r\n        \r\n#         # \u968f\u673a\u9009\u62e9\u63d2\u5165\u4f4d\u7f6e\r\n#         x = random.randint(0, max_x) if max_x > 0 else 0\r\n#         y = random.randint(0, max_y) if max_y > 0 else 0\r\n        \r\n#         # \u83b7\u53d6ROI\u533a\u57df\u5e76\u786e\u4fdd\u4e0e\u7f29\u653e\u540e\u7684\u6b63\u6837\u672c\u5177\u6709\u76f8\u540c\u7684\u5f62\u72b6\r\n#         roi = neg_img[y:y+new_size[1], x:x+new_size[0]]\r\n        \r\n#         # \u786e\u4fddROI\u548cpos_img_resized\u5177\u6709\u76f8\u540c\u7684\u5f62\u72b6\u548c\u901a\u9053\u6570\r\n#         if roi.shape == pos_img_resized.shape:\r\n#             # \u6df7\u5408\u56fe\u50cf\r\n#             blended = cv2.addWeighted(roi, 0.3, pos_img_resized, 0.7, 0)\r\n#             neg_img[y:y+new_size[1], x:x+new_size[0]] = blended\r\n        \r\n#         return Image.fromarray(neg_img)\r\n\r\n#     @staticmethod\r\n#     def flip_image(image):\r\n#         '''\u56fe\u7247\u8f74\u5bf9\u79f0'''\r\n#         return Image.fromarray(np.array(image)[:, ::-1])\r\n    \r\n#     @staticmethod\r\n#     def mirror_half_image(image):\r\n#         img_array = np.array(image)\r\n    \r\n#         # \u83b7\u53d6\u56fe\u7247\u5c3a\u5bf8\r\n#         h, w = img_array.shape[:2]\r\n        \r\n#         # \u53d6\u5de6\u534a\u8fb9\r\n#         half_w = w // 2\r\n#         left_half = img_array[:, :half_w]\r\n        \r\n#         # \u6c34\u5e73\u7ffb\u8f6c\u5de6\u534a\u8fb9\u5f97\u5230\u53f3\u534a\u8fb9\r\n#         right_half = left_half[:, ::-1]\r\n        \r\n#         # \u62fc\u63a5\u4e24\u4e2a\u534a\u8fb9\r\n#         mirrored = np.concatenate([left_half, right_half], axis=1)\r\n        \r\n#         return Image.fromarray(mirrored)\r\n    \r\n\r\n# def augment_dataset(positive_images, negative_images):\r\n#     aug_utils = AugmentationUtils()\r\n#     augmented_data = []\r\n    \r\n#     # \u589e\u5f3a\u6b63\u6837\u672c\r\n#     for pos_img in positive_images:\r\n#         img = Image.open(pos_img).convert('RGB')\r\n#         # \u539f\u56fe\r\n#         augmented_data.append((img, 1))\r\n#         # \u989c\u8272\u906e\u7f69\r\n#         augmented_data.append((aug_utils.add_color_mask(img, True), 1))\r\n#         # \u8f74\u5bf9\u79f0\r\n#         augmented_data.append((aug_utils.flip_image(img), 1))\r\n#         # \u955c\u50cf\u4e00\u534a\r\n#         augmented_data.append((aug_utils.mirror_half_image(img), 1))\r\n#         # \u5d4c\u5165\u76f8\u540c\r\n#         img_id = random.randint(0, len(positive_images)-1)\r\n#         aaa = Image.open(positive_images[img_id]).convert('RGB')\r\n#         augmented_data.append((aug_utils.embed_same(aaa, img), 1))\r\n        \r\n    \r\n#     # \u589e\u5f3a\u8d1f\u6837\u672c\r\n#     for i, neg_img in enumerate(negative_images):\r\n#         img = Image.open(neg_img).convert('RGB')\r\n#         # \u539f\u56fe\r\n#         augmented_data.append((img, 0))\r\n#         # \u989c\u8272\u906e\u7f69\r\n#         augmented_data.append((aug_utils.add_color_mask(img, False), 0))\r\n#         # \u955c\u50cf\u4e00\u534a\r\n#         augmented_data.append((aug_utils.mirror_half_image(img), 0))\r\n#         # \u5d4c\u5165\u6b63\u6837\u672c\r\n#         pos_img = Image.open(positive_images[random.randint(0, len(positive_images)-1)]).convert('RGB')\r\n#         augmented_data.append((aug_utils.embed_positive_in_negative(pos_img, img), 1))\r\n#         # \u5d4c\u5165\u76f8\u540c\r\n#         img_id = random.randint(0, len(negative_images)-1)\r\n#         aaa = Image.open(negative_images[img_id]).convert('RGB')\r\n#         augmented_data.append((aug_utils.embed_same(aaa, img), 0))\r\n        \r\n\r\n        \r\n#     # # \u663e\u793a\u5e76\u4fdd\u5b58\r\n#     # for i, (img, label) in enumerate(augmented_data):\r\n#     #     # img.show()\r\n#     #     os.makedirs('aug_images', exist_ok=True)\r\n#     #     img.save(f'aug_images/aug_{i}.jpg')\r\n    \r\n#     # \u7edf\u8ba1\r\n#     print(f'Positive: {len([x for x, y in augmented_data if y == 1])}, Negative: {len([x for x, y in augmented_data if y == 0])}')\r\n#     return augmented_data\r\n\r\n\r\nclass LinearWarmUpCosineAnnealingLR(LambdaLR):\r\n    def __init__(self, optimizer, *, warmup_iters, max_learning_rate, min_lr, lr_decay_iters, **kwargs):\r\n        self.warmup_iters = warmup_iters\r\n        self.max_learning_rate = max_learning_rate\r\n        self.lr_decay_iters = lr_decay_iters\r\n        self.min_lr = min_lr\r\n        kwargs['optimizer'] = optimizer\r\n        kwargs['lr_lambda'] = self._step_inner\r\n        super().__init__(**kwargs)\r\n\r\n    def _step_inner(self, steps):\r\n        if steps < self.warmup_iters:\r\n            return self.max_learning_rate * steps / self.warmup_iters\r\n        elif steps < self.lr_decay_iters:\r\n            return self.min_lr + 0.5 * (1.0 + np.cos((steps - self.warmup_iters) / (self.lr_decay_iters - self.warmup_iters)*np.pi)) * (self.max_learning_rate - self.min_lr)\r\n        else:\r\n            return self.min_lr\r\n\r\n\r\ndef transform_img(img):\r\n    # \u5904\u7406\u56fe\u7247\r\n    img_np = np.array(img)\r\n    img_tensor = torch.from_numpy(img_np).permute(2, 0, 1)  # C, H, W\r\n    img_tensor = torch.nn.functional.interpolate(img_tensor.unsqueeze(0), size=(image_size, image_size), mode='bilinear', align_corners=False).squeeze(0)\r\n    # normalize\r\n    normalized_img = img_tensor.float() / 255.0\r\n    return normalized_img\r\n\r\n\r\ntransform = transforms.Compose([\r\n    transforms.Resize((image_size, image_size)),\r\n    transforms.ToTensor(),\r\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\r\n])\r\n\r\n\r\ndef transform_img_torchvision(data):\r\n    data['x'] = [transform(img.convert('RGB')) for img in data['image']]\r\n    return data\r\n\r\n\r\nlabel_mapping = {\r\n    'nailong': 0,\r\n    'emoji': 1,\r\n    'anime': 2,\r\n    'others': 3,\r\n    'long': 4\r\n}\r\n\r\ndef extract_datasets():\r\n    ds = load_dataset('refoundd/NailongClassification', cache_dir='data', split='train')\r\n    ds = ds.map(lambda x: {'label': label_mapping[x['label']]})\r\n    ds = ds.map(transform_img_torchvision, remove_columns=['image'], batched=True)\r\n    dataset = ds.train_test_split(test_size=0.2)\r\n    return dataset\r\n\r\ndataset = extract_datasets()\r\n\r\n\r\nclass NaiLongDataset(Dataset):\r\n    def __init__(self, mode='train'):\r\n        assert mode in ['train', 'test']\r\n        self.dataset = dataset[mode]\r\n\r\n    def __len__(self):\r\n        return len(self.dataset)\r\n\r\n    def __getitem__(self, idx):\r\n        item = self.dataset[idx]['x']\r\n        label = self.dataset[idx]['label']\r\n        return torch.tensor(item), torch.tensor(label)\r\n\r\n\r\n\r\nclass NaiLongTrainer(Trainer):\r\n    def __init__(self, config):\r\n        super().__init__(config)\r\n\r\n    def build_model(self):\r\n        # model = resnet18()\r\n        # model.fc = torch.nn.Linear(model.fc.in_features, 2)\r\n        # return model\r\n        return convnext_base(pretrained=False, num_classes=5)\r\n    \r\n    def build_criterion(self):\r\n        return torch.nn.CrossEntropyLoss()\r\n    \r\n    def build_scheduler(self):\r\n        return LinearWarmUpCosineAnnealingLR(self.optimizer, warmup_iters=self.config['warmup_iters'], max_learning_rate=self.config['max_learning_rate'], min_lr=self.config['min_lr'], lr_decay_iters=self.config['lr_decay_iters'])\r\n    \r\n    def build_dataloader(self, mode='train'):\r\n        dataset = NaiLongDataset(mode='train')\r\n        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\r\n    \r\n    def train_step(self, batch):\r\n        x, y = batch\r\n        x, y = x.to(device), y.to(device)\r\n        return self.criterion(self.model(x), y)\r\n    \r\n    def validate(self):\r\n        self.logger.info('Validating...')\r\n        self.model.eval()\r\n        dataloader = self.build_dataloader(mode='test')\r\n        acc = []\r\n        f1 = [[], []]\r\n        with torch.no_grad(): \r\n            for i, (x, y) in enumerate(dataloader):\r\n                x, y = x.to(device), y.to(device)\r\n                # print(f'Validation: {i}, {y}')\r\n                y_hat = self.model(x)\r\n                acc.append(torch.sum(torch.argmax(y_hat, dim=1) == y).item() / len(y))\r\n                f1[0].extend(y.cpu().tolist())\r\n                f1[1].extend(torch.argmax(y_hat, dim=1).cpu().tolist())\r\n            f1_scores = f1_score(f1[0], f1[1], average='macro')\r\n        return {'acc': sum(acc) / len(acc), 'f1': f1_scores}\r\n\r\n\r\ndef main():\r\n    config = {  # test\r\n        'model': 'convnext_tiny',\r\n        'checkpoint_dir': './checkpoints',\r\n        'tensorboard_dir': './tensorboard',\r\n        'device': 'cuda',\r\n        'enable_cudnn_benchmark': True,\r\n        'enable_amp': False,\r\n        'learning_rate': 1,  # \u542f\u52a8lr_scheduler \u8fd9\u91cc\u5fc5\u987b\u662f1\r\n        'betas': [0.9, 0.999],\r\n        'eps': 1e-8,\r\n        'enable_compile': False,\r\n        'weight_decay': 0.0,\r\n        'max_steps': 5000,\r\n        'max_grad_norm': 1.0,\r\n        'save_every': 500,\r\n        'gradient_accumulation_steps': 1,\r\n        'warmup_iters': 500,\r\n        'max_learning_rate': 1e-3,\r\n        'min_lr': 1e-4,\r\n        'lr_decay_iters': 1000\r\n    }\r\n    os.makedirs(config['checkpoint_dir'], exist_ok=True)\r\n    os.makedirs(config['tensorboard_dir'], exist_ok=True)\r\n    trainer = NaiLongTrainer(config)\r\n    trainer.train()\r\n\r\nif __name__ == '__main__':\r\n    # \u5220\u9664tensorboard\u4e0b\u7684\u6587\u4ef6, \u4f46\u4e0d\u5220\u9664\u6587\u4ef6\u5939\r\n    for i in os.listdir('./tensorboard'):\r\n        os.remove(os.path.join('./tensorboard', i))\r\n    # \u5220\u9664checkpoints\u4e0b\u7684\u6587\u4ef6\r\n    for i in os.listdir('./checkpoints'):\r\n        os.remove(os.path.join('./checkpoints', i))\r\n    try:\r\n        main()\r\n    except KeyboardInterrupt:\r\n        print('KeyboardInterrupt')\r\n        \r\n```\r\n\r\n</p>\r\n</details> \r\n\r\n### \u6570\u636e\u589e\u5e7f\r\n\r\n\u539f\u59cb\u6570\u636e\u96c6\u53ea\u6709\u4e24\u767e\u591a\u5f20\u56fe\u7247, \u8fd9\u4e2a\u65f6\u5019\u65e0\u6cd5\u907f\u514d\u7684\u8981\u505a\u6570\u636e\u589e\u5e7f, \u6269\u5c55 nailong \u6807\u7b7e\u7684\u6570\u636e, \u8fd9\u91cc\u56e0\u4e3a\u662f\u521d\u7248\u65b9\u6848, \u4e5f\u6ca1\u6709\u975e\u5e38\u7cbe\u7ec6\u7684\u589e\u5e7f\u65b9\u6848, \u8fd9\u91cc\u4f7f\u7528\u4e86\u4ee5\u4e0b\u51e0\u79cd\u65b9\u5f0f(\u4ee3\u7801\u5728\u5982\u4e0atrain.py\u4e2d):\r\n\r\n- \u7ed9\u56fe\u7247\u6dfb\u52a0\u989c\u8272\u906e\u7f69\r\n    \u8ba9\u6a21\u578b\u4e0d\u8981\u5c06\u9047\u5230\u9ec4\u8272\u7684\u5c31\u5224\u5b9a\u4e3a\u5976\u9f99\r\n- \u5728\u8d1f\u6837\u672c\u4e2d\u5d4c\u5165\u6b63\u6837\u672c\r\n    \u5f88\u7ecf\u5178\u7684\u589e\u5e7f\u6570\u636e\u7684\u624b\u6cd5\r\n- \u56fe\u7247\u8f74\u5bf9\u79f0\r\n- \u53d6\u56fe\u50cf\u7684\u4e00\u534a\u955c\u50cf\u7ffb\u8f6c\r\n\r\n### \u8bad\u7ec3\r\n\r\n#### \u53c2\u6570\u641c\u7d22\r\n\r\n\u867d\u7136\u662f\u4e2a\u4eba\u5c0f\u9879\u76ee, \u7b80\u5355\u7684\u53c2\u6570\u641c\u7d22\u4e0d\u80fd\u5c11, \u7ee7\u7eed\u4e0a\u9762\u5199\u7684 `trainer.py`, \u6211\u4e5f\u5199\u4e86\u4e00\u4e2a\u7b80\u5355\u7684 `hyperparameter_seacher.py` \u6765\u641c\u7d22\u8d85\u53c2\r\n\r\n<details><summary>hyperparameter_seacher.py</summary>\r\n<p>\r\n\r\n```python\r\nfrom trainer import Trainer\r\nimport optuna\r\n\r\n\r\nexample_config = {\r\n    'model': 'convnext_tiny',\r\n    'checkpoint_dir': './checkpoints',\r\n    'tensorboard_dir': './tensorboard',\r\n    'device': 'cuda',\r\n    'enable_cudnn_benchmark': True,\r\n    'enable_amp': False,\r\n    'learning_rate': 1e-4,\r\n    'betas': [0.9, 0.999],\r\n    'eps': 1e-8,\r\n    'enable_compile': False,\r\n    'weight_decay': 0.05,\r\n    'max_steps': 100,\r\n    'max_grad_norm': 1.0,\r\n    'save_every': 1000000,  # \u4e0d\u4fdd\u5b58\r\n    'gradient_accumulation_steps': 4\r\n}\r\n\r\nexample_search_config = {\r\n    'params': {\r\n        'learning_rate': {\r\n            'type': 'float',\r\n            'range': [1e-5, 1e-2],\r\n            'log': True\r\n        },\r\n        'gradient_accumulation_steps': {\r\n            'type': 'int',\r\n            'range': [1, 8],\r\n            'log': False\r\n        }\r\n    },\r\n    'if_save_info': False,\r\n    'n_trials': 10\r\n}\r\n\r\nclass HyperparameterSearcher:\r\n    def __init__(self, config, trainer):\r\n        assert isinstance(trainer, Trainer), 'trainer must be an instance of Trainer'\r\n        self.config = config\r\n        self.trainer = trainer\r\n        \r\n    def objective(self, trial):\r\n        search_params = self.config['params']\r\n        \r\n        for param_name, param_config in search_params.items():\r\n            if param_config['type'] == 'float':\r\n                self.trainer.config[param_name] = trial.suggest_float(\r\n                    param_name,\r\n                    param_config['range'][0],\r\n                    param_config['range'][1],\r\n                    log=param_config.get('log', False)\r\n                )\r\n            elif param_config['type'] == 'int':\r\n                self.trainer.config[param_name] = trial.suggest_int(\r\n                    param_name,\r\n                    param_config['range'][0],\r\n                    param_config['range'][1]\r\n                )\r\n            elif param_config['type'] == 'list':\r\n                self.trainer.config[param_name] = trial.suggest_categorical(\r\n                    param_name,\r\n                    param_config['range']\r\n                )\r\n            else:\r\n                raise ValueError(f'Unsupported parameter type: {param_config['type']}, only support float and int')\r\n        \r\n        self.trainer.setup_training()\r\n        self.trainer.train()\r\n        metric = self.trainer.validate()\r\n        if 'acc' not in metric:\r\n            raise ValueError('metric must contain 'acc'')\r\n        return -metric['acc']  # only support maximizing acc\r\n    \r\n    def search(self):\r\n        study = optuna.create_study(direction='maximize')\r\n        study.optimize(self.objective, n_trials=self.config['n_trials'])\r\n        print('Best params:', study.best_params)\r\n        print('Best value:', -study.best_value)\r\n        if self.config['if_save_info']:\r\n            study.trials_dataframe().to_csv('./output/optuna_results.csv')\r\n        return study.best_params\r\n    \r\ndef main():\r\n    \r\n    pass\r\n\r\nif __name__ == '__main__':\r\n    try:\r\n        main()\r\n    except KeyboardInterrupt:\r\n        pass\r\n```\r\n\r\n</p>\r\n</details> \r\n\r\n\u8d85\u53c2\u641c\u7d22\u9700\u8981 trainer \u7684\u914d\u5408, \u4f7f\u7528\u4e86\u4e0e\u6a21\u578b\u65e0\u5173\u7684 optuna \u6765\u8dd1\u7ed9\u5b9a\u8303\u56f4\u7684\u8d85\u53c2\u503c, \u8fd9\u4e2a\u65f6\u5019\u53ef\u4ee5\u7ed9trainer\u4e00\u4e2a\u6bd4\u8f83\u5bb9\u6613\u8bad\u7ec3\u7684\u8d85\u53c2\u8bbe\u7f6e(\u77ed\u7684epoch\u7b49), \u540c\u65f6\u5173\u95ed\u4fdd\u5b58\u6a21\u5f0f\r\n\r\n\u6211\u4e5f\u5199\u4e86\u4e2a\u7b80\u5355\u7684\u8d85\u53c2\u641c\u7d22\u7684\u4f8b\u5b50\r\n\r\n<details><summary>hyperparameter_seacher\u4f7f\u7528\u6848\u4f8b</summary>\r\n<p>\r\n\r\nfrom example_trainer import Cifer10Trainer\r\nfrom hyperparameter_seacher import HyperparameterSearcher\r\n\r\nclass Cifer10HyperparameterSearcher(HyperparameterSearcher):\r\n    def __init__(self, config, trainer):\r\n        super().__init__(config, trainer)\r\n\r\n\r\ndef main():\r\n    search_config = {\r\n        'params': {\r\n            'learning_rate': {\r\n                'type': 'float',\r\n                'range': [1e-5, 1e-2],\r\n                'log': True\r\n            },\r\n            'gradient_accumulation_steps': {\r\n                'type': 'int',\r\n                'range': [1, 8],\r\n                'log': False\r\n            }\r\n        },\r\n        'if_save_info': True,\r\n        'n_trials': 10\r\n    }\r\n\r\n    trainer_config = {\r\n        'model': 'resnet18',\r\n        'checkpoint_dir': './checkpoints',\r\n        'tensorboard_dir': './tensorboard',\r\n        'device': 'cuda',\r\n        'enable_cudnn_benchmark': True,\r\n        'enable_amp': False,\r\n        'learning_rate': 1e-3,\r\n        'betas': [0.9, 0.999],\r\n        'eps': 1e-8,\r\n        'enable_compile': False,\r\n        'weight_decay': 0.05,\r\n        'max_steps': 500,\r\n        'max_grad_norm': 1.0,\r\n        'save_every': 10000,  # large than max_steps, no save\r\n        'gradient_accumulation_steps': 4,\r\n        'batch_size': 32\r\n    }\r\n    trainer = Cifer10Trainer(trainer_config)\r\n    searcher = Cifer10HyperparameterSearcher(search_config, trainer)\r\n    best_params = searcher.search()\r\n    print(best_params)\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n</p>\r\n</details> \r\n\r\n> \u641c\u7d22\u4e0a\u9762\u90a3\u4e2a\u4f8b\u5b50\u4e2d\u7684\u5408\u9002\u7684\u8d85\u53c2\u6570\r\n\r\n\u5728\u51c6\u5907\u597d\u8fd9\u4e9b\u540e, \u7f16\u5199\u6211\u4eec\u9879\u76ee\u7684\u8d85\u53c2\u641c\u7d22\u5668\r\n\r\n<details><summary>\u4e4b\u540e\u8865\u5145</summary>\r\n<p>\r\n\r\n\r\n\r\n</p>\r\n</details> \r\n\r\n> \u641c\u7d22\u51fa\u6765\u7684\u6700\u4f73\u8d85\u53c2\u662f\u4e00\u4e2a\u5f88\u957f\u7684\u5c0f\u6570, \u56db\u820d\u4e94\u5165\u5408\u9002\u7684\u4f4d\u6570\u5373\u53ef\r\n\r\n#### \u7b2c\u4e00\u6b21\u8bad\u7ec3\r\n\r\n\u5728\u51c6\u5907\u597d\u540e, \u5f00\u59cb\u7b2c\u4e00\u6b21\u8bad\u7ec3\r\n\r\n\u5728\u8f83\u65b0\u7684GPU\u4e0b, \u8bad\u7ec3\u4ee5\u524d\u8f83\u5c0f\u7684\u6a21\u578b\u53ef\u8c13\u964d\u7ef4\u6253\u51fb, \u4e0d\u5230\u4e00\u4e2a\u5c0f\u65f6\u8bad\u7ec3\u5b8c\u6bd5\r\n\r\n\u7136\u800c, \u7b2c\u4e00\u4e2a\u95ee\u9898\u51fa\u6765\u4e86\r\n\r\nacc\u5f88\u9ad8, f1\u5f88\u4f4e\r\n\r\n\u7f16\u5199\u6d4b\u8bd5\u4ee3\u7801:\r\n\r\n<details><summary>test.py(\u975e\u521d\u7248\u4ee3\u7801, \u4ec5\u4f9b\u53c2\u8003)</summary>\r\n<p>\r\n\r\n```python\r\nfrom sklearn.metrics import f1_score\r\nimport torch\r\n\r\nfrom model import convnext_base\r\nfrom PIL import Image\r\nimport numpy as np\r\nfrom glob import glob\r\nfrom torchvision import transforms\r\n\r\ndevice = 'cuda'\r\nimage_size = 224\r\n\r\nmodel = convnext_base(pretrained=False, num_classes=5).to(device)\r\n# model = resnet18()\r\n# model.fc = torch.nn.Linear(model.fc.in_features, 2)\r\ncheckpoint = torch.load('./checkpoints/best.pt', map_location=device)\r\nmodel.load_state_dict(checkpoint['model'])\r\n\r\ntransform = transforms.Compose([\r\n    transforms.Resize((image_size, image_size)),\r\n    transforms.ToTensor(),\r\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\r\n])\r\n\r\ndef transform_img(img):\r\n    # \u5904\u7406\u56fe\u7247\r\n    img_np = np.array(img)\r\n    img_tensor = torch.from_numpy(img_np).permute(2, 0, 1)  # C, H, W\r\n    img_tensor = torch.nn.functional.interpolate(img_tensor.unsqueeze(0), size=(image_size, image_size), mode='bilinear', align_corners=False).squeeze(0)\r\n    # normalize\r\n    normalized_img = img_tensor.float() / 255.0\r\n    return normalized_img\r\n\r\n\r\ndef get_input_images(image_path):\r\n    img = Image.open(image_path).convert('RGB')\r\n    img = transform(img)\r\n    return torch.tensor(img).to(device).unsqueeze(0)\r\n\r\nmodel.eval()\r\n\r\n# \u5bfc\u51faonnx\r\ninput_names = ['input']\r\noutput_names = ['output']\r\ndynamic_axes = {\r\n    'input': {0: 'batch_size'},  # \u8f93\u5165\u7684\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u662f\u52a8\u6001\u7684\r\n    'output': {0: 'batch_size'}  # \u8f93\u51fa\u7684\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u662f\u52a8\u6001\u7684\r\n}\r\ntorch.onnx.export(model, torch.randn(1, 3, 224, 224).to(device), 'model.onnx', input_names=input_names, output_names=output_names, dynamic_axes=dynamic_axes, opset_version=11)\r\n\r\nwith torch.no_grad():\r\n    # image = torch.randn(1, 3, 256, 256)\r\n    print(torch.softmax(model(get_input_images('1.jpg')), dim=1))\r\n    input()\r\n    print(torch.softmax(model(get_input_images('3.jpg')), dim=1))\r\n    input()\r\n    acc = []\r\n    f1 = [[], []]\r\n    \r\n    \r\n    for file in glob('./datasets/nailong/*'):\r\n        y_hat, y = model(get_input_images(file)), torch.tensor([0]).to(device)\r\n        acc.append(torch.sum(torch.argmax(y_hat, dim=1) == y).item() / len(y))\r\n        f1[0].append(y.cpu().tolist()[0])\r\n        f1[1].append(torch.argmax(y_hat, dim=1).cpu().tolist()[0])\r\n        \r\n\r\n    # for file in glob('./datasets/cifer10/*'):\r\n    #     y_hat, y = model(get_input_images(file)), torch.tensor([3]).to(device)\r\n    #     acc.append(torch.sum(torch.argmax(y_hat, dim=1) == y).item() / len(y))\r\n    #     f1[0].append(y.cpu().tolist()[0])\r\n    #     f1[1].append(torch.argmax(y_hat, dim=1).cpu().tolist()[0])\r\n\r\nprint(sum(acc) / len(acc))\r\nf1_scores = f1_score(f1[0], f1[1], average='macro')\r\nprint(f1_scores)\r\n```\r\n\r\n</p>\r\n</details> \r\n\r\n\r\n\u53d1\u73b0 acc \u4e0e f1 \u7684\u503c\u975e\u5e38\u4f4e(\u767e\u5206\u4e4b10\u5230\u767e\u5206\u4e4b20\u9644\u8fd1)\r\n\r\n\u8fd9\u4e2a\u65f6\u5019\u67e5\u770b\u6a21\u578b\u7684\u8f93\u51fa, \u53d1\u73b0\u6a21\u578b\u7684\u8f93\u51fa\u63a5\u8fd1\u521d\u59cb\u5316\u7684\u8f93\u51fa(softmax\u540e), \u6a21\u578b\u6ca1\u6709\u600e\u4e48\u88ab\u8bad\u7ec3\r\n\r\n\u8fd9\u4e2a\u65f6\u5019\u6000\u7591\u662f\u8bad\u7ec3\u4ee3\u7801\u51fa\u4e86\u95ee\u9898, \u7136\u800c cifer10 \u7684 \u8bad\u7ec3\u5e76\u6ca1\u6709\u4ec0\u4e48\u95ee\u9898\r\n\u68c0\u67e5\u6570\u636e\u589e\u5e7f\u4ee3\u7801, \u67e5\u770b\u589e\u5e7f\u540e\u7684\u56fe\u7247, \u53d1\u73b0\u589e\u5e7f\u505a\u7684\u4e0d\u662f\u5f88\u597d, \u6b63\u6837\u672c\u5185\u5d4c\u8d1f\u6837\u672c\u6ca1\u5d4c\u597d\r\n\r\n\u5728\u7ecf\u8fc7\u4fee\u6539\u540e, \u91cd\u542f\u8bad\u7ec3\r\n\u7136\u800c, \u95ee\u9898\u53d8\u6210\u4e86, \u6a21\u578b\u7684\u8f93\u51fa\u63a5\u8fd1(0.5, 0.5)(\u4e8c\u5206\u7c7b\u4efb\u52a1)\r\n\r\n\u8ddf\u4eba\u8ba8\u8bba\u540e, \u8ba4\u4e3a\u662f\u6570\u636e\u96c6\u96be\u5ea6\u592a\u5927, \u68c0\u67e5\u8868\u60c5\u5305\u6570\u636e\u96c6, \u90fd\u662f\u4e00\u4e9b\u5206\u5e03\u4e0e nailong \u6570\u636e\u5dee\u5f02\u5f88\u5927\u7684\u56fe\u7247. \r\n\r\n> \u975e\u4e25\u683c\u63a8\u7406, \u7eaf\u8111\u6d4b\r\n\u6a21\u578b\u53d1\u73b0, \u7ed9\u4e00\u5f20\u65b0\u7684\u56fe\u7247\u9884\u6d4b nailong \u7c7b, \u8fd8\u662f\u5176\u4ed6\u7c7b, \u90fd\u4f1a\u5bfc\u81f4loss\u4e0a\u5347, \u4e8e\u662f\u5e72\u8106\u6446\u70c2\u4e71\u731c,  \u6700\u7ec8\u7684\u6982\u7387\u5206\u5e03\u4f1a\u8f93\u51fa\u6570\u636e\u5206\u5e03, \u7ecf\u8fc7\u6570\u636e\u589e\u5e7f\u540e\u7684\u6570\u636e\u6070\u597d\u662f\u4e24\u7c7b 1:1, \u6a21\u578b\u9000\u5316\u6210\u7edf\u8ba1\u6570\u636e\u96c6\u4e86\r\n\r\n\u5bfc\u81f4\u8fd9\u4e2a\u7684\u6700\u76f4\u63a5\u539f\u56e0\u662f\u8f93\u5165\u7279\u5f81\u4e0d\u591f, \u5230\u56fe\u50cf\u5206\u7c7b\u5c31\u662f\u6a21\u578b\u627e\u4e0d\u5230\u51b3\u5b9a\u56fe\u7247\u5206\u7c7b\u7684\u6a21\u5f0f\r\n\r\n\u4e8e\u662f, \u7b2c\u4e00\u9636\u6bb5\u7684\u8bad\u7ec3\u7ed3\u675f\u4e86\r\n\r\n#### \u7b2c\u4e8c\u6b21\u8bad\u7ec3\r\n\r\n\u5728\u6570\u636e\u96c6\u4f5c\u8005\u4e0d\u65ad\u7684\u52aa\u529b\u4e0b, nailong \u6570\u636e\u96c6\u6709\u4e86\u4e00\u4e9b\u5b8c\u5584, \u4e3b\u8981\u7684\u5b8c\u5584\u70b9\u5728\u4e8e: \r\n1. \u65b0\u6dfb\u66f4\u591a nailong\r\n2. \u4e0d\u662f\u4e8c\u5206\u7c7b\u4e86, \u65b0\u589e\u4e86\u8868\u60c5\u5305\u5206\u7c7b, \u52a8\u753b\u5206\u7c7b\u7b49\u4e94\u5206\u7c7b, \u4e0d\u8fc7\u4e0d\u540c\u7c7b\u522b\u7684\u6570\u636e\u6570\u91cf\u5dee\u5f02\u5f88\u5927(\u4e24\u4e2a\u6570\u91cf\u7ea7)\r\n3. \u52a0\u5165\u4e86\u4e00\u4e9b corner case, \u6bd4\u5982 \u85e4\u7530\u7434\u97f3\u7b49\u5176\u4ed6\u989c\u8272\u4e3a\u9ec4\u8272\u7684\u56fe\u50cf\r\n\r\n\u56e0\u4e3a\u7b2c\u4e00\u6b21\u8bad\u7ec3\u4ee3\u7801\u5df2\u7ecf\u5199\u597d\u4e86, \u6539\u8d77\u6765\u4e5f\u4e0d\u662f\u5f88\u9ebb\u70e6, \u53ea\u9700\u8981\u6362\u4e2a\u6570\u636e\u96c6\u5b9a\u4e49\u4e0e\u8bfb\u53d6. \u4f5c\u8005\u7684\u6570\u636e\u96c6\u653e\u5728 huggingface \u4e0a, \u4e8e\u662f\u6211\u4eec\u4f7f\u7528 datasets \u8fdb\u884c\u8bfb\u53d6.\r\n\r\n> \u6211\u4e5f\u4e0d\u77e5\u9053\u662f\u4e0d\u662f\u6211\u5199\u7684\u95ee\u9898, datasets\u8bfb\u8d77\u6765\u5f88\u6162, dataloader \u540e, \u4f1a\u628a label \u81ea\u52a8\u53d8\u6210torch.tensor\u683c\u5f0f, \u4f46\u662f n, c, h, w \u683c\u5f0f\u7684\u56fe\u7247\u53ea\u4f1a\u628a w \u7ef4\u5ea6\u53d8\u6210 torch.tensor \u683c\u5f0f, \u5176\u4ed6\u7ef4\u5ea6\u8fd8\u662f List, \u9700\u8981\u5728 dataset \u7c7b\u5b9a\u4e49\u7684\u65f6\u5019\u4f7f\u7528 __getitem__() \u5c06\u6570\u636e\u63d0\u524d\u53d8\u4e3a torch.tensor\r\n> \u7136\u540e\u4e0d\u652f\u6301\u591a\u7ebf\u7a0b\u8bfb\u53d6(\u4f1a\u5361\u4f4f), \u5355\u7ebf\u7a0b\u8bfb\u53d6\u8bfb\u8d77\u6765\u5f88\u6162, gpu \u7684 cuda \u5448\u73b0\u5c16\u523a\u72b6\r\n> \u7136\u540e, dataset \u7684\u8bfb\u53d6**\u8981\u5148**\u8bfb\u53d6 id \u518d\u8bfb\u53d6 x \u8ddf label\r\n> \u6ca1\u600e\u4e48\u7528\u8fc7 dataset, \u8fd9\u6b21\u5c5e\u5b9e\u662f\u5b66\u5230\u4e86\r\n\r\n\u4fee\u6539\u597d\u540e\u6570\u636e\u52a0\u8f7d\u7684\u4ee3\u7801\u540e\u5e76\u6ce8\u91ca\u6389\u5148\u524d\u7684\u6570\u636e\u589e\u5e7f\u4ee3\u7801\u540e(\u540e\u7eed\u7814\u7a76), \u7b2c\u4e8c\u6b21\u8bad\u7ec3\u5f00\u59cb\u4e86\r\n\r\n\u8fd9\u6b21\u7ed3\u679c\u597d\u8fc7\u5934\u4e86\r\n\u6a21\u578b\u7684 loss \u6536\u655b\u5230\u4e86 $1e^{-5}$, acc\u8ddff1\u66f4\u662f\u5230\u8fbe\u4e86 $100%$\r\n\r\n\u4f7f\u7528\u6d4b\u8bd5\u4ee3\u7801\u7b80\u5355\u6d4b\u8bd5, \u53d1\u73b0\u5728\u6570\u636e\u96c6\u7684\u6570\u636e\u90fd\u80fd\u5b8c\u7f8e\u5206\u7c7b, \u4e0d\u5728\u6570\u636e\u96c6\u7684\u5206\u7c7b\u53ea\u8981\u5206\u4e0d\u51fa\u662f\u5976\u9f99\u5373\u53ef. \u68c0\u67e5\u6a21\u578b\u8f93\u51fa\u6743\u91cd, \u4e5f\u6ca1\u5565\u95ee\u9898, \u770b\u8d77\u6765\u662f\u5b8c\u7f8e\u4e86?\r\n\r\n\u7136\u800c \u8fd9\u5f20\u56fe\u8fd8\u662f\u7ed9\u4e86\u6a21\u578b\u4e00\u62f3\r\n\r\n<details><summary>\u56fe</summary>\r\n<p>\r\n\r\n![22](https://github.com/user-attachments/assets/3cb2b111-e01f-44dd-8e71-0509ab2bb6c0)\r\n\r\n</p>\r\n</details> \r\n\r\n\r\n\u4ed6\u4f1a\u8bc6\u522b\u6210 nailong, \u4e0d\u8fc7\u6211\u89c9\u5f97\u95ee\u9898\u4e0d\u5927(\u786e\u5b9e\u6709\u4eba\u628a\u4ed6\u62bd\u8c61\u7684\u8ba4\u6210 nailong)\r\n\r\n\u4e0a\u9762\u7684 `test.py` \u4e2d \u5199\u4e86onnx\u5bfc\u51fa\u7684\u4ee3\u7801, \u652f\u6301\u4efb\u610f batch \u7684\u8f93\u5165(\u89e3\u9501\u4e86 n, c, h, w \u7684 n \u7ef4\u5ea6)\r\n\r\n\u7b80\u5355\u7f16\u5199onnx\u63a8\u7406\u4ee3\u7801\r\n\r\n<details><summary>onnx_inference.py</summary>\r\n<p>\r\n\r\n```python\r\n# from torchvision import transforms\r\nimport onnxruntime as ort\r\nfrom PIL import Image\r\nimport numpy as np\r\n\r\nimg_size = 224\r\n\r\n# transform = transforms.Compose([\r\n#     transforms.Resize((img_size, img_size)),\r\n#     transforms.ToTensor(),\r\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\r\n# ])\r\n\r\ndef transform_img(img: Image, image_size=224, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\r\n    img = img.convert('RGB').resize((image_size, image_size), Image.Resampling.LANCZOS)\r\n    img = np.array(img)\r\n    img = (img / 255 - mean) / std\r\n    img = img.transpose((2, 0, 1))\r\n    img = np.expand_dims(img, axis=0)\r\n    return img.astype(np.float32)\r\n\r\n\r\nlabel_mapping = {\r\n    'nailong': 0,\r\n    'emoji': 1,\r\n    'anime': 2,\r\n    'others': 3,\r\n    'long': 4\r\n}\r\n\r\nreverse_label_mapping = {v: k for k, v in label_mapping.items()}\r\n\r\nmodel_path = 'model.onnx'\r\nsession = ort.InferenceSession(model_path)\r\n\r\nimage_path = '3.jpg'\r\nimage = Image.open(image_path).convert('RGB')\r\n# image = transform(image).unsqueeze(0).numpy()\r\nimage = transform_img(image)\r\n\r\n# \u8fd0\u884c\u63a8\u7406\r\ninput_name = session.get_inputs()[0].name\r\noutput_name = session.get_outputs()[0].name\r\noutputs = session.run([output_name], {input_name: image})\r\n\r\n# \u83b7\u53d6\u5206\u7c7b\u7ed3\u679c\r\noutput = outputs[0]\r\npredicted_class = np.argmax(output, axis=1)\r\npredicted_label = reverse_label_mapping[predicted_class[0]]\r\n\r\nprint(f'Predicted class: {predicted_label}')\r\n\r\n```\r\n\r\n</p>\r\n</details> \r\n\r\n> \u8bad\u7ec3\u7684\u65f6\u5019\u5f15\u5165\u4e86 torchvision \u7684 transforms, \u8fd9\u91cc\u4e3a\u4e86\u51cf\u5c11\u4f9d\u8d56, \u9009\u62e9\u624b\u52a8\u5b9e\u73b0, \u6709\u9700\u8981\u4e5f\u53ef\u4ee5\u81ea\u884c\u53d6\u6d88\u6ce8\u91ca\u5e76\u4fee\u6539\r\n\r\n### \u90e8\u7f72\r\n\r\nTODO\r\n\u3002", "top": 0, "createdAt": 1732625582, "style": "", "script": "<script>MathJax = {tex: {inlineMath: [[\"$\", \"$\"]]}};</script><script async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2024-11-26", "dateLabelColor": "#bc4c00"}}, "singeListJson": {}, "labelColorDict": {"blog": "#f9d0c4", "game": "#7B3EB2", "help wanted": "#008672", "idea": "#fef2c0", "question": "#d876e3", "wontfix": "#ffffff"}, "displayTitle": "FairyOwO \u7684 Blog", "faviconUrl": "https://github.githubassets.com/favicons/favicon.svg", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "primerCSS": "<link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />", "homeUrl": "https://FairyOwO.github.io", "prevUrl": "disabled", "nextUrl": "disabled"}