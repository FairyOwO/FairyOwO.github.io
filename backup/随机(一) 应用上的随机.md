> 写在前面: 本文探讨的真随机与伪随机的定义: 只要满足随机性检测的, 即为真随机. 其余皆为伪随机
> 区别于通过物理现象或不可预测的事件产生的真随机数, 与程序生成的伪随机数, 两者将在下一篇介绍
> 常见的伪随机有: 抽卡低保算法等

## 引言

大多数人知道"随机", 但在实际生活中, 缺混淆相关概念, 这里做一个简单的介绍, 本文希望从统计测试视角介绍随机表现性质.

在应用篇视角下，我们关注的是一个数字序列在特定场景中“表现”得如何，而非其“出身”如何。这种视角更侧重于随机数序列的实用性和表观行为。

首先我们需要对本文定义的"真随机"进行一个定义: 任何能够成功通过一套预先设定的、公认的随机性检测标准的数字序列, 无论其生成源头是物理过程还是确定性算法, 均可被视为"应用层面上的真随机". 只要一个序列在统计学上表现出了足够的随机特性, 没有显露出可被识别的模式或规律性, 那么在应用层面就可以信赖其随机.

相应地, "应用层面上的伪随机"则指那些未能通过这些严格统计检验的序列, 或者其设计初衷并非追求统计学上的完美随机性. 一个典型的例子便是游戏中常见的"抽卡保底"算法, 这类算法的设计目标是为了平衡玩家体验或实现特定的商业逻辑, 而非生成在统计学意义上无偏的随机序列. 因此, 此定义下的"伪随机"范畴, 不仅包括了那些因算法缺陷导致统计特性不佳的序列, 也囊括了那些为了特定应用目标而"有意为之"的不完全随机系统.

这一种定义方式, 将能够通过所有相关统计测试的伪随机数生成器 (PRNG) 的输出在应用层赋予"真随机"的地位, 方便我们之后讨论相关内容.

## 随机性检验

为了客观评估一个数字序列是否"足够随机", 学术界和工业界发展出了一系列随机性测试方法和标准. 这些测试的必要性在于, 人眼的直观判断往往不可靠, 需要借助严格的数学工具来甄别序列中可能存在的非随机模式. 

目前国际上公认的随机性测试套件主要包括美国国家标准与技术研究院 (NIST) 发布的SP 800-22测试集和由George Marsaglia教授开发的Diehard测试集。这些测试套件通常包含多种独立的统计检验方法, 例如:

频率测试: 检测序列中的数字的个数是否大致相等
块内频率测试: 分块后, 检测各块内各个数字的比例
游程测试: 检查序列中连续相同比特(游程)的长度分布是否符合随机序列的期望
等, 详情可见[https://en.wikipedia.org/wiki/Statistical_randomness](https://en.wikipedia.org/wiki/Statistical_randomness)

在这些测试中, P值(P-value)是一个核心概念. 它表示在原假设 (即"被测序列是随机的") 成立的前提下. 观测到当前检验统计量或更极端情况的概率 1. 通常, 如果一个P值非常小 (例如小于预设的显著性水平α, 如0.01或0.001), 则认为有足够证据拒绝原假设, 即该序列未能通过此项特定测试, 表现出非随机性 2 .

> 统计学假设检验之 p检验

需要强调的是, "通过测试"并不意味着绝对证明了序列是"真随机"的. 它仅仅表明, 在所实施的统计检验方法下, 该序列没有暴露出明显的非随机模式或统计偏差. 任何测试套件都是有限的, 一个序列即使通过了所有已知的测试, 也无法保证它在未来面对新的、更精密的测试方法时依然表现完美. 统计决策本身也带有概率性, 存在错误地拒绝一个真随机序列 (第一类错误) 或错误地接受一个非随机序列 (第二类错误) 的风险. 此外, 随机性测试标准本身也在不断发展和完善, 对随机数生成器的要求也随之提高.

## "应用伪随机"

我们首先建立一个 "真随机" 的标准, 假设在一个游戏中, 抽中某个稀有物品的概率是固定的 5%, 每次抽卡都是一次独立的伯努利试验. 在这种情况下, 玩家首次抽中该物品需要的抽卡次数 $X$ 符合几何分布:

$$
P(X=x) = p(1-p)^{x-1}, x = 1, 2, 3 \dots
$$

此时, 期望为 $E[X] = \frac{1}{p}$. 在 $p=0.05$ 时, 期望为 $E[X] = \frac{1}{p} = 20$, 平均而言, 玩家需要抽20次才能获得此物品.

> $E[X] = \sum_{x=1}^\infty x \cdot P(X=x)=\sum_{x=1}^\infty xp(1-p)^{x-1} = \frac{1}{p}$

方差为: $Var(X) = \frac{1-p}{p^2}$

在 $p = 0.05$ 时, 方差为 $Var(X) = \frac{1-0.05}{0.05^2} = 380$

巨大的方差意味着巨大的波动, 在前二十次抽出的概率是 $1-(1-0.05)^20 = 0.6415$, 也就是说, 近四成的玩家无法再期望中抽到
> 在前 100 次抽出的概率是 $1-(1-0.05)^100 = 0.9941$, 大约在200个人中有一个100抽内无法抽到

为了解决 "真随机" 带来的体验问题, "伪随机" 应运而生, "伪随机" 改变了概率分布, 让概率分布不再均匀, 从而实现 减少方差.

以伪随机分布PRD(Pseudo Random Distribution)算法为例, 事件发生的概率随着失败次数的增加而线性增长.

以我们的例子为例, 为了达到5%的长期概率, 假设设定了一个初始概率常数 $C= 0.38%$, 那么
- $P(1) = 0.0038 \times 1$
- 如果第一次失败, $P(2) = 0.0038 \times 2$
- 如果第二次失败, $P(3) = 0.0038 \times 3$
- ...
- 如果第十九次失败, $P(19) = 0.0038 \times 19$

可以预想到的是, 在200次左右, 概率将到达 100%

PRD算法的C经过巧妙的设计, 保证其长期期望与真随机下的期望值相等, 在上面这个例子中, 期望抽卡次数仍然是20次. 然而引入了线性增长的概率, 大大降低了方差, 概率分布更集中于期望值附近. 极早期就抽中(例如第1抽)的概率被人为降低了(从5%降至0.38%), 而连续失败的"惩罚"也减小了, 因为概率会稳步提升, 使得极度"脸黑"的情况变得非常罕见. 这使得玩家的获得体验更加稳定和可预测, 有效避免了纯粹随机可能带来的挫败感 . 

无论是明确的抽卡保底还是PRD, 它们都是设计者在随机性与确定性之间进行权衡的产物. 其目的在于服务特定的应用目标, 例如游戏平衡, 玩家留存和商业化, 而非追求纯粹的统计随机性 . 它们是"应用伪随机"的绝佳范例, 其"缺陷"恰恰是其设计意图所在. 


## 参考
1. [https://zhuanlan.zhihu.com/p/22987906](https://zhuanlan.zhihu.com/p/22987906)
2. [https://blog.xav1er.com/p/probability-theory-in-gacha-games/](https://blog.xav1er.com/p/probability-theory-in-gacha-games/)
3. [https://blog.oonne.com/detail/pseudo-random](https://blog.oonne.com/detail/pseudo-random)