<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>FairyOwO 的 Blog</title><link>https://FairyOwO.github.io</link><description>记录一些琐事</description><copyright>FairyOwO 的 Blog</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://github.githubassets.com/favicons/favicon.svg</url><title>avatar</title><link>https://FairyOwO.github.io</link></image><lastBuildDate>Tue, 13 Jan 2026 07:18:19 +0000</lastBuildDate><managingEditor>FairyOwO 的 Blog</managingEditor><ttl>60</ttl><webMaster>FairyOwO 的 Blog</webMaster><item><title>2025 Advent of Code 复盘与答案</title><link>https://FairyOwO.github.io/post/2025%20Advent%20of%20Code%20-fu-pan-yu-da-an.html</link><description>[Advent of Code](https://adventofcode.com/)

使用 python 编写, 没有整理代码, 所以非常乱(变量乱取名, 没有注释, 逻辑奇怪, 并非最佳实现)
可以使用 gpt 相关工具辅助查看

&gt; 如果没有特意说明, 变量 aaa 统一存放所有原始字符串

&gt; 今年的 ai 翻译工具好了很多 翻译的更容易懂了 许多

&gt; 今年只有12题

## 第一题

[https://adventofcode.com/2025/day/1](https://adventofcode.com/2025/day/1)

### 1题

钟表 一共100个刻度(0-99) 往左转或者往右转 n 个度数
刚好转到 0 的次数是多少
模拟即可

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
counts = 0
now = 50
for line in aaa.splitlines():
    direction = line[0]
    steps = int(line[1:])
    if direction == 'L':
        now -= steps
    elif direction == 'R':
        now += steps
    
    now = (now + 100) % 100
    if now == 0:
        counts += 1

print(counts)
```

&lt;/p&gt;
&lt;/details&gt; 

### 2题

在1题的基础上 变成旋转过程中 经过0的次数 (转到0也算)

继续模拟 注意有的会转超过一圈

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python

counts = 0
now = 50
for line in aaa.splitlines():
    is_zero_now = (now == 0)
    direction = line[0]
    steps = int(line[1:])

    cricles = steps // 100
    counts += cricles
    steps = steps % 100

    if direction == 'L':
        now -= steps
    elif direction == 'R':
        now += steps
    
    if not is_zero_now:
        if now &lt;= 0:
            counts += 1
    
    if now &gt;= 100:
        counts += 1

    now = (now + 100) % 100
    # if now == 0:
    #     counts += 1

print(counts)
```

&lt;/p&gt;
&lt;/details&gt; 

## 第二题

[https://adventofcode.com/2025/day/2](https://adventofcode.com/2025/day/2)

### 1题

给定范围 找到仅由重复两次的数序数字组成的任何 ID

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
input_list = aaa.split(',')

ans = 0
for range_pair in input_list:
    start, end = map(int, range_pair.split('-'))
    for number in range(start, end + 1):
        str_number = str(number)
        if len(str_number) % 2 != 0:
            # if len(set(str_number)) == 1:
            #     ans += number
            continue
        else:
            mid = len(str_number) // 2
            first_half = str_number[:mid]
            second_half = str_number[mid:]
            if first_half == second_half:
                ans += number
                continue
print(ans)
```

&lt;/p&gt;
&lt;/details&gt; 

### 2题

在1题的基础上 重复两次变成重复若干次

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
input_list = aaa.split(',')

def get_factors(n: int) -&gt; list:
    factors = []
    for i in range(2, int(sqrt(n)) + 1):
        if n % i == 0:
            factors.append([i, n // i])
            
    factors.append([1, n])
    return factors


ans = 0
for range_pair in input_list:
    start, end = map(int, range_pair.split('-'))
    for number in range(start, end + 1):
        str_number = str(number)
        if len(str_number) &lt; 2:
            continue
            
        factors = get_factors(len(str_number))
        for f1, f2 in factors:
            if str_number[:f1] * f2 == str_number:
                # print(number)
                ans += number
                break
            if 1 not in [f1, f2]:
                if str_number[:f2] * f1 == str_number:
                    # print(number)
                    ans += number
                    break

print(ans)
```

&lt;/p&gt;
&lt;/details&gt; 

## 第三题

[https://adventofcode.com/2025/day/3](https://adventofcode.com/2025/day/3)

### 1题

给定一串数字 这些数字在里面取 相对位置固定的两个数字 来组成最大的数

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
ans = 0
for line in aaa.split('\n'):
    max_left_num = int(line[0])
    max_left_index = 0
    for i, c in enumerate(line[1:-1]):
        if int(c) &gt; max_left_num:
            max_left_num = int(c)
            max_left_index = i + 1
    max_right_num = int(line[-1])
    for j in range(len(line) - 2, max_left_index, -1):
        if int(line[j]) &gt; max_right_num:
            max_right_num = int(line[j])
    
    ans += max_left_num * 10 + max_right_num

print(ans)
```

&lt;/p&gt;
&lt;/details&gt; 

### 2题

在1题的基础上 取12个数字 组成最大的数

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
ans = 0
n = 12
for line in aaa.split('\n'):
    dp = [[-1] * (n + 1) for _ in range(len(line) + 1)]
    dp[0][0] = 0
    for i in range(len(line)):
        for j in range(n + 1):
            if dp[i][j] == -1:
                continue
            # 不选
            dp[i + 1][j] = max(dp[i + 1][j], dp[i][j])
            # 选
            if j &lt; n:
                dp[i + 1][j + 1] = max(dp[i + 1][j + 1], dp[i][j] * 10 + int(line[i]))
    ans += dp[len(line)][n]

print(ans)
```

&lt;/p&gt;
&lt;/details&gt; 

## 第四题

[https://adventofcode.com/2025/day/4](https://adventofcode.com/2025/day/4)

### 1题

给定一张图 标记所有东西的位置 找出 相邻八个位置中 满足一半或更多的位置是空的 有东西的数量

&gt; 有点难描述 可以点进原文查看原题

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
a = [list(line) for line in aaa.splitlines()]

for row in range(len(a)):
    for col in range(len(a[0])):
        # if a[row][col] == '.':
        pairs = []
        # 统计周围八个少于4
        for i in [-1, 0, 1]:
            for j in [-1, 0, 1]:
                if i == 0 and j == 0:
                    continue
                if 0 &lt;= row + i &lt; len(a) and 0 &lt;= col + j &lt; len(a[0]):
                    if a[row + i][col + j] == '@' or a[row + i][col + j] == 'x':
                        pairs.append((row + i, col + j))
        # 少于4个@ 则将@变成x
        if len(pairs) &lt; 4 and a[row][col] == '@':
            # for i, j in pairs:
            #     a[i][j] = 'x'
            a[row][col] = 'x'

ans = 0
for row in range(len(a)):
    for col in range(len(a[0])):
        if a[row][col] == 'x':
            ans += 1

print(ans)
```

&lt;/p&gt;
&lt;/details&gt; 

&gt; 特别解法
&gt; 可以将空的位置转换成0 有东西的位置转换成1 这样获得了一个矩阵
&gt; 对他进行卷积 取值小于等于4的点即可 padding要是1

### 2题

在1题的基础上 找出来的位置都可以去掉变成空的 然后继续找

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
a = [list(line) for line in aaa.splitlines()]
ans = 0
num_of_at = sum(row.count('@') for row in a)
temp_num_of_at = num_of_at
while True:
    for row in range(len(a)):
        for col in range(len(a[0])):
            pairs = []
            # 统计周围八个少于4
            for i in [-1, 0, 1]:
                for j in [-1, 0, 1]:
                    if i == 0 and j == 0:
                        continue
                    if 0 &lt;= row + i &lt; len(a) and 0 &lt;= col + j &lt; len(a[0]):
                        if a[row + i][col + j] == '@' or a[row + i][col + j] == 'x':
                            pairs.append((row + i, col + j))
            # 少于4个@ 则将@变成x
            if len(pairs) &lt; 4 and a[row][col] == '@':
                a[row][col] = '.'
    new_num_of_at = sum(row.count('@') for row in a)
    if new_num_of_at == temp_num_of_at:
        break
    temp_num_of_at = new_num_of_at

ans = num_of_at - new_num_of_at 

print(ans)
```

&lt;/p&gt;
&lt;/details&gt; 

&gt; 相似的 也可以使用 1题的卷积思路

## 第五题

[https://adventofcode.com/2025/day/5](https://adventofcode.com/2025/day/5)

### 1题

给定一个范围 给定一系列数字 找到所有在范围内的数

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
fresh_ranges_text, vegtables_text = aaa.split('\n\n')

fresh_ranges = []
for line in fresh_ranges_text.strip().splitlines():
    start, end = map(int, line.split('-'))
    fresh_ranges.append((start, end))

ans = 0
for line in vegtables_text.strip().splitlines():
    iid = int(line)
    is_fresh = any(start &lt;= iid &lt;= end for start, end in fresh_ranges)
    ans += is_fresh

print(ans)
```

&lt;/p&gt;
&lt;/details&gt; 

### 2题

在1题的基础上 一系列数字不需要使用
找到范围内有多少数字
需要合并区间

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
fresh_ranges_text, vegtables_text = aaa.split('\n\n')

fresh_ranges = []
for line in fresh_ranges_text.strip().splitlines():
    start, end = map(int, line.split('-'))
    fresh_ranges.append((start, end))

# merge
fresh_ranges.sort()
merged_fresh = []
for start, end in fresh_ranges:
    if not merged_fresh or merged_fresh[-1][1] &lt; start:
        merged_fresh.append((start, end))
    else:
        merged_fresh[-1] = (merged_fresh[-1][0], max(merged_fresh[-1][1], end))

ans = 0
for i, j in merged_fresh:
    ans += j - i + 1
print(ans)
```

&lt;/p&gt;
&lt;/details&gt; 

## 第六题

[https://adventofcode.com/2025/day/6](https://adventofcode.com/2025/day/6)

### 1题

竖着的加法乘法计算器

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
a = aaa.splitlines()
b = [list(map(int, line.split())) for line in a if line and not line.startswith('*')]
c = [line.split() for line in a[-1]]

# 去除split后的空字符串
b = [[num for num in line if num] for line in b]
c = [op[0] for op in c if op]


result = 0
for i in range(len(b[0])):
    match c[i]:
        case '+':
            result += b[0][i] + b[1][i] + b[2][i] + b[3][i]
        case '*':
            result += b[0][i] * b[1][i] * b[2][i] * b[3][i]

print(result)
```

&lt;/p&gt;
&lt;/details&gt; 

### 2题

竖着的 按位的 加减乘除计算器

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
ops = aaa.splitlines()[-1]
ops = [op for op in ops.split(' ') if op]
total_lines = aaa.splitlines()
lines = total_lines[:-1]

width = len(lines[0])
height = len(lines)

ans = 0
temp = []
number = 0

for pos_x in range(width - 1, -1, -1):
    for pos_y in range(height + 1):
        char = total_lines[pos_y][pos_x]
        if pos_y == height:
            temp.append(number)
            number = 0
        if char == ' ' and pos_y != height:
            continue
        elif char in '+*':
            if char == '+':
                ans += sum(temp)
            if char == '*':
                prod = 1
                for t in temp:
                    if t != 0:
                        prod *= t
                ans += prod
            temp = []
        else:
            if char == ' ':
                continue
            number *= 10
            number += int(char)

print(ans)
```

&lt;/p&gt;
&lt;/details&gt; 

## 第七题

[https://adventofcode.com/2025/day/7](https://adventofcode.com/2025/day/7)

### 1题

图 发射一道激光往下 遇到 ^分成左右两股激光继续向下
会分裂多少次

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
a = [list(line) for line in aaa.splitlines()]
width = len(a[0])
height = len(a)

pos_s_x, pos_s_y = aaa.index('S'), 0

deque = [(pos_s_x, pos_s_y)]
ans = 0
visited = set()
for i in deque:
    x, y = i
    if y + 1 &gt;= height:
        continue
    if a[y + 1][x] == '^':
        if y + 1 &lt; height:
            if (x + 1, y + 1) not in visited:
                deque.append((x + 1, y + 1))
                visited.add((x + 1, y + 1))
            if (x - 1, y + 1) not in visited:
                deque.append((x - 1, y + 1))
                visited.add((x - 1, y + 1))
            ans += 1
    elif a[y + 1][x] == '.':
        if y + 1 &lt; height:
            if (x, y + 1) not in visited:
                deque.append((x, y + 1))
                visited.add((x, y + 1))
    

print(len(visited))
print(ans)
```

&lt;/p&gt;
&lt;/details&gt; 

### 2题

在1题的基础上 有多少条不同路径的光线 (到达同一个目的地的不同路径光线 也需要计算)

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
a = [list(line) for line in aaa.splitlines()]
width = len(a[0])
height = len(a)

pos_s_x, pos_s_y = aaa.index('S'), 0

cache = {}

def dfs(x, y):
    if y + 1 &gt;= height or x &lt; 0 or x &gt;= width:
        return 0
    if (x, y) in cache:
        return cache[(x, y)]
    
    if a[y + 1][x] == '^':
        result = dfs(x + 1, y + 1) + dfs(x - 1, y + 1) + 1
    elif a[y + 1][x] == '.':
        result = dfs(x, y + 1)
    else:
        result = 0
    
    cache[(x, y)] = result
    return result
    
print(dfs(pos_s_x, pos_s_y) + 1)
```

&lt;/p&gt;
&lt;/details&gt; 

## 第八题

[https://adventofcode.com/2025/day/8](https://adventofcode.com/2025/day/8)

### 1题

给定若干个x,y,z坐标的点 连接最近的1000个点 问 有多少簇

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
a = aaa.split('\n')
a = [list(map(int, i.split(','))) for i in a]

def distance(x1, y1, z1, x2, y2, z2):
    return ((x1 - x2) ** 2 + (y1 - y2) ** 2 + (z1 - z2) ** 2) ** 0.5

distances = []
for i in range(len(a)):
    for j in range(i + 1, len(a)):
        dist = distance(a[i][0], a[i][1], a[i][2], a[j][0], a[j][1], a[j][2])
        distances.append((dist, i, j))

distances.sort(key=lambda x: x[0])

needed_distances = distances[:1000]

# 邻接表
graph = {i: [] for i in range(len(a))}
for dist, i, j in needed_distances:
    graph[i].append((j, dist))
    graph[j].append((i, dist))

# dfs 遍历所有群组 获取所有群组节点数量
visited = [False] * len(a)
def dfs(node):
    stack = [node]
    size = 0
    while stack:
        current = stack.pop()
        if not visited[current]:
            visited[current] = True
            size += 1
            for neighbor, _ in graph[current]:
                if not visited[neighbor]:
                    stack.append(neighbor)
    return size
group_sizes = []
for i in range(len(a)):
    if not visited[i]:
        group_size = dfs(i)
        group_sizes.append(group_size)
group_sizes.sort()
print(group_sizes)
```

&lt;/p&gt;
&lt;/details&gt; 

### 2题

最小生成树
一点一点连接最近的两个点 最后连接的两个点是哪两个点

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
a = aaa.split('\n')
a = [list(map(int, i.split(','))) for i in a]

def distance(x1, y1, z1, x2, y2, z2):
    return ((x1 - x2) ** 2 + (y1 - y2) ** 2 + (z1 - z2) ** 2) ** 0.5

distances = []
for i in range(len(a)):
    for j in range(i + 1, len(a)):
        dist = distance(a[i][0], a[i][1], a[i][2], a[j][0], a[j][1], a[j][2])
        distances.append((dist, i, j))

distances.sort(key=lambda x: x[0])

# 构建最小生成树
parent = [i for i in range(len(a))]
num_components = len(a)
def find(x):
    if parent[x] != x:
        parent[x] = find(parent[x])
    return parent[x]

def union(x, y):
    rootX = find(x)
    rootY = find(y)
    if rootX != rootY:
        parent[rootY] = rootX
        return True
    return False

mst_edges = []
for dist, i, j in distances:
    if union(i, j):
        mst_edges.append((dist, i, j))
        num_components -= 1
        if num_components == 1:
            print('i, j, dist:', i, j, dist)
            break

# 获取 i, j 对应的点坐标
point_i = a[i]
point_j = a[j]

print('Point i:', point_i)
print('Point j:', point_j)
```

&lt;/p&gt;
&lt;/details&gt; 

## 第九题

[https://adventofcode.com/2025/day/9](https://adventofcode.com/2025/day/9)

### 1题

给定若干个x y 坐标点 找到最大的 以这两个点作为对角的 矩形

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
a = [(int(x), int(y)) for x, y in (line.split(',') for line in aaa.split('\n'))]

def cal_space(x1, y1, x2, y2):
    return (abs(x1 - x2) + 1) * (abs(y1 - y2) + 1)

ans = 0
for i in range(len(a) - 1):
    for j in range(i + 1, len(a)):
        x1, y1 = a[i]
        x2, y2 = a[j]
        space = cal_space(x1, y1, x2, y2)
        if space &gt; ans:
            ans = space

print(ans)
```

&lt;/p&gt;
&lt;/details&gt; 

### 2题

在1题的基础上 上一个点 的x或者y与下一个点的x或者y相同 由此绕出了一个形状 需要找到最大的 在这个形状中的 以这两个点作为对角的矩形

&gt; 检查这个矩形在不在这个形状里

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
a = [(int(x), int(y)) for x, y in (line.split(',') for line in aaa.split('\n'))]

def cal_space(x1, y1, x2, y2):
    return (abs(x1 - x2) + 1) * (abs(y1 - y2) + 1)

def check(edges, x1, y1, x2, y2) -&gt; bool:
    min_x = min(x1, x2)
    max_x = max(x1, x2)
    min_y = min(y1, y2)
    max_y = max(y1, y2)
    mid_x = (x1 + x2) / 2
    mid_y = (y1 + y2) / 2

    def ray_cast(midpoint, edges):
        count = 0
        mx, my = midpoint
        for (x3, y3), (x4, y4) in edges:
            if x3 &gt; mx and x4 &gt; mx:
                if min(y3, y4) &lt;= my &lt; max(y3, y4):
                    count += 1
        return count % 2 == 1
    
    if not ray_cast((mid_x, mid_y), edges):
        return False

    for (x3, y3), (x4, y4) in edges:
        if x3 == x4:
            if min_x &lt; x3 &lt; max_x:
                edge_min_y = min(y3, y4)
                edge_max_y = max(y3, y4)
                if not (edge_max_y &lt;= min_y or edge_min_y &gt;= max_y):
                    return False
                
        elif y3 == y4:
            if min_y &lt; y3 &lt; max_y:
                edge_min_x = min(x3, x4)
                edge_max_x = max(x3, x4)
                if not (edge_max_x &lt;= min_x or edge_min_x &gt;= max_x):
                    return False
    return True


edges = []
for i in range(0, len(a) - 1):
    edges.append((a[i], a[i + 1]))

edges.append((a[-1], a[0]))

ans = 0
for i in range(0, len(a) - 1):
    for j in range(i + 1, len(a)):
        x1, y1 = a[i]
        x2, y2 = a[j]
        space = cal_space(x1, y1, x2, y2)
        if space &lt;= ans:
            continue
        if check(edges, x1, y1, x2, y2):
            ans = space

print(ans)
```

&lt;/p&gt;
&lt;/details&gt; 

## 第十题

[https://adventofcode.com/2025/day/10](https://adventofcode.com/2025/day/10)

### 1题

给定 灯最终需要到达的状态
给定 每个按钮 按下按钮后 各个灯的变化
给定 计数器 (第一题 用不到)

求 将完全关闭的灯泡 切换到 最终需要到达的状态需要按多少次

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
import re

a = aaa.splitlines()
t = []
for i in a:
    lights = re.findall(r'\[(.*?)\]', i)[0]
    steps = re.findall(r'\((.*?)\)', i)
    joltages = re.findall(r'\{(.*?)\}', i)[0]
    steps = [list(map(int, s.split(','))) if s else [] for s in steps]
    joltages = list(map(int, joltages.split(',')))
    lights_len = len(lights)
    t.append((lights, lights_len, steps, joltages))

ans = 0
for lights, lights_len, steps, joltages in t:
    # 最少操作数让所有灯满足lights中的要求 不需要考虑电压
    dp = {0: 0}  # 状态压缩dp
    for step in steps:
        next_dp = {}
        for state, cnt in dp.items():
            # 不按
            if state not in next_dp or next_dp[state] &gt; cnt:
                next_dp[state] = cnt
            # 按
            next_state = state
            for s in step:
                next_state ^= (1 &lt;&lt; s)
            if next_state not in next_dp or next_dp[next_state] &gt; cnt + 1:
                next_dp[next_state] = cnt + 1
        dp = next_dp
    final_state = 0
    for i in range(lights_len):
        if lights[i] == '#':
            final_state |= (1 &lt;&lt; i)
    ans += dp[final_state]

print(ans)
```

&lt;/p&gt;
&lt;/details&gt; 

### 2题

题意转换为
给定 灯最终需要到达的状态 (第二题用不到)
给定 每个按钮 按下按钮后 各个灯电压加一
给定 电压计数器

达到给定电压(超过也行) 需要按多少次
&gt; 这是一题 带有约束的 正整数线性方程求解 直接调用库求解即可

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
import re

a = aaa.splitlines()
t = []
for i in a:
    lights = re.findall(r'\[(.*?)\]', i)[0]
    steps = re.findall(r'\((.*?)\)', i)
    joltages = re.findall(r'\{(.*?)\}', i)[0]
    steps = [list(map(int, s.split(','))) if s else [] for s in steps]
    joltages = list(map(int, joltages.split(',')))
    lights_len = len(lights)
    t.append((lights, lights_len, steps, joltages))

import z3

def solve(lights, lights_len, steps, joltages):
    # 设 steps 列表中的第 i 个元素为xi 约束: xi大于等于0 且整数
    # sum(Aji * xi) = bj
    # Aji = 1 表示第i个按钮可以控制到第j个灯, 也就是可以增加第j个灯的电压
    # xi 表示第i个按钮按下的次数
    # bj 表示第j个灯的目标电压
    # 求解 minimize sum(xi)
    s = z3.Solver()
    x = [z3.Int(f'x{i}') for i in range(len(steps))]
    for i in range(len(steps)):
        s.add(x[i] &gt;= 0)
    for j in range(lights_len):
        coeffs = []
        for i in range(len(steps)):
            if j in steps[i]:
                coeffs.append(x[i])
        s.add(z3.Sum(coeffs) == joltages[j])
    obj = z3.Sum(x)
    h = z3.Optimize()
    h.add(s.assertions())
    h.minimize(obj)
    if h.check() == z3.sat:
        m = h.model()
        res = [m.evaluate(x[i]).as_long() for i in range(len(steps))]
        return res
    else:
        return [-1] * len(steps)
    
results = []
for lights, lights_len, steps, joltages in t:
    res = solve(lights, lights_len, steps, joltages)
    results.append(res)
for res in results:
    print(' '.join(map(str, res)))

ans = sum(sum(r) for r in results)
print(ans)
```

&lt;/p&gt;
&lt;/details&gt; 

## 第十一题

[https://adventofcode.com/2025/day/11](https://adventofcode.com/2025/day/11)

### 1题

给定 若干个映射 f(a) -&gt; b
b不一定只有一个 找到从 特定字符开始 到结束字符的 每条路径

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
a = {}
for i in aaa.splitlines():
    p, q = i.split(': ')
    a[p] = q.split(' ')


def dfs(t):
    if t == 'out':
        return 0
    nt = a[t]
    b = 0
    for i in nt:
        b += dfs(i)
    return b + len(nt) - 1

print(dfs('you') + 1)
```

&lt;/p&gt;
&lt;/details&gt; 

### 2题

 在1题的基础上 特定字符换了一下 且需要同时经过 某两个字符(顺序无关)

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
import functools

a = {}
for i in aaa.splitlines():
    p, q = i.split(': ')
    a[p] = q.split(' ')


@functools.lru_cache(None)
def dfs(t, visited_dac, visited_fft):
    if t == 'dac':
        visited_dac = True
    if t == 'fft':
        visited_fft = True
    if t == 'out':
        if visited_dac and visited_fft:
            return 1, visited_dac, visited_fft
        else:
            return 0, visited_dac, visited_fft
    nt = a[t]
    b = 0
    for i in nt:
        b += dfs(i, visited_dac, visited_fft)[0]
    return b, visited_dac, visited_fft

print(dfs('svr', False, False))
```

&lt;/p&gt;
&lt;/details&gt; 

&gt; 路线一下就不是同一个数量级的

## 第十二题

[https://adventofcode.com/2025/day/12](https://adventofcode.com/2025/day/12)

### 只有一题

建议看原文

给定特定形状 给定一个二维箱子 大小有限 可以旋转 以及见缝插针

问这些东西能不能放进去

&gt; 这是一题整蛊题 正常来说算法实现非常困难

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
all_data = aaa.split('\n\n')
t = all_data[:-1]
gifts = []
for i in t:
    gift = []
    for line in i.split('\n'):
        if ':' in line:
            continue
        gift.append(line)
    gifts.append(gift)

print(gifts)
gift_area = []
for i in gifts:
    area_num = 0
    for line in i:
        area_num += line.count('#')
    gift_area.append(area_num)

print(gift_area)

ans = 0
a = all_data[-1]
for line in a.splitlines():
    region, gift_nums_str = line.split(': ')
    wide_, long_ = region.split('x')
    gift_nums = gift_nums_str.split(' ')
    all_area = int(wide_) * int(long_)
    needed_area = 0
    for i in range(len(gift_nums)):
        needed_area += int(gift_nums[i]) * gift_area[i]
    if needed_area &lt;= all_area:
        ans += 1

print(ans)
```

&lt;/p&gt;
&lt;/details&gt; 

&gt; 仅需检测面积即可。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/2025%20Advent%20of%20Code%20-fu-pan-yu-da-an.html</guid><pubDate>Tue, 13 Jan 2026 07:16:09 +0000</pubDate></item><item><title>opencode 上手与入门</title><link>https://FairyOwO.github.io/post/opencode%20-shang-shou-yu-ru-men.html</link><description>## opencode 上手与入门

随着code agent的发展 效率与可用性已经大幅上涨
部分mcp服务器 (例如content7或者是定向检索代码的mcp服务器) 可以明显提升效果
这里记录一下 如何安装并配置opencode

这里选择的是 opencode + 反重力 cli to api 的方案

同类agent:
claude code(cc)
codex(cx)

&gt; 如果选择更省心的方案推荐 claude code + 购买中转服务 + 自行配置一些mcp(尽管不是必须的, 但配置好后效果会大幅提升)
&gt; 如果选择多 agent 方案 推荐 主模型使用 opus 细节使用 gpt5.x-codex 干活模型使用 glm-4.x [https://github.com/bfly123/claude_code_bridge](https://github.com/bfly123/claude_code_bridge) 注: 模型具有时效性

### 安装 你喜欢的包管理器

这里选择的是 [nodejs](https://nodejs.org/zh-cn/download/) 中的 npx

&gt; 许多 mcp 服务器使用了npx来启动, 如果仅使用 opencode 的话 可以不进行安装

### 安装 opencode

详见文档, 这里仅作记录

[https://github.com/anomalyco/opencode](https://github.com/anomalyco/opencode)

```sh
# YOLO
curl -fsSL https://opencode.ai/install | bash

# Package managers
npm i -g opencode-ai@latest        # or bun/pnpm/yarn
scoop bucket add extras; scoop install extras/opencode  # Windows
choco install opencode             # Windows
brew install opencode              # macOS and Linux
paru -S opencode-bin               # Arch Linux
mise use -g opencode               # Any OS
nix run nixpkgs#opencode           # or github:anomalyco/opencode for latest dev branch
```

或者是前往 [release](https://github.com/anomalyco/opencode/releases) 直接下载安装包或者免安装的 zip

### 获取 api

&gt; 有成品账号代理直连,中转等方案能拿到api可以忽略此章节

#### 反重力

这是一个google出的类似于cursor的代码编辑器 开通google ai pro会员, 在正确的地区, 即可使用

&gt; 你可以用家庭组 大号需要拉五个小号进家庭组 注意地区与付款锁 这样额度可以乘以六倍

这里使用 [CLIProxyAPI](https://github.com/router-for-me/CLIProxyAPI) 来将其转换成api提供给opencode使用

&gt; 注意 当前 opencode 与 CLIProxyAPI 一起使用 会有兼容性问题 调用工具的时候会中断 需要手动继续
&gt; [6.6.89](https://github.com/router-for-me/CLIProxyAPI/releases/tag/v6.6.89) 已修复

1. 下载 release 并解压
2. 修改配置文件 `config.example.yaml` 并修改成 `config.yaml`

    ```yaml
    proxy-url: socks5://user:pass@192.168.1.1:1080/  # 你需要改成你的, 这里proxy用于访问google 所以ip需要纯净
    allow-remote: false  # 如果你在服务器上搭建, 则需要允许
    ```

3. 启动, 并访问 [webui](http://localhost:8317/management.html)
4. 根据实际情况配置
5. OAuth 登录 反重力
    &gt; 注意 这里需要 CLIProxyAPI 与 浏览器 一起经过代理

这个时候已经可以用了 response 地址是: [http://localhost:8317/v1/responses](http://localhost:8317/v1/responses)

&gt; 不止有 response 的地址 还支持不同厂的地址, 例如 openai 地址

模型列表是:

```text
gemini-2.5-flash
gemini-3-pro-image-preview
gemini-2.5-computer-use-preview-10-2025
gemini-3-pro-preview
gemini-3-flash-preview
gemini-2.5-flash-lite
gemini-claude-sonnet-4-5
gemini-claude-opus-4-5-thinking
gemini-claude-sonnet-4-5-thinking
gpt-oss-120b-medium
```

&gt; 注意 因为羊毛被薅太多 这里claude与gpt模型用量非常少

### opencode 配置代理与中转

opencode使用环境变量配置代理

```sh
export HTTPS_PROXY=https://proxy.example.com:8080
export HTTP_PROXY=http://proxy.example.com:8080
export NO_PROXY=localhost,127.0.0.1
# 注意 NO_PROXY 也要
```

opencode 需要手动写配置文件 `~/config/opencode/opencode.json`

参考:

```jsonc
{
  'plugin': [
    'oh-my-opencode',
    // 'opencode-antigravity-auth@1.2.7'  // 自带的反重力插件 这里我们用其他方案代替, 所以不需要
  ],
  '$schema': 'https://opencode.ai/config.json',
  'provider': {
    'Antigravity': {
      'npm': '@ai-sdk/openai-compatible',
      'name': 'Antigravity',
      'options': {
        'baseURL': 'http://localhost:8317/v1'  // 默认CLIProxyAPI地址
      },
      'models': {
        'gemini-3-pro-preview': {
          'name': 'Gemini 3 Pro preview (Antigravity)',
          'thinking': true,
          'attachment': true,
          'limit': {
            'context': 1048576,
            'output': 65535
          },
          'modalities': {
            'input': [
              'text',
              'image',
              'pdf'
            ],
            'output': [
              'text'
            ]
          }
        },
        'gemini-3-flash-preview': {
          'name': 'Gemini 3 Flash (Antigravity)',
          'attachment': true,
          'limit': {
            'context': 1048576,
            'output': 65536
          },
          'modalities': {
            'input': [
              'text',
              'image',
              'pdf'
            ],
            'output': [
              'text'
            ]
          }
        },
        'gemini-2.5-flash-lite': {
          'name': 'Gemini 2.5 Flash Lite (Antigravity)',
          'attachment': true,
          'limit': {
            'context': 1048576,
            'output': 65536
          },
          'modalities': {
            'input': [
              'text',
              'image',
              'pdf'
            ],
            'output': [
              'text'
            ]
          }
        }
      }
    },
  }
}
```

&gt; 你可以通过ai来自动生成配置文件

如果这边是中转的 api, 那么仅需要修改 opencode.json 中 修改 provider 中的 options.baseURL
新版本的 opencode 可能不会创建这个配置文件, 需要手动配置
配置好后去 opencode 中配置 api 即可

### 安装oh my opencode

#### 使用opencode agent 安装

既然都安装了 opencode 不妨来试试
直接将 [oh my opencode](https://github.com/code-yeongyu/oh-my-opencode) 项目地址发送给 opencode 来让他帮你安装吧

[https://github.com/code-yeongyu/oh-my-opencode](https://github.com/code-yeongyu/oh-my-opencode)

&gt; 注意 这里opencode 会自动帮你安装 bunx 来安装这个项目, 如果不想要的话 需要强调使用npx安装
&gt; oh my opencode 也可以用来登录反重力账号, 插件名称是 opencode-antigravity-auth, 将反重力支持模型到opencode中运行 但如果之前使用 CLIProxyAPI 来使用反重力, 则无需安装这个插件

#### 手动

详见文档, 这里仅作记录

```sh
bunx oh-my-opencode install
# or use npx if bunx doesn't work
npx oh-my-opencode install
```

#### 配置文件

之前使用我们自己的 opencode 配置文件 这里当然需要自行配置 oh-my-opencode 的配置文件, 当然你也可以让ai来做这件事情 详见文档

这里摘抄一下每个agent是干什么的

```text
Sisyphus (anthropic/claude-opus-4-5)：默认 Agent。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/opencode%20-shang-shou-yu-ru-men.html</guid><pubDate>Mon, 05 Jan 2026 09:02:21 +0000</pubDate></item><item><title>入门逆向工程之新手题writeup</title><link>https://FairyOwO.github.io/post/ru-men-ni-xiang-gong-cheng-zhi-xin-shou-ti-writeup.html</link><description>## 概述

逆向新手题 `jocker.exe` 的windows可执行文件。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/ru-men-ni-xiang-gong-cheng-zhi-xin-shou-ti-writeup.html</guid><pubDate>Wed, 03 Dec 2025 06:28:06 +0000</pubDate></item><item><title>rust 非语法问题记录</title><link>https://FairyOwO.github.io/post/rust%20-fei-yu-fa-wen-ti-ji-lu.html</link><description>## 记录一些学习 rust 中的非语法问题

&gt; 常来说 每个语言都有专门的工作流，用这个语言编写某个项目/功能时，需要遵循一定的规范，这里做一点记录。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/rust%20-fei-yu-fa-wen-ti-ji-lu.html</guid><pubDate>Tue, 04 Nov 2025 09:00:40 +0000</pubDate></item><item><title>个人常用命令</title><link>https://FairyOwO.github.io/post/ge-ren-chang-yong-ming-ling.html</link><description>## 工具

1. edit
    命令行编辑器
    [https://github.com/microsoft/edit](https://github.com/microsoft/edit)
2. chscr
    换源工具
    [https://github.com/RubyMetric/chsrc](https://github.com/RubyMetric/chsrc)
3. cherry-studio
    llm聊天前端 免安装
    [https://github.com/CherryHQ/cherry-studio](https://github.com/CherryHQ/cherry-studio)

## 安装命令

### apt

```sh
test -e /etc/apt/sources.list || echo 'deb http://mirrors.aliyun.com/debian bookworm main' &gt; /etc/apt/sources.list &amp;&amp; \
echo 'deb http://mirrors.aliyun.com/debian-security bookworm-security main' &gt;&gt; /etc/apt/sources.list &amp;&amp; \
echo 'deb http://mirrors.aliyun.com/debian bookworm-updates main' &gt;&gt; /etc/apt/sources.list

# 清华源
# test -e /etc/apt/sources.list || echo 'deb http://mirrors.tuna.tsinghua.edu.cn/debian bookworm main' &gt; /etc/apt/sources.list &amp;&amp; \
# echo 'deb http://mirrors.tuna.tsinghua.edu.cn/debian-security bookworm-security main' &gt;&gt; /etc/apt/sources.list &amp;&amp; \
# echo 'deb http://mirrors.tuna.tsinghua.edu.cn/debian bookworm-updates main' &gt;&gt; /etc/apt/sources.list
```

### docker 安装

```sh
curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun
```

```sh
apt update
apt upgrade -y
apt install curl vim wget gnupg dpkg apt-transport-https lsb-release ca-certificates

curl -sSL https://download.docker.com/linux/debian/gpg | gpg --dearmor &gt; /usr/share/keyrings/docker-ce.gpg
echo 'deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-ce.gpg] https://download.docker.com/linux/debian $(lsb_release -sc) stable' &gt; /etc/apt/sources.list.d/docker.list

# 清华源
# curl -sS https://download.docker.com/linux/debian/gpg | gpg --dearmor &gt; /usr/share/keyrings/docker-ce.gpg
# echo 'deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-ce.gpg] https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/debian $(lsb_release -sc) stable' &gt; /etc/apt/sources.list.d/docker.list

apt update
apt install docker-ce docker-ce-cli containerd.io docker-compose-plugin
```

```sh
cat &gt; /etc/docker/daemon.json &lt;&lt; EOF
{
    'log-driver': 'json-file',
    'log-opts': {
        'max-size': '20m',
        'max-file': '3'
    },
    'ipv6': true,
    'fixed-cidr-v6': 'fd00:dead:beef:c0::/80',
    'experimental':true,
    'ip6tables':true,
    'registry-mirrors': [
        'https://func.ink',
        'https://dytt.online',
        'https://lispy.org',
        'https://docker.m.daocloud.io',
        'https://a.ussh.net',
        'https://docker.zhai.cm',
        'https://6azbt4va.mirror.aliyuncs.com'
    ]
}
EOF

systemctl restart docker
```

单脚本换源

```sh
bash &lt;(curl -sSL https://linuxmirrors.cn/docker.sh) --only-registry
```

### conda 安装

```sh
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
chmod +x Miniconda3-latest-Linux-x86_64.sh
./Miniconda3-latest-Linux-x86_64.sh
# 一路默认
```

### rust 安装

```sh
# 过时
mkdir -p ~/.cargo &amp;&amp; echo 'export RUSTUP_DIST_SERVER='https://rsproxy.cn'
export RUSTUP_UPDATE_ROOT='https://rsproxy.cn/rustup'' &gt;&gt;~/.bashrc &amp;&amp; echo '[source.crates-io]
replace-with = 'rsproxy-sparse'
[source.rsproxy]
registry = 'https://rsproxy.cn/crates.io-index'
[source.rsproxy-sparse]
registry = 'sparse+https://rsproxy.cn/index/'
[registries.rsproxy]
index = 'https://rsproxy.cn/crates.io-index'
[net]
git-fetch-with-cli = true' &gt;~/.cargo/config &amp;&amp; source ~/.bashrc &amp;&amp; curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh &amp;&amp; source ~/.cargo/env &amp;&amp; rustup target add x86_64-unknown-linux-musl &amp;&amp; cargo version &amp;&amp; rustc --version

```

### gcc

```sh
apt update
apt install build-essential
```

### golang

```sh
rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.24.1.linux-amd64.tar.gz
wget https://go.dev/dl/go1.24.1.linux-amd64.tar.gz
tar -C /usr/local -xzf go1.22.5.linux-amd64.tar.gz
echo '# GO install
export PATH=$PATH:/usr/local/go/bin
export GOPATH=$HOME/go
export PATH=$PATH:$GOPATH/bin
# END GO install
' &gt;&gt; ~/.profile
source ~/.profile
```

## 使用

### docker

根据 pid 来定位 docker 容器

```sh
# 1. 获取 pid, 使用 ps -aux, top等
# 2. 查找容器 ID
cat /proc/[PID]/cgroup
# 输出类似于 1:name=systemd:/docker/[CONTAINER_ID]
# 3. 截取 前十二位[短容器ID], 使用docker ps即可
docker ps | grep [短容器ID]
```

docker compose 修改端口映射

如果没有启动则直接修改 `docker-compose.yaml` 文件
如果已经在运行, 则

```sh
docker compose stop
docker compose up -d
```

进行手动重启, `docker compose restart` 无法修改

### pip

镜像源

1. 清华源: &lt;https://pypi.tuna.tsinghua.edu.cn/simple&gt;
2. 阿里源: &lt;https://mirrors.aliyun.com/pypi/simple/&gt;
3. 中科大源: &lt;https://pypi.mirrors.ustc.edu.cn/simple&gt;

### uv

python 镜像源

```sh
UV_PYTHON_INSTALL_MIRROR=https://github.com/indygreg/python-build-standalone/releases/download uv python install 3.13.1  # python镜像 或者 export

```

pip 镜像源 `export UV_DEFAULT_INDEX='https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple'`

或

```sh
# pyproject.toml
[[tool.uv.index]]
url = 'https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple'
default = true
```

```sh
# uv.toml
[[index]]
url = 'https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple'
default = true
```

初始化

```sh
cd /path/to/project
uv init
uv venv 名字 --python 3.12
```

虚拟环境

```sh
source .venv/bin/activate  # 激活虚拟环境

deactivate  # 退出虚拟环境
```

### github

镜像源: &lt;https://gh-proxy.com/&gt;

升级git:

```sh
# git update  # 2.17.1之前
git update-git-for-windows
# linux 直接使用包管理工具更新
```

```sh
git config --global user.name 名字
git config --global user.email 邮箱
```

安装 glt lfs

```sh
apt install git-lfs  # 安装lfs
# yum install git-lfs
git lfs install
```

### huggingface

镜像源: &lt;https://hf-mirror.com&gt;

```sh
export HF_ENDPOINT=https://hf-mirror.com
```

```powershell
$env:HF_ENDPOINT = 'https://hf-mirror.com'
```

### conda

给环境新增名字(vscode 创建 conda 环境无名字)

```sh
conda config --describe envs_dirs
conda config --append envs_dirs /path/to/the/parent_dir
```

### vim

解决粘贴格式混乱

进入命令模式输入 `:set paste`
粘贴完成后输入 `:set nopaste` 恢复默认缩进

### linux

添加环境变量

1. 临时

    ```sh
    export PATH=/to/your/path/:$PATH
    ```

2. 永久

    ```sh
    echo '
    export PATH=/to/your/path/:$PATH
    ' &gt;&gt; ~/.bashrc  # debian 系
    # echo '
    # export PATH=/to/your/path/:$PATH
    # ' &gt;&gt; /etc/profile
    ```

单脚本换源

[https://linuxmirrors.cn/use/](https://linuxmirrors.cn/use/)

```sh
bash &lt;(curl -sSL https://linuxmirrors.cn/main.sh)
```

重装 linux

&gt; 需要先切换软件源到国内

[https://github.com/leitbogioro/Tools](https://github.com/leitbogioro/Tools)

```sh
wget --no-check-certificate -qO InstallNET.sh 'https://gitee.com/mb9e8j2/Tools/raw/master/Linux_reinstall/InstallNET.sh' &amp;&amp; chmod a+x InstallNET.sh

bash InstallNET.sh -debian
```

查看端口占用

```sh
lsof -i:[端口号]
```

排序cpu与内存占用

```sh
top  # 打开top后,按大写的P来根据cpu排序,按大写的M来根据内存排序
```

### screen

```sh
screen -S yourname  # 新建一个叫yourname的session
screen -ls  # 列出当前所有的session
screen -r yourname  # 回到yourname这个session
screen -d yourname  # 远程detach某个session
screen -d -r yourname  # 结束当前session并回到yourname这个session  注: 一个session只能由一个用户进入, 第二个用户需要使用 -d -r 来中断前面一个用户的session
```

解压

```sh
tar -xvf xxxx.tar
tar -xzvf xxxx.tar.gz

unzip xxxx.zip  # 创建文件夹后, 解压到对应文件夹
```

### git 命令

1. 覆盖本地拉代码

    ```sh
    git fetch
    git reset --hard HEAD
    git pull
    ```
。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/ge-ren-chang-yong-ming-ling.html</guid><pubDate>Fri, 10 Oct 2025 01:48:29 +0000</pubDate></item><item><title>随机(二) 计算机的随机实现</title><link>https://FairyOwO.github.io/post/sui-ji-%28-er-%29%20-ji-suan-ji-de-sui-ji-shi-xian.html</link><description>&gt; 写在前面, 本文讨论的真伪随机与前文不同, 真随机数为通过物理现象或不可预测的事件产生, 伪随机数为确定性的数学算法产生, 区别于前文随机性检验来区分真伪随机.

## 引言

当我们深入到随机数的底层实现, 关注点则从随机数的表现行为转向到生成本源. 在计算机程序视角下, 在没有相关硬件下, 无法生成真随机数.

在讨论之前, 先对真伪随机数下定义:

真随机数: 物理世界中客观存在的, 本质上不可预测或至少是极难精确预测的自然过程, 这些过程被称为'熵源'
伪随机数: 由确定性的数学算法 (即伪随机数生成器PRNG) 产生.

真随机数的产生不依赖于任何确定性的算法, 理论上, 其输出序列是不可重现, 不可预测的. 每一个新的真随机数都是对物理熵源在某一时刻状态的独立'采样'.

伪随机数则不同, PRNG从一个初始值——称为 '种子' ——开始, 通过固定的递推公式或算法规则, 生成一个数字序列. 这个序列虽然在统计特性上可能与真随机序列非常相似, 甚至能够通过严格的随机性测试, 但其本质是完全确定的: 只要种子和算法相同, 产生的序列也必然完全相同. 

## '捕获' 真随机数

对应PRNG, 真随机数有一个真随机数生成器TRNG, 是通过硬件设备捕捉和处理物理熵源信号, 将其转换为可用的随机比特流.

常见的物理熵源有: 热噪声, [大气噪声](https://www.random.org/), 放射性衰变, 量子现象, 非线性动力学混沌系统.
对于一些x86处理器, 带有硬件随机数生成器, 使用了CPU硬件电路中物理噪声源.

有了熵源, 接下来就是 '捕获' 他, 最经典的就是使用传感器: 例如光电二极管, 核辐射探测器, 放大器等, 获得的信号通常为为连续的, 微弱的模拟信号, 这个时候我们需要将他转换成数字信号.

在经过信号放大后, 使用模数转换器即可完成转换成数字信号: 将模拟信号在特定时刻的幅值量化成数字值.
&gt; 参考音频模拟信号转数字信号

对于事件驱动的随机事件(放射性衰变), 则可以测量连续事件之间的时间间隔, 然后将这些时间间隔数字化.

在转换成数字信号后, 原始比特流仍然不完美, 可能会出现统计偏差或者出现相关性. 因此, 几乎所有的实用TRNG都会包含一个后处理阶段, 对原始比特流进行'提纯', 以改善其统计特性, 使其更接近理想的均匀分布和独立性. 详见 [https://en.wikipedia.org/wiki/Randomness_extractor](https://en.wikipedia.org/wiki/Randomness_extractor)

TRNG提供了最高质量的, 不可预测的随机性. 但是代价是不可复现, 生成速率较低等问题, 成本与复杂度较高, 环境敏感. 由于熵源可能退化或受到干扰, TRNG必须内置持续的健康监测机制. 这些测试 (通常是一些简化的在线随机性统计测试) 用于实时监控熵源和输出比特流的质量, 一旦检测到异常 (如输出长时间为全0或全1，或者统计特性明显偏离预期等随机性检验), 系统应能发出警报或停止输出, 以防止使用低质量的随机数. 

一个'真'的TRNG, 其真实性不仅取决于其所依赖的物理现象的随机本质, 更依赖于整个信号链 (从采集, 数字化到后处理) 的精心设计, 严格实现和持续验证. 任何环节的瑕疵都可能损害其输出的随机性质量.

## '伪造' 随机数

伪随机数生成器 (PRNG) 是利用确定性的数学算法来产生数字序列的程序. 这些序列虽然不是真正随机的, 但其设计目标是使其在统计特性上尽可能地模仿真随机序列. 

PRNG的核心运作需要一个种子与一个确定性算法

TODO

。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/sui-ji-%28-er-%29%20-ji-suan-ji-de-sui-ji-shi-xian.html</guid><pubDate>Fri, 01 Aug 2025 08:46:54 +0000</pubDate></item><item><title>随机(一) 应用上的随机</title><link>https://FairyOwO.github.io/post/sui-ji-%28-yi-%29%20-ying-yong-shang-de-sui-ji.html</link><description>&gt; 写在前面: 本文探讨的真随机与伪随机的定义: 只要满足随机性检测的, 即为真随机. 其余皆为伪随机
&gt; 区别于通过物理现象或不可预测的事件产生的真随机数, 与程序生成的伪随机数, 两者将在下一篇介绍
&gt; 常见的伪随机有: 抽卡低保算法等

## 引言

大多数人知道'随机', 但在实际生活中, 缺混淆相关概念, 这里做一个简单的介绍, 本文希望从统计测试视角介绍随机表现性质.

在应用篇视角下，我们关注的是一个数字序列在特定场景中“表现”得如何，而非其“出身”如何。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/sui-ji-%28-yi-%29%20-ying-yong-shang-de-sui-ji.html</guid><pubDate>Tue, 03 Jun 2025 07:20:05 +0000</pubDate></item><item><title>2024 Advent of Code 复盘与答案 (三) (17-25题)</title><link>https://FairyOwO.github.io/post/2024%20Advent%20of%20Code%20-fu-pan-yu-da-an-%20%28-san-%29%20%2817-25-ti-%29.html</link><description>[Advent of Code](https://adventofcode.com/)

使用 python 编写, 没有整理代码, 所以非常乱(变量乱取名, 没有注释, 逻辑奇怪, 并非最佳实现)
可以使用 gpt 相关工具辅助查看

&gt; 如果没有特意说明, 变量 a 统一存放所有原始字符串

## 第十七题

[https://adventofcode.com/2024/day/17](https://adventofcode.com/2024/day/17)

### 1题

尝试模拟计算机, 寄存器+特定操作码

&gt; 题非常麻烦, 建议看原文

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python

def main():
    input_data = a.splitlines()
    
    # Parse initial register values
    registers = {
        'A': int(input_data[0].split(': ')[1]),
        'B': int(input_data[1].split(': ')[1]),
        'C': int(input_data[2].split(': ')[1]),
    }
    
    # Parse program
    program = list(map(int, input_data[4].split(': ')[1].split(',')))
    
    output = []
    ip = 0
    program_length = len(program)
    
    def get_combo_value(operand):
        if operand == 4:
            return registers['A']
        elif operand == 5:
            return registers['B']
        elif operand == 6:
            return registers['C']
        else:
            return operand  # 0-3
    
    while ip &lt; program_length:
        if ip + 1 &gt;= program_length:
            break  # Invalid instruction, halt
        opcode = program[ip]
        operand = program[ip + 1]
        
        if opcode == 0:  # adv
            denominator = 2 ** get_combo_value(operand)
            registers['A'] = registers['A'] // denominator
            ip += 2
        elif opcode == 1:  # bxl
            registers['B'] ^= operand
            ip += 2
        elif opcode == 2:  # bst
            registers['B'] = get_combo_value(operand) % 8
            ip += 2
        elif opcode == 3:  # jnz
            if registers['A'] != 0:
                ip = operand
            else:
                ip += 2
        elif opcode == 4:  # bxc
            registers['B'] ^= registers['C']
            ip += 2
        elif opcode == 5:  # out
            output_value = get_combo_value(operand) % 8
            output.append(str(output_value))
            ip += 2
        elif opcode == 6:  # bdv
            denominator = 2 ** get_combo_value(operand)
            registers['B'] = registers['A'] // denominator
            ip += 2
        elif opcode == 7:  # cdv
            denominator = 2 ** get_combo_value(operand)
            registers['C'] = registers['A'] // denominator
            ip += 2
        else:
            ip += 2  # Invalid opcode, skip
        
    print(','.join(output))

if __name__ == '__main__':
    main()
``` 

&lt;/p&gt;
&lt;/details&gt; 

### 2题

在给定操作码下, 推断最小的寄存器中的满足条件的值(寄存器中的值等于输出的值)

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
program = list(map(int, a.splitlines()[4].split(': ')[1].split(',')))
n = len(program)
program = program[::-1]

choices = next_choices = [0]
for oo in program:
    choices = next_choices
    next_choices = []
    while choices:
        a = choices.pop()
        for i in range(8):
            aa = (a &lt;&lt; 3) | i
            bb = i ^ 1
            cc = aa &gt;&gt; bb
            bb = (bb ^ cc ^ 4) % 8
            if bb == oo:
                next_choices.append(aa)
    # print(next_choices)
    
print(min(next_choices))

# z = (A % 8) ^ 1
# out = z ^ (A &gt;&gt; z) ^ 4
``` 

&lt;/p&gt;
&lt;/details&gt; 

## 第十八题

[https://adventofcode.com/2024/day/18](https://adventofcode.com/2024/day/18)

### 1题

走二维迷宫

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python

import heapq
from collections import defaultdict

# print(all_map)

start = (0, 0)
end = (70, 70)




flag = False
temp = a.splitlines()
temp = temp[:1024]
# while not flag:
all_map = [['.' for _ in range(71)] for _ in range(71)]
for i in temp:
    x, y = map(int, i.split(','))
    all_map[x][y] = '#'


def get_allow_pos(pos):
    for i in [(0, 1), (1, 0), (-1, 0), (0, -1)]:
        next_step = (pos[0] + i[0], pos[1] + i[1])
        if len(all_map) &gt; next_step[0] &gt;= 0 and len(all_map[0]) &gt; next_step[1] &gt;= 0:
            if all_map[next_step[0]][next_step[1]] == '.':
                yield (pos[0] + i[0], pos[1] + i[1])

node = []
heapq.heappush(node, (0, start))
visited = defaultdict(int)
visited[start] = 0

while node:
    step, (x, y) = heapq.heappop(node)
    if (x, y) == end:
        flag = True
        print(step)
        break
    if visited[(x, y)] &lt; step:
        continue
    for next in get_allow_pos((x, y)):
        if next not in visited or step + 1 &lt; visited[(next[0], next[1])]:
            visited[(next[0], next[1])] = step + 1
            heapq.heappush(node, (step + 1, next))
``` 

&lt;/p&gt;
&lt;/details&gt; 

### 2题

让迷宫无解的最新的坐标是多少

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python

import heapq
from collections import defaultdict

# print(all_map)

start = (0, 0)
end = (70, 70)

flag = False
temp = a.splitlines()
temp.append('')
while not flag:
    all_map = [['.' for _ in range(71)] for _ in range(71)]
    print(temp.pop())
    for i in temp:
        x, y = map(int, i.split(','))
        all_map[x][y] = '#'

    def get_allow_pos(pos):
        for i in [(0, 1), (1, 0), (-1, 0), (0, -1)]:
            next_step = (pos[0] + i[0], pos[1] + i[1])
            if len(all_map) &gt; next_step[0] &gt;= 0 and len(all_map[0]) &gt; next_step[1] &gt;= 0:
                if all_map[next_step[0]][next_step[1]] == '.':
                    yield (pos[0] + i[0], pos[1] + i[1])

    node = []
    heapq.heappush(node, (0, start))
    visited = defaultdict(int)
    visited[start] = 0

    while node:
        step, (x, y) = heapq.heappop(node)
        if (x, y) == end:
            flag = True
            print(step)
            break
        if visited[(x, y)] &lt; step:
            continue
        for next in get_allow_pos((x, y)):
            if next not in visited or step + 1 &lt; visited[(next[0], next[1])]:
                visited[(next[0], next[1])] = step + 1
                heapq.heappush(node, (step + 1, next))
``` 

&lt;/p&gt;
&lt;/details&gt; 

## 第十九题

[https://adventofcode.com/2024/day/19](https://adventofcode.com/2024/day/19)

### 1题

给定几个图案片段, 与几种图案(包括不可完成图案), 找出有多少种设计方案

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python

aa = a.splitlines()
has = aa[0]
has = has.split(', ')
all_possible = aa[2:]


def dp(s):
    n = len(s)
    dp_state = [0] * (n + 1)
    dp_state[0] = 1
    for i in range(n):
        if dp_state[i]:
            for j in has:
                l = len(j)
                if i + l &lt;= n and s[i:i+l] == j:
                    dp_state[i+l] = 1
    return dp_state[n]

print(dp('abaa'))

ans = 0
for i in all_possible:
    ans += dp(i)

print(ans)

``` 

&lt;/p&gt;
&lt;/details&gt; 

### 2题

再将每个设计的不同方法的数量加起来

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
aa = a.splitlines()
has = aa[0]
has = has.split(', ')
all_possible = aa[2:]

def dp(s):
    n = len(s)
    dp_state = [0] * (n + 1)
    dp_state[0] = 1
    for i in range(n):
        if dp_state[i]:
            for j in has:
                l = len(j)
                if i + l &lt;= n and s[i:i+l] == j:
                    dp_state[i+l] += dp_state[i]
    return dp_state[n]


ans = 0
for i in all_possible:
    ans += dp(i)

print(ans)

``` 

&lt;/p&gt;
&lt;/details&gt; 

## 第二十题

[https://adventofcode.com/2024/day/20](https://adventofcode.com/2024/day/20)

### 1题

走迷宫, 但允许穿墙 走一步1ps, 允许穿墙一次 最多2 皮秒内禁止碰撞一次
求有多少种能省100ps的穿墙方式

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python

aa = [list(i) for i in a.splitlines()]
walls = [(-1, -1)]
# 开始结束
for i in range(len(aa)):
    for j in range(len(aa[i])):
        if aa[i][j] == 'S':
            start = (i, j)
            aa[i][j] = '.'
        elif aa[i][j] == 'E':
            end = (i, j)
            aa[i][j] = '.'
        elif aa[i][j] == '#':
            walls.append((i, j))

new_walls = []
for i in range(len(walls)):
    if i == 0:
        continue
    if walls[i][0] == 0 or walls[i][0] == len(aa) - 1 or walls[i][1] == 0 or walls[i][1] == len(aa[0]) - 1:
        continue
    # 如果wall的上下或者左右无墙, 则加入new_walls, 注意边界
    if walls[i][0] - 1 &gt;= 0 and aa[walls[i][0] - 1][walls[i][1]] != '#' and walls[i][0] + 1 &lt; len(aa) and aa[walls[i][0] + 1][walls[i][1]] != '#':
        new_walls.append((walls[i][0], walls[i][1]))
    if walls[i][1] - 1 &gt;= 0 and aa[walls[i][0]][walls[i][1] - 1] != '#' and walls[i][1] + 1 &lt; len(aa[0]) and aa[walls[i][0]][walls[i][1] + 1] != '#':
        new_walls.append((walls[i][0], walls[i][1]))

# print(new_walls)
walls = new_walls

print(start, end)

from copy import deepcopy
import heapq
from collections import defaultdict

ans = []
aaa = 0
flag = True
for i in walls:
    all_map = deepcopy(aa)
    if not flag:
        all_map[i[0]][i[1]] = '.'

    def get_allow_pos(pos):
        for i in [(0, 1), (1, 0), (-1, 0), (0, -1)]:
            next_step = (pos[0] + i[0], pos[1] + i[1])
            if len(all_map) &gt; next_step[0] &gt;= 0 and len(all_map[0]) &gt; next_step[1] &gt;= 0:
                if all_map[next_step[0]][next_step[1]] == '.':
                    yield (pos[0] + i[0], pos[1] + i[1])


    node = []
    heapq.heappush(node, (0, start))
    visited = defaultdict(int)
    visited[start] = 0
    while node:
        step, (x, y) = heapq.heappop(node)
        if (x, y) == end:
            if flag:
                aaa = step
                flag = False
            else:
                if aaa - step &gt;= 100:
                    ans.append(aaa - step)
            break
        if visited[(x, y)] &lt; step:
            continue
        for next in get_allow_pos((x, y)):
            if next not in visited or step + 1 &lt; visited[(next[0], next[1])]:
                visited[(next[0], next[1])] = step + 1
                heapq.heappush(node, (step + 1, next))
print(len(ans))

``` 

&lt;/p&gt;
&lt;/details&gt; 

### 2题

穿墙允许20ps, 作弊不需要使用全部 20 皮秒；作弊可以持续任意时间，最长可达 20 皮秒, 但还是只能用一次, 任何未使用的作弊时间都会丢失；它不能被保存以供以后再次作弊。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/2024%20Advent%20of%20Code%20-fu-pan-yu-da-an-%20%28-san-%29%20%2817-25-ti-%29.html</guid><pubDate>Tue, 06 May 2025 06:41:19 +0000</pubDate></item><item><title>基于 deepseek 的 r1 复现, 对传统强化学习的思考 (挖坑)</title><link>https://FairyOwO.github.io/post/ji-yu-%20deepseek%20-de-%20r1%20-fu-xian-%2C%20-dui-chuan-tong-qiang-hua-xue-xi-de-si-kao-%20%28-wa-keng-%29.html</link><description>&gt; 本文存在大量口齿不清

deepseek-r1持续火热, 已有大量的复现训练过程

1. [open-r1](https://github.com/huggingface/open-r1)
2. [mini-deepseek-r1](https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/mini-deepseek-r1-aha-grpo.ipynb)
3. [open-r1-multimodal](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal)
4. [open-thoughts](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal)
5. [TinyZero](https://github.com/Jiayi-Pan/TinyZero)
6. [simpleRL-reason](https://github.com/hkust-nlp/simpleRL-reason)
7. [RAGEN](https://github.com/ZihanWang314/RAGEN)
8. [unsloth r1-reasoning](https://unsloth.ai/blog/r1-reasoning)
9. [oat-zero](https://github.com/sail-sg/oat-zero)
10. [Logic-RL](https://github.com/Unakar/Logic-RL)
11. [deepseek-r1-gsm8k](https://github.com/Mryangkaitong/deepseek-r1-gsm8k)

具体效果可以看具体的仓库与其中的论文或者博客, 这里主要想法是, 是否可以使用这一训练范式, 来为传统强化学习任务, 带来可对人带来参考的思维过程

例如, 希望给围棋使用此范式, 一个可能的 pipeline 是:
1. sft, 为模型带入围棋知识
    &gt; 我认为是必要的, 因为通用型大模型此类知识较少, 如果没有的话, 搜索解空间太大, 不容易搜索到正确的 token, 可以使用其他强力模型或者程序进行辅助构建
2. 设定一个通用格式, 继续sft, 让模型输出正确格式(非必要, 在上述复现中, 无需特意sft也可以让模型输出正确格式)
3. rule based RL
    &gt; 考虑到围棋中间的reward非常难量化, 可以使用专业围棋模型进行反馈
    &gt; 让大模型进行一定的思考, 输出一个答案, 之后与围棋模型进行比对, 前几选 reward +1, 其他 reward -1, 或者更进一步的, 使用围棋模型的 logits
    &gt; 如果是简单的任务, 则可以通过env反馈奖励(与r1相同), 参考 [RAGEN](https://github.com/ZihanWang314/RAGEN)


。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/ji-yu-%20deepseek%20-de-%20r1%20-fu-xian-%2C%20-dui-chuan-tong-qiang-hua-xue-xi-de-si-kao-%20%28-wa-keng-%29.html</guid><pubDate>Tue, 11 Feb 2025 07:47:26 +0000</pubDate></item><item><title>幻兽帕鲁繁殖树搜索, 用于搜索从A帕鲁到B帕鲁的所有最短路径</title><link>https://FairyOwO.github.io/post/huan-shou-pa-lu-fan-zhi-shu-sou-suo-%2C%20-yong-yu-sou-suo-cong-A-pa-lu-dao-B-pa-lu-de-suo-you-zui-duan-lu-jing.html</link><description>## 相关原理

详见[这里](https://paldb.cc/cn/Breeding_Farm#BreedingFarm)

简单来说, 子帕鲁是两个父帕鲁繁殖力与1的和整除2, 特殊配方需要按照专有配方才可以繁殖

剪枝的 bfs 即可完成

## 准备数据

你需要从[帕鲁编年史](https://paldb.cc)中, 自行整理 `a.tsv` (所有帕鲁的CombiRank 繁殖力 IndexOrder 索引), `b.tsv` (特殊繁殖配方)
&gt; 此索引与帕鲁id不同
&lt;details&gt;&lt;summary&gt;两个表的格式示例&lt;/summary&gt;
&lt;p&gt;

## a.tsv

[帕鲁编年史 Breed Combi](https://paldb.cc/cn/Breeding_Farm)

```tsv
name	CombiRank	IndexOrder
皮皮鸡	1500	66
壶小象	1490	17
喵丝特	1480	7
```

&gt; 注意中间是 `\t` 制表符

## b.tsv

[帕鲁编年史 Breed Unique](https://paldb.cc/cn/Breeding_Farm#BreedUnique)
[帕鲁编年史 Breed Self](https://paldb.cc/cn/Breeding_Farm#BreedSelf)


```tsv
parent	孵化完成
佩克龙  伏特喵	派克龙
炎魔羊  噬魂兽	暗魔羊
喵丝特  企丸丸	冰丝特
```
&gt; 注意中间是 `\t` 制表符, parent中间是两个空格

&lt;/p&gt;
&lt;/details&gt; 

&gt; 注意语言

## 代码

&lt;details&gt;&lt;summary&gt;未整理代码&lt;/summary&gt;
&lt;p&gt;

```python
from collections import deque
import csv

def load_data():
    data = {}
    with open(r'a.tsv', 'r', newline='', encoding='utf-8') as f:
        reader = csv.reader(f, delimiter='\t')
        next(reader)
        for row in reader:
            data[row[0]] = {'fertility': int(row[1]), 'index_order': int(row[2])}

    data2 = {}
    exclusive = set()

    with open(r'b.tsv', 'r', newline='', encoding='utf-8') as f:
        reader = csv.reader(f, delimiter='\t')
        next(reader)
        for row in reader:
            parents = tuple(sorted(row[0].split('  ')))
            child = row[1]
            data2[parents] = child
            exclusive.add(child)

    return data, data2, exclusive

pal_data, special_breeding, exclusive_pals = load_data()

def breed(parent1, parent2):
    key = tuple(sorted([parent1, parent2]))
    if key in special_breeding:
        return special_breeding[key]
    
    # 塔主处理
    p1_fertility = pal_data[parent1]['fertility']
    p2_fertility = pal_data[parent2]['fertility']
    if p1_fertility == 9999 or p2_fertility == 9999:
        return '皮皮鸡'
    
    value = (p1_fertility + p2_fertility + 1) // 2
    
    valid_pals = [
        (name, info) for name, info in pal_data.items() 
        if name not in exclusive_pals
    ]
    
    closest_pal = min(
        valid_pals,
        key=lambda x: (abs(x[1]['fertility'] - value), x[1]['index_order'])
    )
    return closest_pal[0]

def find_breeding_path(start, end):
    if start not in pal_data or end not in pal_data:
        return []
    
    target_fertility = pal_data[end]['fertility']
    visited = {}
    queue = deque([(start, [start])])
    results = []
    min_steps = None

    while queue:
        level_size = len(queue)
        found = False
        
        for _ in range(level_size):
            current_pal, path = queue.popleft()
            
            if min_steps and len(path) &gt; min_steps:
                continue
                
            if current_pal == end:
                if not min_steps:
                    min_steps = len(path)
                if len(path) == min_steps:
                    results.append(path)
                found = True
                continue
                
            # 获取当前帕鲁繁殖力
            current_fertility = pal_data[current_pal]['fertility']
            
            for mate in pal_data:
                if mate == current_pal:
                    continue
                
                key = tuple(sorted([current_pal, mate]))
                
                # 特殊繁殖直接处理
                if key in special_breeding:
                    child = special_breeding[key]
                    new_path = path + [mate, child]
                    if child not in visited or len(new_path) &lt; visited.get(child, float('inf')):
                        visited[child] = len(new_path)
                        queue.append((child, new_path))
                    continue
                
                mate_fertility = pal_data[mate]['fertility']
                
                # 塔主特殊处理
                if current_fertility == 9999 or mate_fertility == 9999:
                    child = '皮皮鸡'
                else:
                    if current_fertility &lt; target_fertility and mate_fertility &lt; current_fertility:
                        continue  # 配偶生育力更低，子代只会更小
                    if current_fertility &gt; target_fertility and mate_fertility &gt; current_fertility:
                        continue  # 配偶生育力更高，子代只会更大
                    
                    child = breed(current_pal, mate)
                
                new_path = path + [mate, child]
                
                # 如果子代生育力偏离目标方向
                child_fertility = pal_data[child]['fertility']
                if (target_fertility &gt; current_fertility and child_fertility &lt; current_fertility) or \
                   (target_fertility &lt; current_fertility and child_fertility &gt; current_fertility):
                    continue
                
                if child not in visited or len(new_path) &lt; visited[child]:
                    visited[child] = len(new_path)
                    queue.append((child, new_path))
                elif len(new_path) == visited[child]:
                    queue.append((child, new_path))
        
        if found:
            break

    final_results = [p for p in results if len(p) == min_steps]
    return final_results if final_results else []

start_pal = '混沌骑士'
end_pal = '八云犬'
paths = find_breeding_path(start_pal, end_pal)

if paths:
    print(f'找到{len(paths)}条最短繁殖路径（步数：{(len(paths[0])-1)//2}）:')
    for i, path in enumerate(paths, 1):
        print(f'\n路径{i}:')
        for j in range(0, len(path)-1, 2):
            print(f'{path[j]} + {path[j+1]} → {path[j+2]}')
else:
    print('无可行繁殖路径')

```

&lt;/p&gt;
&lt;/details&gt; 

&gt; 不知道为什么, 相对于帕鲁编年史的实现, 我的实现慢一点, 估计是他预计算好了。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/huan-shou-pa-lu-fan-zhi-shu-sou-suo-%2C%20-yong-yu-sou-suo-cong-A-pa-lu-dao-B-pa-lu-de-suo-you-zui-duan-lu-jing.html</guid><pubDate>Wed, 05 Feb 2025 03:03:11 +0000</pubDate></item><item><title>2024 Advent of Code 复盘与答案 (二) (9-16题)</title><link>https://FairyOwO.github.io/post/2024%20Advent%20of%20Code%20-fu-pan-yu-da-an-%20%28-er-%29%20%289-16-ti-%29.html</link><description>[Advent of Code](https://adventofcode.com/)

使用 python 编写, 没有整理代码, 所以非常乱(变量乱取名, 没有注释, 逻辑奇怪, 并非最佳实现)
可以使用 gpt 相关工具辅助查看

&gt; 如果没有特意说明, 变量 a 统一存放所有原始字符串

## 第九题

[https://adventofcode.com/2024/day/9](https://adventofcode.com/2024/day/9)

### 1题

一个磁盘中的文件表示方法, 每两位数字各表示, x块文件, x块空的区域
例如
`12345`
表示 一块文件 两块空文件 三块文件 四块空文件 五块文件
每个文件从前往后的id为他们在磁盘中的顺序, 例如上述可以表示为
`0..111....22222`

希望从后往前的将文件塞入从前往后中空的地方

例如
```text
0..111....22222
02.111....2222.
022111....222..
0221112...22...
02211122..2....
022111222......
```

最后输出每个块位置*id号之和

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
# 构建
t = []
flag = True
file_id = 0
for i in list(a):
    if flag:
        for _ in range(int(i)):
            t.append(file_id)
        flag = False
        file_id += 1
    else:
        for _ in range(int(i)):
            t.append('.')
        flag = True

i = 0
j = len(t) - 1

while i &lt; j:
    if t[i] == '.':
        if t[j] != '.':
            t[i], t[j] = t[j], t[i]
        else:
            i -= 1
        j -= 1
    i += 1

ans = 0
for i, j in enumerate(t):
    if j != '.':
        ans += i * j
print(ans)

```

&lt;/p&gt;
&lt;/details&gt; 

### 2题

在一题的基础上, 修改从后往前放入从前往后空区域的方法, 从单个块移动转换为整个文件移动, 例如
```text
00...111...2...333.44.5555.6666.777.888899
0099.111...2...333.44.5555.6666.777.8888..
0099.1117772...333.44.5555.6666.....8888..
0099.111777244.333....5555.6666.....8888..
00992111777.44.333....5555.6666.....8888..
```

最后输出每个块位置*id号之和

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
# 构建
t = []
flag = True
file_id = 0
for i in list(a):
    if flag:
        for _ in range(int(i)):
            t.append(file_id)
        flag = False
        file_id += 1
    else:
        for _ in range(int(i)):
            t.append('.')
        flag = True


def get_file_size(file_id):
    return t.count(file_id)

for i in range(file_id - 1, -1, -1):
    file_size = get_file_size(i)
    file_index = t.index(i)
    flag = False
    size = 0
    for id, j in enumerate(t):
        if j == '.':
            flag = True
        else:
            flag = False
            size = 0
        
        if flag:
            size += 1
            if size == file_size:
                # change位置
                for k in range(id, id - file_size, -1):
                    t[k], t[file_index] = t[file_index], t[k]
                    file_index += 1

                break
        if id &gt;= file_index:
            break

ans = 0
for i, j in enumerate(t):
    if j != '.':
        ans += i * j
print(ans)

```

&lt;/p&gt;
&lt;/details&gt; 

## 第十题

[https://adventofcode.com/2024/day/10](https://adventofcode.com/2024/day/10)

### 1题

给定一幅由数字组成的地图, 从 0 开始, 一步一步上下左右移动到 9
求一副图中, 每一个 0 能到达的 9 有多少个, 输出他们的和

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
aaa = []
for i in a.splitlines():
    aaa.append(list(map(int, list(i))))

# print(aaa)

# 获取所有 0 跟 9 的位置
pos_0 = []
pos_9 = []
for i in range(len(aaa)):
    for j in range(len(aaa[i])):
        if aaa[i][j] == 0:
            pos_0.append((i, j))
        elif aaa[i][j] == 9:
            pos_9.append((i, j))

def get_allow_pos(pos):
    for i in [(0, 1), (1, 0), (-1, 0), (0, -1)]:
        next_step = (pos[0] + i[0], pos[1] + i[1])
        if len(aaa) &gt; next_step[0] &gt;= 0 and len(aaa[0]) &gt; next_step[1] &gt;= 0:
            if aaa[next_step[0]][next_step[1]] == aaa[pos[0]][pos[1]] + 1:
                yield (pos[0] + i[0], pos[1] + i[1])


def dfs(pos, pos_9):
    if pos == pos_9:
        return 1
    aa = 0
    for p in get_allow_pos(pos):
        aa = dfs(p, pos_9)
        if aa &gt; 0:
            return aa
    return aa

ans = 0
for i in pos_0:
    for j in pos_9:
        ans += dfs(i, j)

print(ans)

``` 

&lt;/p&gt;
&lt;/details&gt; 

### 2题

求 0 到每一个 9 有多少种不同的走法 (注意与第一题的区别, 第一题只要求到达, 第二题需要找到所有路线)

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
aaa = []
for i in a.splitlines():
    aaa.append(list(map(int, list(i))))

# print(aaa)

# 获取所有 0 跟 9 的位置
pos_0 = []
pos_9 = []
for i in range(len(aaa)):
    for j in range(len(aaa[i])):
        if aaa[i][j] == 0:
            pos_0.append((i, j))
        elif aaa[i][j] == 9:
            pos_9.append((i, j))

def get_allow_pos(pos):
    for i in [(0, 1), (1, 0), (-1, 0), (0, -1)]:
        next_step = (pos[0] + i[0], pos[1] + i[1])
        if len(aaa) &gt; next_step[0] &gt;= 0 and len(aaa[0]) &gt; next_step[1] &gt;= 0:
            if aaa[next_step[0]][next_step[1]] == aaa[pos[0]][pos[1]] + 1:
                yield (pos[0] + i[0], pos[1] + i[1])


def dfs(pos, pos_9):
    if pos == pos_9:
        return 1
    aa = 0
    for p in get_allow_pos(pos):
        aa = dfs(p, pos_9)
        if aa &gt; 0:
            return aa
    return aa

ans = 0
for i in pos_0:
    for j in pos_9:
        ans += dfs(i, j)

print(ans)

```

&lt;/p&gt;
&lt;/details&gt; 

&gt; 在一题的基础上, dfs固定返回1 变成 dfs 返回上一次dfs + 1

## 第十一题

[https://adventofcode.com/2024/day/11](https://adventofcode.com/2024/day/11)

### 1题

给定一串数字, 根据以下规则变换

1. 如果数字是0, 则数字是1
2. 如果数字是偶数位, 则数字变成两个数字, 左半跟右半, 例如 1000 变成 10 跟 00, 不保留前导0, 00变成0
3. 如果没有碰到前两条规则, 则数字=数字*2024

顺序都会被保留 (但是这题没有用到, 而且会误导第二题)
模拟上述规则25次

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
aa = a.split(' ')

for _ in range(25):
    i = 0
    while i &lt; len(aa):
        if aa[i] == '0':
            aa[i] = '1'
        elif len(aa[i]) % 2 == 0:
            left = aa[i][:len(aa[i]) // 2]
            left = str(int(left))
            right = aa[i][len(aa[i]) // 2:]
            right = str(int(right))
            aa.insert(i+1, right)
            aa[i] = left
            i += 1
        else:
            aa[i] = str(int(aa[i]) * 2024)
        
        i += 1

print(len(aa))
```

&lt;/p&gt;
&lt;/details&gt; 

&gt; 模拟即可, 25次蛮少的可以直接出来

### 2题

在1题的基础上, 模拟75次

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python
from collections import defaultdict
from tqdm import tqdm
aa = list(map(int, a.split(' ')))

def get_length(num):
    i = 0
    while num &gt; 0:
        num //= 10
        i += 1
    return i

t = defaultdict(int)

for i in aa:
    t[i] += 1

for _ in tqdm(range(75)):
    tt = defaultdict(int)
    for i, j in t.items():
        length = get_length(i)
        if i == 0:
            tt[1] += j
        elif length % 2 == 0:
            tt[i // 10 ** (length // 2)] += j
            tt[i % 10 ** (length // 2)] += j
        else:
            tt[i * 2024] += j
        
        t = tt

print(sum(t.values()))
```

&lt;/p&gt;
&lt;/details&gt; 

&gt; `tqdm` 是为了监控速度, 非必要引入
&gt; 与第一题不同, 这题指数爆炸, 75次会超时, 因为答案不要求顺序, 所以可以用缓存

## 第十二题

[https://adventofcode.com/2024/day/12](https://adventofcode.com/2024/day/12)

### 1题

划分区域

```text
AAAA
BBCD
BBCC
EEEC
```

划分成

```text
+-+-+-+-+
|A A A A|
+-+-+-+-+     +-+
              |D|
+-+-+   +-+   +-+
|B B|   |C|
+   +   + +-+
|B B|   |C C|
+-+-+   +-+ +
          |C|
+-+-+-+   +-+
|E E E|
+-+-+-+
```

分为五个区域, 
计算每个区域的周长*面积之和

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python

aa = []
for i in a.splitlines():
    aa.append(list(i))

def get_perimeter(arr):
    perimeter = 0
    for i in range(len(arr)):
        for j in range(len(arr[0])):
            if arr[i][j] == 1:
                perimeter += 4
                if i &gt; 0 and arr[i - 1][j] == 1:
                    perimeter -= 1
                if j &gt; 0 and arr[i][j - 1] == 1:
                    perimeter -= 1
                if i &lt; len(arr) - 1 and arr[i + 1][j] == 1:
                    perimeter -= 1
                if j &lt; len(arr[0]) - 1 and arr[i][j + 1] == 1:
                    perimeter -= 1
    
    return perimeter

def get_area(arr):
    area = 0
    for i in range(len(arr)):
        for j in range(len(arr[0])):
            if arr[i][j] == 1:
                area += 1
    return area

def get_allow_pos(pos):
    allow_pos = [(1, 0), (0, 1), (-1, 0), (0, -1)]
    char = aa[pos[0]][pos[1]]
    for i in allow_pos:
        next_pos = (pos[0] + i[0], pos[1] + i[1])
        if len(aa) &gt; next_pos[0] &gt;= 0 and len(aa[0]) &gt; next_pos[1] &gt;= 0:
            if aa[next_pos[0]][next_pos[1]] == char:
                yield next_pos


ans = 0

already_visited = np.zeros((len(aa), len(aa[0])))
for i in range(len(aa)):
    for j in range(len(aa[i])):
        if already_visited[i][j] == 0:
            # print(aa[i][j])
            array_ = np.zeros((len(aa), len(aa[0])))
            already_visited[i][j] = 1
            array_[i][j] = 1
            def dfs(pos):
                for next_pos in get_allow_pos(pos):
                    if already_visited[next_pos[0]][next_pos[1]] == 0:
                        already_visited[next_pos[0]][next_pos[1]] = 1
                        array_[next_pos[0]][next_pos[1]] = 1
                        dfs(next_pos)
            
            dfs((i, j))
            area = get_area(array_)
            perimeter = get_perimeter(array_)
            ans += area * perimeter
        
print(ans)

```

&lt;/p&gt;
&lt;/details&gt; 

&gt; 加一个正方形边长+4, 如果旁边每有一个正方形就-1的边长

### 2题

1题的边长变成边的数量

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python

aa = []
for i in a.splitlines():
    aa.append(list(i))



def get_area(arr):
    area = 0
    for i in range(len(arr)):
        for j in range(len(arr[0])):
            if arr[i][j] == 1:
                area += 1
    return area

def get_perimeter(region):
        
    max_x = len(region) - 1
    max_y = len(region[0]) - 1
    min_x = min_y = 0
    
    def state(x, y):
        if x &lt; 0 or x &gt; max_x or y &lt; 0 or y &gt; max_y:
            return False
        return region[x][y]
    
    perimeter = 0
    
    # 垂直方向扫描
    for i in range(max_x + 1):
        st = state(i, -1)
        for j in range(max_y + 2):
            if st != state(i, j):
                if st != state(i-1, j-1) or st == state(i-1, j):
                    perimeter += 1
                if st != state(i+1, j-1) or st == state(i+1, j):
                    perimeter += 1
                st = not st
    
    # 水平方向扫描
    for j in range(max_y + 1):
        st = state(-1, j)
        for i in range(max_x + 2):
            if st != state(i, j):
                if st != state(i-1, j-1) or st == state(i, j-1):
                    perimeter += 1
                if st != state(i-1, j+1) or st == state(i, j+1):
                    perimeter += 1
                st = not st
    
    return perimeter // 2

def get_allow_pos(pos):
    allow_pos = [(1, 0), (0, 1), (-1, 0), (0, -1)]
    char = aa[pos[0]][pos[1]]
    for i in allow_pos:
        next_pos = (pos[0] + i[0], pos[1] + i[1])
        if len(aa) &gt; next_pos[0] &gt;= 0 and len(aa[0]) &gt; next_pos[1] &gt;= 0:
            if aa[next_pos[0]][next_pos[1]] == char:
                yield next_pos

ans = 0

already_visited = np.zeros((len(aa), len(aa[0])))
for i in range(len(aa)):
    for j in range(len(aa[i])):
        if already_visited[i][j] == 0:
            # print(aa[i][j])
            array_ = np.zeros((len(aa), len(aa[0])))
            already_visited[i][j] = 1
            array_[i][j] = 1
            def dfs(pos):
                for next_pos in get_allow_pos(pos):
                    if already_visited[next_pos[0]][next_pos[1]] == 0:
                        already_visited[next_pos[0]][next_pos[1]] = 1
                        array_[next_pos[0]][next_pos[1]] = 1
                        
                        dfs(next_pos)
            
            dfs((i, j))
            area = get_area(array_)
            perimeter = get_perimeter(array_)
            # print(perimeter)
            ans += area * perimeter
        
print(ans)

```

&lt;/p&gt;
&lt;/details&gt; 

&gt; 一条边有两个角, 找到所有角之后整除2即可

## 第十三题

[https://adventofcode.com/2024/day/13](https://adventofcode.com/2024/day/13)

### 1题

按下按钮A或B, 移动老虎爪, 让老虎爪移动到指定的位置, 按下A需要三块钱, B需要一块钱
有可能无解(无法移动到指定位置)

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python

aa = a.split('\n\n')
from sympy import *

ans = 0
for i in aa:
    inputs = i.split('\n')
    # 提取输入
    ax, ay = inputs[0].split(': ')[1].split(', ')
    bx, by = inputs[1].split(': ')[1].split(', ')
    t1, t2 = inputs[2].split(': ')[1].split(', ')
    ax, ay = int(ax[2:]), int(ay[2:])
    bx, by = int(bx[2:]), int(by[2:])
    t1, t2 = int(t1[2:]), int(t2[2:])
    m = Symbol('m')
    n = Symbol('n')
    temp = solve([m * ax + n * bx - t1, m * ay + n * by - t2], [m, n])
    # print(temp)
    if temp[m].is_Integer and temp[n].is_Integer:
        ans += int(temp[m]) * 3 + int(temp[n])

print(ans)

```

&lt;/p&gt;
&lt;/details&gt; 
&gt; 二元一次方程组的整数解

### 2题

在一题的基础上, X轴和Y轴上都高出10000000000000(大数)

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python

aa = a.split('\n\n')
from sympy import *

ans = 0
for i in aa:
    inputs = i.split('\n')
    # 提取输入
    ax, ay = inputs[0].split(': ')[1].split(', ')
    bx, by = inputs[1].split(': ')[1].split(', ')
    t1, t2 = inputs[2].split(': ')[1].split(', ')
    ax, ay = int(ax[2:]), int(ay[2:])
    bx, by = int(bx[2:]), int(by[2:])
    t1, t2 = int(t1[2:]) + 10000000000000, int(t2[2:]) + 10000000000000
    m = Symbol('m')
    n = Symbol('n')
    temp = solve([m * ax + n * bx - t1, m * ay + n * by - t2], [m, n])
    # print(temp)
    if temp[m].is_Integer and temp[n].is_Integer:
        ans += int(temp[m]) * 3 + int(temp[n])

print(ans)
```

&lt;/p&gt;
&lt;/details&gt; 

&gt; python无限精度整数, 不需要额外处理

## 第十四题

[https://adventofcode.com/2024/day/14](https://adventofcode.com/2024/day/14)

### 1题

给定点坐标, 点的移动速度, 大小固定的图

求 四个象限内点的数量 之积
象限是去掉最中间一列与一行使得图分成四个区域
点到边界会传送到另一侧 (mod)

&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;
&lt;p&gt;

```python

def main():
    lines = a.splitlines()

    robots = []
    for line in lines:
        p_part, v_part = line.strip().split()
        p_x, p_y = map(int, p_part[2:].split(','))
        v_x, v_y = map(int, v_part[2:].split(','))
        robots.append({'p': (p_x, p_y), 'v': (v_x, v_y)})

    positions = {}
    for robot in robots:
        x = (robot['p'][0] + robot['v'][0] * 100) % 101
        y = (robot['p'][1] + robot['v'][1] * 100) % 103
        positions[(x, y)] = positions.get((x, y), 0) + 1

    q1 = q2 = q3 = q4 = 0
    for (x, y), count in positions.items():
        if x &lt; 50 and y &lt; 51:
            q1 += count
        elif x &gt; 50 and y &lt; 51:
            q2 += count
        elif x &gt; 50 and y &gt; 51:
            q3 += count
        elif x &lt; 50 and y &gt; 51:
            q4 += count

    safety_factor = q1 * q2 * q3 * q4
    print(safety_factor)

if __name__ == '__main__':
    main()

```

&lt;/p&gt;
&lt;/details&gt; 

### 2题

直接贴原题, 因为原题是阅读理解

During the bathroom break, someone notices that these robots seem awfully similar to ones built and used at the North Pole. If they're the same type of robots, they should have a hard-coded Easter egg: very rarely, most of the robots should arrange themselves into a picture of a Christmas tree.
&gt; 在上厕所的时候，有人注意到这些机器人看起来与在北极建造和使用的机器人非常相似。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/2024%20Advent%20of%20Code%20-fu-pan-yu-da-an-%20%28-er-%29%20%289-16-ti-%29.html</guid><pubDate>Wed, 18 Dec 2024 02:03:04 +0000</pubDate></item><item><title>免训练的RAG pipeline</title><link>https://FairyOwO.github.io/post/mian-xun-lian-de-RAG%20pipeline.html</link><description>&gt; 本文的框架是 `llama_index`&#13;
&#13;
&gt; 整理参考自&#13;
&gt; [EasyRAG](https://github.com/BUAADreamer/EasyRAG)&#13;
&gt; [RAG 最佳实践](https://zhuanlan.zhihu.com/p/8861103446)&#13;
&#13;
## 数据读取与处理&#13;
&#13;
将其他类型转换为几种基础类型, 再将对基础类型进行解析&#13;
&#13;
使用到的基础类型为: `markdown`, `html`, `pdf`&#13;
&#13;
使用到的库为: [dify_rag](https://github.com/hustyichi/dify-rag)&#13;
是一个不错的基础类型解析方案, 解析成 `langchain` 的 `Document` 形式&#13;
&gt; 尽管他是给 `dify` 设计, 但他是通用式设计, 可以用在其他地方&#13;
&#13;
```sh&#13;
pip install dify_rag&#13;
```&#13;
&#13;
```python&#13;
from dify_rag.extractor.html_extractor import HtmlExtractor&#13;
from dify_rag.extractor.markdown_extractor import MarkdownExtractor&#13;
&#13;
documents = HtmlExtractor(r'path/to/data.html').extract()  # MarkdownExtractor(r'path/to/data.md').extract()&#13;
&#13;
# 转换成 llama_index 的 Document 格式&#13;
docs = []&#13;
for i in documents:&#13;
    i.metadata['titles'] = str(i.metadata['titles'])  # 兼容性问题, llama_index 不支持 titles 里用list[str]&#13;
    docs.append(Document(text=i.page_content, metadata=i.metadata))&#13;
&#13;
```&#13;
&#13;
## 切分文档&#13;
&#13;
`chunk_size` 在 256 512 1024 中选择, 这里选择的是 512&#13;
`chunk_overlap` 视存储成本, 这里选择是 40&#13;
&#13;
### 父子切分&#13;
&#13;
&gt; 将一份 Document 中的文档, 切分成父子的形式, 检索到子节点的时候, 使用父节点返回, 来拓展上下文&#13;
&#13;
```python&#13;
node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=[512 * 4, 512], chunk_overlap=40)&#13;
nodes = node_parser.get_nodes_from_documents(docs)&#13;
```&#13;
&#13;
### 普通切分&#13;
&#13;
```python&#13;
node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=40)&#13;
nodes = node_parser.get_nodes_from_documents(docs)&#13;
```&#13;
&#13;
### embedding&#13;
&#13;
最好的仍然是使用 `llm` 调整成的 `embedding`, 不过这里考虑到推理成本, 这里使用的是[BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)&#13;
&#13;
```python&#13;
embed_model = HuggingFaceEmbedding(model_name='BAAI/bge-m3', cache_folder='cache')&#13;
```&#13;
&#13;
TODO。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/mian-xun-lian-de-RAG%20pipeline.html</guid><pubDate>Fri, 13 Dec 2024 09:52:55 +0000</pubDate></item><item><title>2024 Advent of Code 复盘与答案 (一) 1-8题</title><link>https://FairyOwO.github.io/post/2024%20Advent%20of%20Code%20-fu-pan-yu-da-an-%20%28-yi-%29%201-8-ti.html</link><description>[Advent of Code](https://adventofcode.com/)&#13;
&#13;
使用 python 编写, 没有整理代码, 所以非常乱(变量乱取名, 没有注释, 逻辑奇怪, 并非最佳实现)&#13;
可以使用 gpt 相关工具辅助查看&#13;
&#13;
&gt; 如果没有特意说明, 变量 a 统一存放所有原始字符串&#13;
&#13;
## 第一题&#13;
&#13;
[https://adventofcode.com/2024/day/1](https://adventofcode.com/2024/day/1)&#13;
&#13;
### 1题&#13;
&#13;
请将数字配对并测量它们之间的距离。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/2024%20Advent%20of%20Code%20-fu-pan-yu-da-an-%20%28-yi-%29%201-8-ti.html</guid><pubDate>Mon, 02 Dec 2024 07:16:27 +0000</pubDate></item><item><title>使用k3s搭建的k8s集群搭建一些简单的应用</title><link>https://FairyOwO.github.io/post/shi-yong-k3s-da-jian-de-k8s-ji-qun-da-jian-yi-xie-jian-dan-de-ying-yong.html</link><description>&gt; 可能并非最佳实现, 没有进行系统性学习, 欢迎交流&#13;
&#13;
## 镜像&#13;
&#13;
### 镜像管理&#13;
&#13;
安装镜像管理平台 Harbor, 来为集群提供镜像源&#13;
&#13;
在[上一篇](https://fairyowo.github.io/post/shi-yong-%20k3s%20-da-jian-%20k8s%20-ji-qun-%28-shi-yong-guo-nei-jing-xiang-%29.html), 介绍了集群的搭建, 在那个时候配置了集群的镜像源, 也就是说, 这个集群有简单的拉镜像能力, 其次, 安装了 helm, 可以根据 helm 仓库拉取应用&#13;
&#13;
这里介绍拉取原始chart仓库的相关文件的方法安装&#13;
&#13;
1. 获取chart仓库的文件&#13;
  前往 [harbor-helm](harbor-helm) 的 github 仓库&#13;
  前往 release, 下载源代码, 解压之后拿到其中的 `templates` 文件夹, `Chart.yaml` 文件, `values.yaml` 文件&#13;
2. 修改 `values.yaml` 文件&#13;
  详见 附录一&#13;
    &gt; 我们的小集群既没有https, 又没有持久化存储, 所以这些都不需要写在配置文件中, 直接关掉这些功能即可&#13;
    &gt; admin 的初始化密码在网页登陆后也能改&#13;
3. 创建 harbor 专属的 namespace&#13;
  ```sh&#13;
  kubectl create namespace harbor&#13;
  ```&#13;
4. 启动&#13;
  ```sh&#13;
  cd /path/to/harbor&#13;
  helm --namespace harbor install harbor .&#13;
  ```&#13;
  如果对配置项后悔, 使用 以下命令更新&#13;
  ```sh&#13;
  helm --namespace harbor upgrade harbor .&#13;
  ```&#13;
&#13;
到 kubepi 中, 即可看到 harbor 的相关镜像在拉取了, 如果镜像配置正确的话, 过一段时间就会拉取成功&#13;
&gt; 在 kubepi 中他会不断重试拉取, 实际上会缓慢拉取成功&#13;
&gt; 我是2M小服务器拉了半个小时+&#13;
&#13;
在拉取成功之后, 第一时间在刚刚 `values.yaml` 中配置的url中登录, 修改admin密码(如果没有修改默认密码)&#13;
&#13;
### 向 Harbor 上传镜像&#13;
&#13;
需要一台使用 docker 的机器, 向 Harbor 上传镜像 `docker pull`&#13;
&gt; 这里我在安装好后才意识到, 我的集群使用的容器运行时 containerd, 好像没有能力对镜像进行修改, 只能对镜像进行部署, 所以令起了一台机子, 安装 docker, 配置镜像源(TODO)&#13;
&gt; 另一种方案是重装集群, 使用 docker 作为 容器运行时, 但我没有选择这个方案&#13;
&#13;
做好准备后 需要对 harbor 进行信任&#13;
&gt; 没有 https 导致的, 有 https 可以跳过这一步&#13;
&#13;
向 `/etc/docker/daemon.json` 写入&#13;
```text&#13;
'insecure-registries': ['harbor_ip:port']&#13;
```&#13;
&#13;
&gt; 你需要自行处理他的json语法&#13;
&#13;
后, 使用&#13;
&#13;
```sh&#13;
systemctl daemon-reload&#13;
systemctl restart docker&#13;
```&#13;
&#13;
重启 docker&#13;
&#13;
在配置好后, 可以推一个简单的 helloworld镜像&#13;
&#13;
```sh&#13;
docker login harbor_ip:port&#13;
# 这里填入你的账号与密码. 我这里是 admin 与相对于的密码&#13;
&#13;
docker run hello-world:latest&#13;
# 修改tag&#13;
docker tag hello-world:latest harbor_ip:port/library/hello-world:latest&#13;
docker push harbor_ip:port/library/hello-world:latest&#13;
```&#13;
&#13;
登录到 harbor 控制台, 即可看到刚刚推送上来的镜像&#13;
&#13;
## 集群拉取镜像&#13;
&#13;
&gt; 如果你没有 https, 则需要以下额外一步, 如果有, 则保证集群可以连接到 harbor 即可&#13;
&#13;
根据不同的集群搭建方法(这里是k3s), 将 harbor 添加进集群可以拉取的镜像源&#13;
&#13;
与普通添加镜像一致, 首先需要到 `/etc/rancher/k3s` 目录下&#13;
&#13;
向其中的 `registries.yaml` 添加内容&#13;
&#13;
```yaml&#13;
mirrors:&#13;
  harbor_ip:port:&#13;
    endpoint:&#13;
     - http://harbor_ip:port&#13;
&#13;
configs:&#13;
  harbor_ip:port:&#13;
    auth:&#13;
      username: 你的 harbor 账号&#13;
      password: 你的 harbor 密码&#13;
```&#13;
&#13;
&gt; 这里可能 `registries.yaml` 已经有内容了(配置过镜像源), 你需要根据 yaml 的语法自行处理他们的关系&#13;
&gt; 每一台集群都要这么做&#13;
&#13;
之后, 重启即可&#13;
```sh&#13;
systemctl restart k3s  # systemctl restart k3s-agent if agent&#13;
```&#13;
&#13;
在编写好 kubectl 使用的 yaml 后, 即可从 harbor 拉取镜像&#13;
&gt; 感谢 claude-sonnet 帮我写yaml&#13;
&#13;
## 实战&#13;
&#13;
这里搭建了一个求生之路2的服务器&#13;
&#13;
&gt; 这里应该有一个求生之路2的简单介绍&#13;
&#13;
### docker机子拉取镜像&#13;
&#13;
这里使用的是 [HoshinoRei/l4d2server-docker](https://github.com/HoshinoRei/l4d2server-docker)&#13;
&#13;
```sh&#13;
docker pull hoshinorei/l4d2server:edge&#13;
```&#13;
&#13;
### 提交镜像&#13;
&#13;
```sh&#13;
docker tag hoshinorei/l4d2server:edge harbor_ip:port/library/hoshinorei/l4d2server:edge&#13;
docker push harbor_ip:port/library/hoshinorei/l4d2server:edge&#13;
```&#13;
&#13;
### 编写 kubectl 使用的 yaml&#13;
&#13;
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```yaml&#13;
apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  name: l4d2server&#13;
spec:&#13;
  replicas: 1&#13;
  selector:&#13;
    matchLabels:&#13;
      app: l4d2server&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: l4d2server&#13;
    spec:&#13;
      containers:&#13;
      - name: l4d2server&#13;
        image: harbor_ip:port/library/hoshinorei/l4d2server:edge&#13;
        command: ['/home/steam/l4d2server/srcds_run', '-game left4dead2', '-secure', '+exec', 'server.cfg', '+map', 'c1m1_hotel', '-port', '27015', '-tickrate 60', '+sv_setmax 31']&#13;
        ports:&#13;
        - containerPort: 27015&#13;
          name: tcp-game&#13;
        - containerPort: 27015&#13;
          protocol: UDP&#13;
          name: udp-game&#13;
        volumeMounts:&#13;
        - name: addons&#13;
          mountPath: /home/steam/l4d2server/left4dead2/addons/&#13;
        - name: server-config&#13;
          mountPath: /home/steam/l4d2server/left4dead2/cfg/server.cfg&#13;
          subPath: server.cfg&#13;
        - name: host-file&#13;
          mountPath: /home/steam/l4d2server/left4dead2/host.txt&#13;
          subPath: host.txt&#13;
        - name: motd-file&#13;
          mountPath: /home/steam/l4d2server/left4dead2/motd.txt&#13;
          subPath: motd.txt&#13;
        - name: cfg&#13;
          mountPath: /home/steam/l4d2server/left4dead2/cfg/&#13;
        &#13;
      volumes:&#13;
        - name: addons&#13;
          hostPath:&#13;
            path: /root/l4d2/addons/&#13;
            type: Directory&#13;
        - name: server-config&#13;
          hostPath:&#13;
            path: /root/l4d2/cfg/&#13;
        - name: host-file&#13;
          configMap:&#13;
            name: l4d2server-host&#13;
        - name: motd-file&#13;
          configMap:&#13;
            name: l4d2server-motd&#13;
        - name: cfg&#13;
          hostPath:&#13;
            path: /root/l4d2/cfg/&#13;
            type: Directory&#13;
&#13;
---&#13;
apiVersion: v1&#13;
kind: Service&#13;
metadata:&#13;
  name: l4d2server&#13;
spec:&#13;
  type: NodePort&#13;
  ports:&#13;
  - name: tcp-game&#13;
    port: 27015&#13;
    targetPort: 27015&#13;
    protocol: TCP&#13;
    nodePort: 30015&#13;
  - name: udp-game&#13;
    port: 27015&#13;
    targetPort: 27015&#13;
    protocol: UDP&#13;
    nodePort: 30016&#13;
  selector:&#13;
    app: l4d2server&#13;
&#13;
---&#13;
apiVersion: v1&#13;
kind: ConfigMap&#13;
metadata:&#13;
  name: l4d2server-host&#13;
data:&#13;
  host.txt: |&#13;
    # 这里放置 host.txt 的内容&#13;
&#13;
---&#13;
apiVersion: v1&#13;
kind: ConfigMap&#13;
metadata:&#13;
  name: l4d2server-motd&#13;
data:&#13;
  motd.txt: |&#13;
    # 这里放置 motd.txt 的内容&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&gt; 具体的参数细节参考 [这里](https://www.bilibili.com/opus/736922474423255104)&#13;
&gt; 需要在当前目录创建 `host.txt` 与 `motd.txt`, 并在里面输入内容, 在服务器进入的时候会显示这些内容&#13;
&#13;
执行&#13;
&#13;
```sh&#13;
kubectl apply -f l4d2server.yaml&#13;
```&#13;
&#13;
### 其他&#13;
&#13;
&gt; 挂载了宿主机的目录来放置 l4d2 mod&#13;
&#13;
进入 kubepi 中, 查看具体被分配到了哪台机子上, 然后去那台机子的 ~/cfg 中, 放置 原始 l4d2 服务器的 cfg (可以通过刚开始的 docker 机子, 进入 docke 容器取得), 之后在 kubepi 重启即可&#13;
&#13;
如果需要添加 mods, 则将 mod 移动到 addons 跟 cfg 中即可(注意 linux 兼容性), 然后重启, 如果需要更改启动命令则需要修改原 yaml&#13;
&#13;
## 附录&#13;
&#13;
### 一&#13;
这里给出常用的 harbor 的 values.yaml 的选项, 复制自 [(https://blog.starry-s.moe/posts/2023/harbor-helm-chart/)](https://blog.starry-s.moe/posts/2023/harbor-helm-chart/)&#13;
&#13;
&lt;details&gt;&lt;summary&gt;修改的选项&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```yaml&#13;
expose:&#13;
# expose type, 可以设置为 ingress, clusterIP, nodePort, nodeBalancer，区分大小写&#13;
# 默认为 ingress（如果不想使用 80/443 标准端口，可以设置为 nodePort，端口为高位 3000X）&#13;
type: ingress&#13;
tls:&#13;
  # 是否启用 TLS (HTTPS)，建议启用&#13;
  enabled: true&#13;
  # TLS Certificate 的来源，可以为 auto, secret 或 none&#13;
  # 如果为 secret，需要在安装 Chart 之前先创建 TLS Secret&#13;
  # 1) auto: generate the tls certificate automatically&#13;
  # 2) secret: read the tls certificate from the specified secret.&#13;
  # The tls certificate can be generated manually or by cert manager&#13;
  # 3) none: configure no tls certificate for the ingress. If the default&#13;
  # tls certificate is configured in the ingress controller, choose this option&#13;
  certSource: secret&#13;
  secret:&#13;
    # The name of secret which contains keys named:&#13;
    # 'tls.crt' - the certificate&#13;
    # 'tls.key' - the private key&#13;
    secretName: 'harbor-tls'&#13;
    # Only needed when the 'expose.type' is 'ingress'.&#13;
    notarySecretName: 'harbor-tls'&#13;
ingress:&#13;
  hosts:&#13;
    # Ingress Host，如果需要允许任意域名/IP 都能访问，将其设置为空字符串（不建议）&#13;
    # 这里填写的域名务必能解析到当前集群&#13;
    core: harbor.example.com&#13;
    notary: notary.example.com&#13;
&#13;
# Harbor external URL&#13;
# 与 Ingress Host 相对应，如果启用了 TLS，那就是 https://&lt;domain&gt;&#13;
# 如果没启用 TLS，那就是 http://&lt;domain&gt;&#13;
# 如果 expose type 为 nodePort，则填写 http(s)://&lt;IP_ADDRESS&gt;:3000X (端口号不能丢)&#13;
externalURL: https://harbor.example.com&#13;
&#13;
# 持久卷配置，默认为 true，如果是测试环境可以设置为 enabled: false (重新安装 Chart 时仓库里所有的数据都会丢失，不建议！)&#13;
# 如果需要启用持久卷，可以在安装 Chart 之前提前创建好 PVC，并配置 subPath&#13;
persistence:&#13;
enabled: true&#13;
resourcePolicy: 'keep'&#13;
persistentVolumeClaim:&#13;
  registry:&#13;
    # 填写已经创建好的 PVC&#13;
    existingClaim: 'harbor-pvc'&#13;
    storageClass: ''&#13;
    # 如果共用一个 PVC，需要设置子目录&#13;
    subPath: 'registry'&#13;
    accessMode: ReadWriteOnce&#13;
    size: 5Gi&#13;
    annotations: {}&#13;
  jobservice:&#13;
    jobLog:&#13;
      existingClaim: 'harbor-pvc'&#13;
      storageClass: ''&#13;
      subPath: 'jobservice'&#13;
      accessMode: ReadWriteOnce&#13;
      size: 1Gi&#13;
      annotations: {}&#13;
  database:&#13;
    existingClaim: 'harbor-pvc'&#13;
    storageClass: ''&#13;
    subPath: 'database'&#13;
    accessMode: ReadWriteOnce&#13;
    size: 1Gi&#13;
    annotations: {}&#13;
  redis:&#13;
    existingClaim: 'harbor-pvc'&#13;
    storageClass: ''&#13;
    subPath: 'redis'&#13;
    accessMode: ReadWriteOnce&#13;
    size: 1Gi&#13;
    annotations: {}&#13;
  trivy:&#13;
    existingClaim: 'harbor-pvc'&#13;
    storageClass: ''&#13;
    subPath: 'trivy'&#13;
    accessMode: ReadWriteOnce&#13;
    size: 5Gi&#13;
    annotations: {}&#13;
&#13;
# Admin 初始密码&#13;
harborAdminPassword: 'Harbor12345'&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/shi-yong-k3s-da-jian-de-k8s-ji-qun-da-jian-yi-xie-jian-dan-de-ying-yong.html</guid><pubDate>Mon, 02 Dec 2024 07:13:01 +0000</pubDate></item><item><title>nailong数据集, 检测nailong的模型, 训练与推理 (一)</title><link>https://FairyOwO.github.io/post/nailong-shu-ju-ji-%2C%20-jian-ce-nailong-de-mo-xing-%2C%20-xun-lian-yu-tui-li-%20%28-yi-%29.html</link><description>&gt; 回归 老本行&#13;
&#13;
偶然得到 nailong 数据集, 分为两块, 一种是给[分类模型使用的数据集](https://huggingface.co/datasets/refoundd/NailongClassification), 另一种是给[目标检测模型使用的数据集](https://huggingface.co/datasets/refoundd/NailongDetection)&#13;
&#13;
&gt; 后者的数据量不是非常多(6张), 等到有足够多的数据或者我一时兴起手动标注在进行研究&#13;
&#13;
## 初版方案&#13;
&#13;
### 数据集选择&#13;
&#13;
在我刚接触这个数据集的时候, 数据集是只有奶龙的(无其他标签的数据), 这个时候第一个想法就是引入其他分类, 这里采用cifer10数据集的数据, 对数据集进行增广&#13;
&#13;
然而, cifer10 的数据分布毕竟与常见群聊内发送的图片不同, 我觉得会影响最终能力, 应该有选择性而不是随意添加其他类型的图片, 在一番搜索之后, 选中了 [表情包数据集](https://github.com/LLM-Red-Team/emo-visual-data)&#13;
&#13;
虽然这个数据集的原计划是用来检测 VLLM 的能力, 但我认为在我们这个任务中也可以使用&#13;
&#13;
### 模型&#13;
&#13;
在敲定数据集之后, 就开始挑选模型了, 因为是个人小项目, 这里采用我个人喜好的模型选择, 使用了 [convnext 系模型](https://github.com/facebookresearch/ConvNeXt)&#13;
&#13;
这个模型的论文是一篇非常经典的实验文, 里面大量探索了一些技巧对模型能力的影响 (各类消融实验), 虽然他是 2020 年推出, 但他对现在的卷积网络的训练技巧的指引很大&#13;
&#13;
具体细节可以搜索相关的模型解析, 这里不再赘述&#13;
&#13;
&lt;details&gt;&lt;summary&gt;model.py&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
# copy from facebook/ConvNeXt&#13;
import torch&#13;
import torch.nn as nn&#13;
import torch.nn.functional as F&#13;
from timm.models.layers import trunc_normal_, DropPath&#13;
from timm.models.registry import register_model&#13;
&#13;
class Block(nn.Module):&#13;
    r''' ConvNeXt Block. There are two equivalent implementations:&#13;
    (1) DwConv -&gt; LayerNorm (channels_first) -&gt; 1x1 Conv -&gt; GELU -&gt; 1x1 Conv; all in (N, C, H, W)&#13;
    (2) DwConv -&gt; Permute to (N, H, W, C); LayerNorm (channels_last) -&gt; Linear -&gt; GELU -&gt; Linear; Permute back&#13;
    We use (2) as we find it slightly faster in PyTorch&#13;
    &#13;
    Args:&#13;
        dim (int): Number of input channels.&#13;
        drop_path (float): Stochastic depth rate. Default: 0.0&#13;
        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.&#13;
    '''&#13;
    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):&#13;
        super().__init__()&#13;
        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv&#13;
        self.norm = LayerNorm(dim, eps=1e-6)&#13;
        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers&#13;
        self.act = nn.GELU()&#13;
        self.pwconv2 = nn.Linear(4 * dim, dim)&#13;
        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), &#13;
                                    requires_grad=True) if layer_scale_init_value &gt; 0 else None&#13;
        self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity()&#13;
&#13;
    def forward(self, x):&#13;
        input = x&#13;
        x = self.dwconv(x)&#13;
        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -&gt; (N, H, W, C)&#13;
        x = self.norm(x)&#13;
        x = self.pwconv1(x)&#13;
        x = self.act(x)&#13;
        x = self.pwconv2(x)&#13;
        if self.gamma is not None:&#13;
            x = self.gamma * x&#13;
        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -&gt; (N, C, H, W)&#13;
&#13;
        x = input + self.drop_path(x)&#13;
        return x&#13;
&#13;
class ConvNeXt(nn.Module):&#13;
    r''' ConvNeXt&#13;
        A PyTorch impl of : `A ConvNet for the 2020s`  -&#13;
          https://arxiv.org/pdf/2201.03545.pdf&#13;
&#13;
    Args:&#13;
        in_chans (int): Number of input image channels. Default: 3&#13;
        num_classes (int): Number of classes for classification head. Default: 1000&#13;
        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]&#13;
        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]&#13;
        drop_path_rate (float): Stochastic depth rate. Default: 0.&#13;
        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.&#13;
        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.&#13;
    '''&#13;
    def __init__(self, in_chans=3, num_classes=1000, &#13;
                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0., &#13;
                 layer_scale_init_value=1e-6, head_init_scale=1.,&#13;
                 ):&#13;
        super().__init__()&#13;
&#13;
        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers&#13;
        stem = nn.Sequential(&#13;
            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),&#13;
            LayerNorm(dims[0], eps=1e-6, data_format='channels_first')&#13;
        )&#13;
        self.downsample_layers.append(stem)&#13;
        for i in range(3):&#13;
            downsample_layer = nn.Sequential(&#13;
                    LayerNorm(dims[i], eps=1e-6, data_format='channels_first'),&#13;
                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),&#13;
            )&#13;
            self.downsample_layers.append(downsample_layer)&#13;
&#13;
        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks&#13;
        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] &#13;
        cur = 0&#13;
        for i in range(4):&#13;
            stage = nn.Sequential(&#13;
                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], &#13;
                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]&#13;
            )&#13;
            self.stages.append(stage)&#13;
            cur += depths[i]&#13;
&#13;
        self.norm = nn.LayerNorm(dims[-1], eps=1e-6) # final norm layer&#13;
        self.head = nn.Linear(dims[-1], num_classes)&#13;
&#13;
        self.apply(self._init_weights)&#13;
        self.head.weight.data.mul_(head_init_scale)&#13;
        self.head.bias.data.mul_(head_init_scale)&#13;
&#13;
    def _init_weights(self, m):&#13;
        if isinstance(m, (nn.Conv2d, nn.Linear)):&#13;
            trunc_normal_(m.weight, std=.02)&#13;
            nn.init.constant_(m.bias, 0)&#13;
&#13;
    def forward_features(self, x):&#13;
        for i in range(4):&#13;
            x = self.downsample_layers[i](x)&#13;
            x = self.stages[i](x)&#13;
        return self.norm(x.mean([-2, -1])) # global average pooling, (N, C, H, W) -&gt; (N, C)&#13;
&#13;
    def forward(self, x):&#13;
        x = self.forward_features(x)&#13;
        x = self.head(x)&#13;
        return x&#13;
&#13;
class LayerNorm(nn.Module):&#13;
    r''' LayerNorm that supports two data formats: channels_last (default) or channels_first. &#13;
    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with &#13;
    shape (batch_size, height, width, channels) while channels_first corresponds to inputs &#13;
    with shape (batch_size, channels, height, width).&#13;
    '''&#13;
    def __init__(self, normalized_shape, eps=1e-6, data_format='channels_last'):&#13;
        super().__init__()&#13;
        self.weight = nn.Parameter(torch.ones(normalized_shape))&#13;
        self.bias = nn.Parameter(torch.zeros(normalized_shape))&#13;
        self.eps = eps&#13;
        self.data_format = data_format&#13;
        if self.data_format not in ['channels_last', 'channels_first']:&#13;
            raise NotImplementedError &#13;
        self.normalized_shape = (normalized_shape, )&#13;
    &#13;
    def forward(self, x):&#13;
        if self.data_format == 'channels_last':&#13;
            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)&#13;
        elif self.data_format == 'channels_first':&#13;
            u = x.mean(1, keepdim=True)&#13;
            s = (x - u).pow(2).mean(1, keepdim=True)&#13;
            x = (x - u) / torch.sqrt(s + self.eps)&#13;
            x = self.weight[:, None, None] * x + self.bias[:, None, None]&#13;
            return x&#13;
&#13;
&#13;
model_urls = {&#13;
    'convnext_tiny_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth',&#13;
    'convnext_small_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth',&#13;
    'convnext_base_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth',&#13;
    'convnext_large_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth',&#13;
    'convnext_tiny_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth',&#13;
    'convnext_small_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth',&#13;
    'convnext_base_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth',&#13;
    'convnext_large_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth',&#13;
    'convnext_xlarge_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth',&#13;
}&#13;
&#13;
@register_model&#13;
def convnext_tiny(pretrained=False,in_22k=False, **kwargs):&#13;
    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)&#13;
    if pretrained:&#13;
        url = model_urls['convnext_tiny_22k'] if in_22k else model_urls['convnext_tiny_1k']&#13;
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu', check_hash=True)&#13;
        model.load_state_dict(checkpoint['model'])&#13;
    return model&#13;
&#13;
@register_model&#13;
def convnext_small(pretrained=False,in_22k=False, **kwargs):&#13;
    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], **kwargs)&#13;
    if pretrained:&#13;
        url = model_urls['convnext_small_22k'] if in_22k else model_urls['convnext_small_1k']&#13;
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')&#13;
        model.load_state_dict(checkpoint['model'])&#13;
    return model&#13;
&#13;
@register_model&#13;
def convnext_base(pretrained=False, in_22k=False, **kwargs):&#13;
    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], **kwargs)&#13;
    if pretrained:&#13;
        url = model_urls['convnext_base_22k'] if in_22k else model_urls['convnext_base_1k']&#13;
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')&#13;
        model.load_state_dict(checkpoint['model'])&#13;
    return model&#13;
&#13;
@register_model&#13;
def convnext_large(pretrained=False, in_22k=False, **kwargs):&#13;
    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], **kwargs)&#13;
    if pretrained:&#13;
        url = model_urls['convnext_large_22k'] if in_22k else model_urls['convnext_large_1k']&#13;
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')&#13;
        model.load_state_dict(checkpoint['model'])&#13;
    return model&#13;
&#13;
@register_model&#13;
def convnext_xlarge(pretrained=False, in_22k=False, **kwargs):&#13;
    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048], **kwargs)&#13;
    if pretrained:&#13;
        assert in_22k, 'only ImageNet-22K pre-trained ConvNeXt-XL is available; please set in_22k=True'&#13;
        url = model_urls['convnext_xlarge_22k']&#13;
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')&#13;
        model.load_state_dict(checkpoint['model'])&#13;
    return model&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&#13;
### 代码&#13;
&#13;
训练代码大部分都是模板, 不过, 我发现我还没有一个属于自己的 trainer, 趁着这次训练模型的时候补充一个&#13;
&#13;
使用别人的 trainer 难免会遇到 debug, 而代码不熟的情况, 自己写的 trainer 可以掌握各种细节&#13;
&#13;
在整理了一些以前代码后, 总结出了覆盖许多训练模型情况的流程, 趁着这个时候测试一下现在ai编码的能力, 将流程发给 claude-sonnet 后, 输出了一版代码, 在我的一些小修小补(补充日志)后, 就可以跑起来了&#13;
&#13;
&lt;details&gt;&lt;summary&gt;trainer.py&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
import gc&#13;
import json&#13;
import logging&#13;
import os&#13;
import shutil&#13;
&#13;
import torch&#13;
from torch import optim&#13;
from torch.amp import GradScaler&#13;
from torch.nn.utils import clip_grad_norm_&#13;
from torch.utils.tensorboard import SummaryWriter&#13;
from tqdm import tqdm&#13;
&#13;
example_config = {&#13;
    'model': 'model_name',&#13;
    'checkpoint_dir': './checkpoints',&#13;
    'tensorboard_dir': './tensorboard',&#13;
    'device': 'cuda',&#13;
    'enable_cudnn_benchmark': True,&#13;
    'enable_amp': False,&#13;
    'learning_rate': 1e-4,&#13;
    'betas': [0.9, 0.999],&#13;
    'eps': 1e-8,&#13;
    'enable_compile': False,&#13;
    'weight_decay': 0.05,&#13;
    'max_steps': 100000,&#13;
    'max_grad_norm': 1.0,&#13;
    'save_every': 10000,&#13;
    'gradient_accumulation_steps': 4&#13;
}&#13;
&#13;
&#13;
class Trainer:&#13;
    def __init__(self, config):&#13;
        self.config = config&#13;
        self.setup_logging()&#13;
        self.setup_device()&#13;
        self.setup_model()&#13;
        self.setup_training()&#13;
        &#13;
    def setup_logging(self):&#13;
        '''设置日志'''&#13;
        logging.basicConfig(&#13;
            level=logging.INFO,&#13;
            format='%(asctime)s %(levelname)s %(message)s'&#13;
        )&#13;
        self.logger = logging.getLogger(__name__)&#13;
        self.writer = SummaryWriter(self.config['tensorboard_dir'])&#13;
        &#13;
    def setup_device(self):&#13;
        '''设置设备'''&#13;
        self.device = torch.device(self.config['device'])&#13;
        torch.backends.cudnn.benchmark = self.config.get('enable_cudnn_benchmark', True)&#13;
        if self.device.type == 'cuda':&#13;
            self.logger.info(f'Using device: {self.device} ({torch.cuda.get_device_name()})')&#13;
        else:&#13;
            self.logger.info(f'Using device: {self.device}')&#13;
            &#13;
    def setup_model(self):&#13;
        '''设置模型、损失函数等'''&#13;
        self.model = self.build_model().to(self.device)&#13;
        if self.config.get('enable_compile', False):&#13;
            self.model.compile()&#13;
        self.criterion = self.build_criterion()&#13;
        &#13;
        # 打印模型信息&#13;
        n_parameters = sum(p.numel() for p in self.model.parameters())&#13;
        self.logger.info(f'Number of parameters: {n_parameters:,}')&#13;
        &#13;
    def setup_training(self):&#13;
        '''设置训练相关组件'''&#13;
        # 优化器&#13;
        self.optimizer = self.build_optimizer()&#13;
        &#13;
        # 学习率调度器&#13;
        self.scheduler = self.build_scheduler()&#13;
        &#13;
        # 梯度缩放器(用于混合精度训练)&#13;
        self.scaler = GradScaler(&#13;
            enabled=self.config.get('enable_amp', False)&#13;
        )&#13;
        self.gradient_accumulation_steps = self.config.get('gradient_accumulation_steps', 1)&#13;
        &#13;
        # 加载检查点&#13;
        self.steps = 0&#13;
        self.best_metric = {}&#13;
        self.load_checkpoint()&#13;
        &#13;
    def build_model(self):&#13;
        '''构建模型(需要子类实现)'''&#13;
        raise NotImplementedError&#13;
        &#13;
    def build_criterion(self):&#13;
        '''构建损失函数(需要子类实现)'''&#13;
        raise NotImplementedError&#13;
        &#13;
    def build_optimizer(self):&#13;
        '''构建优化器'''&#13;
        # 区分需要和不需要weight decay的参数&#13;
        decay_params = []&#13;
        no_decay_params = []&#13;
        for name, param in self.model.named_parameters():&#13;
            if 'bias' in name or 'norm' in name:&#13;
                no_decay_params.append(param)&#13;
            else:&#13;
                decay_params.append(param)&#13;
                &#13;
        opt_params = [&#13;
            {'params': decay_params, 'weight_decay': self.config['weight_decay']},&#13;
            {'params': no_decay_params, 'weight_decay': 0.0}&#13;
        ]&#13;
        &#13;
        return optim.AdamW(&#13;
            opt_params,&#13;
            lr=self.config['learning_rate'],&#13;
            betas=self.config.get('betas', (0.9, 0.999)),&#13;
            eps=self.config.get('eps', 1e-8)&#13;
        )&#13;
        &#13;
    def build_scheduler(self):&#13;
        '''构建学习率调度器(需要子类实现)'''&#13;
        return NotImplementedError&#13;
        &#13;
    def build_dataloader(self):&#13;
        '''构建数据加载器(需要子类实现)'''&#13;
        raise NotImplementedError&#13;
        &#13;
    def train_step(self, batch):&#13;
        '''单步训练(需要子类实现)'''&#13;
        raise NotImplementedError&#13;
        &#13;
    def validate(self):&#13;
        '''验证(需要子类实现)'''&#13;
        raise NotImplementedError&#13;
        &#13;
    def save_checkpoint(self, is_best=False):&#13;
        '''保存检查点'''&#13;
        state = {&#13;
            'model': self.model.state_dict(),&#13;
            'optimizer': self.optimizer.state_dict(),&#13;
            'scheduler': self.scheduler.state_dict(),&#13;
            'scaler': self.scaler.state_dict(),&#13;
            'steps': self.steps,&#13;
            'best_metric': self.best_metric,&#13;
            'config': self.config&#13;
        }&#13;
        &#13;
        # 保存最新检查点&#13;
        torch.save(&#13;
            state,&#13;
            os.path.join(self.config['checkpoint_dir'], 'latest.pt')&#13;
        )&#13;
        &#13;
        # 保存最佳检查点&#13;
        if is_best:&#13;
            shutil.copy(&#13;
                os.path.join(self.config['checkpoint_dir'], 'latest.pt'),&#13;
                os.path.join(self.config['checkpoint_dir'], 'best.pt')&#13;
            )&#13;
            &#13;
    def load_checkpoint(self):&#13;
        '''加载检查点'''&#13;
        checkpoint_path = os.path.join(&#13;
            self.config['checkpoint_dir'],&#13;
            'latest.pt'&#13;
        )&#13;
        &#13;
        if os.path.exists(checkpoint_path):&#13;
            checkpoint = torch.load(&#13;
                checkpoint_path,&#13;
                map_location=self.device&#13;
            )&#13;
            &#13;
            self.model.load_state_dict(checkpoint['model'])&#13;
            self.optimizer.load_state_dict(checkpoint['optimizer'])&#13;
            self.scheduler.load_state_dict(checkpoint['scheduler'])&#13;
            self.scaler.load_state_dict(checkpoint['scaler'])&#13;
            self.steps = checkpoint['steps']&#13;
            self.best_metric = checkpoint['best_metric']&#13;
            &#13;
            self.logger.info(f'Loaded checkpoint from {checkpoint_path}')&#13;
            self.logger.info(f'Training will resume from step {self.steps}')&#13;
    &#13;
    @staticmethod&#13;
    def is_better_performance(baseline_dict, compare_dict):&#13;
        '''&#13;
        判断compare_dict中的指标是否全面超过baseline_dict&#13;
        &#13;
        Args:&#13;
            baseline_dict: 基准字典,格式为 {指标名: 值}&#13;
            compare_dict: 比较字典,格式为 {指标名: 值} &#13;
        &#13;
        Returns:&#13;
            bool: 如果compare_dict中所有指标都严格大于baseline_dict则返回True,否则返回False&#13;
        '''&#13;
        if not baseline_dict:&#13;
            return True&#13;
        &#13;
        # 检查两个字典的键是否一致&#13;
        if set(baseline_dict.keys()) != set(compare_dict.keys()):&#13;
            return False&#13;
            &#13;
        # 检查每个指标是否都有提升&#13;
        for metric in baseline_dict:&#13;
            if compare_dict[metric] &lt;= baseline_dict[metric]:&#13;
                return False&#13;
                &#13;
        return True&#13;
            &#13;
    def train(self):&#13;
        '''训练流程'''&#13;
        train_loader = self.build_dataloader()&#13;
        self.model.train()&#13;
        &#13;
        self.logger.info('Start training...')&#13;
        pbar = tqdm(total=self.config['max_steps'], initial=self.steps)&#13;
        &#13;
        while self.steps &lt; self.config['max_steps']:&#13;
            for batch in train_loader:&#13;
                # 训练一步&#13;
                with torch.autocast(device_type=self.config['device'], enabled=self.config.get('enable_amp', False)):&#13;
                    loss = self.train_step(batch)&#13;
                self.scaler.scale(loss / self.gradient_accumulation_steps).backward()&#13;
                &#13;
                if (self.steps + 1) % self.gradient_accumulation_steps == 0:&#13;
                    # 梯度裁剪&#13;
                    if self.config.get('max_grad_norm', 0) &gt; 0:&#13;
                        self.scaler.unscale_(self.optimizer)&#13;
                        clip_grad_norm_(&#13;
                            self.model.parameters(),&#13;
                            self.config['max_grad_norm']&#13;
                        )&#13;
&#13;
                    # 优化器步进&#13;
                    self.scaler.step(self.optimizer)&#13;
                    self.scaler.update()&#13;
                    self.optimizer.zero_grad(set_to_none=True)&#13;
                self.scheduler.step()&#13;
                &#13;
                # 记录&#13;
                self.writer.add_scalar('train/loss', loss, self.steps)&#13;
                self.writer.add_scalar(&#13;
                    'train/lr',&#13;
                    self.scheduler.get_last_lr()[0],&#13;
                    self.steps&#13;
                )&#13;
                &#13;
                self.steps += 1&#13;
                pbar.update(1)&#13;
                &#13;
                # 验证和保存&#13;
                if self.steps % self.config['save_every'] == 0:&#13;
                    metric = self.validate()&#13;
                    for i in metric:&#13;
                        self.logger.info(f'Validation {i}: {metric[i]}')&#13;
                        self.writer.add_scalar(f'val/{i}', metric[i], self.steps)&#13;
                    &#13;
                    is_best = self.is_better_performance(self.best_metric, metric)&#13;
                    if is_best:&#13;
                        self.best_metric = metric&#13;
&#13;
                    self.model.train()&#13;
                    self.save_checkpoint(is_best)&#13;
                    &#13;
                if self.steps &gt;= self.config['max_steps']:&#13;
                    break&#13;
                &#13;
            gc.collect()&#13;
            torch.cuda.empty_cache()&#13;
                    &#13;
        pbar.close()&#13;
        self.logger.info('Training finished!')&#13;
&#13;
&#13;
def main():&#13;
    '''主函数'''&#13;
    # 加载配置&#13;
    with open('config.json') as f:&#13;
        config = json.load(f)&#13;
        &#13;
    # 创建输出目录&#13;
    os.makedirs(config['checkpoint_dir'], exist_ok=True)&#13;
    os.makedirs(config['tensorboard_dir'], exist_ok=True)&#13;
    &#13;
    # 训练&#13;
    trainer = Trainer(config)&#13;
    trainer.train()&#13;
    &#13;
if __name__ == '__main__':&#13;
    try:&#13;
        main()&#13;
    except KeyboardInterrupt:&#13;
        pass&#13;
``` &#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
`trainer.py` 简单整合了几个常用的训练手段, 比如混合精度训练, 梯度裁剪, 梯度累计, weight_decay(写死了), tensorboard的记录, 断点续训等操作, 需要注意的是, `trainer.py` 没有使用 epoch 作为训练进度, 而是用了更精细的 step (每次迭代参数即为一个step), 使用的时候需要自行实现模型构建, 损失函数构建 学习率调度器 数据集加载器, 单步训练, 验证的流程的子类实现&#13;
&#13;
然后将一些配置放到config中便于读取, 其中有一些配置是必须的, 其他则是子类实现的时候需要的&#13;
&#13;
听起来可能有点抽象, 下面是一个简单的trainer使用案例&#13;
&#13;
&lt;details&gt;&lt;summary&gt;trainer使用案例&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
import torchvision&#13;
import torch&#13;
from trainer import Trainer&#13;
from torchvision.models import resnet18&#13;
from torch.optim.lr_scheduler import LambdaLR&#13;
&#13;
&#13;
&#13;
transform = torchvision.transforms.Compose([&#13;
    torchvision.transforms.ToTensor(),&#13;
    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))&#13;
])&#13;
&#13;
class ConstantLambdaLR(LambdaLR):&#13;
    def __init__(self, optimizer, **kwargs):&#13;
        kwargs['optimizer'] = optimizer&#13;
        kwargs['lr_lambda'] = self._step_inner&#13;
        super().__init__(**kwargs)&#13;
&#13;
    def _step_inner(self, steps):&#13;
        return 1&#13;
&#13;
&#13;
class Cifer10Trainer(Trainer):&#13;
    def __init__(self, config):&#13;
        super().__init__(config)&#13;
&#13;
    def build_model(self):&#13;
        model = resnet18()&#13;
        model.fc = torch.nn.Linear(model.fc.in_features, 10)&#13;
        return model&#13;
    &#13;
    def build_criterion(self):&#13;
        return torch.nn.CrossEntropyLoss()&#13;
    &#13;
    def build_scheduler(self):&#13;
        return ConstantLambdaLR(self.optimizer)&#13;
    &#13;
    def build_dataloader(self):&#13;
        train_dataset = torchvision.datasets.CIFAR10(root='./temp', train=True, download=True, transform=transform)&#13;
        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.config['batch_size'], shuffle=True, num_workers=1)&#13;
        return train_loader&#13;
    &#13;
    def train_step(self, batch):&#13;
        inputs, labels = batch&#13;
        inputs, labels = inputs.to(self.device), labels.to(self.device)&#13;
        outputs = self.model(inputs)&#13;
        loss = self.criterion(outputs, labels)&#13;
        return loss&#13;
    &#13;
    def validate(self):&#13;
        self.model.eval()&#13;
        test_dataset = torchvision.datasets.CIFAR10(root='./temp', train=False, download=True, transform=transform)&#13;
        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=self.config['batch_size'], shuffle=False, num_workers=1)&#13;
        acc = []&#13;
        with torch.inference_mode():&#13;
            for batch in test_loader:&#13;
                inputs, labels = batch&#13;
                inputs, labels = inputs.to(self.device), labels.to(self.device)&#13;
                y_hat = self.model(inputs)&#13;
                acc.append((y_hat.argmax(dim=1) == labels).sum().item() / labels.size(0))&#13;
                &#13;
        return {'acc': sum(acc) / len(acc)}&#13;
                &#13;
&#13;
def main():&#13;
    config = {&#13;
        'model': 'resnet18',&#13;
        'checkpoint_dir': './checkpoints',&#13;
        'tensorboard_dir': './tensorboard',&#13;
        'device': 'cuda',&#13;
        'enable_cudnn_benchmark': True,&#13;
        'enable_amp': False,&#13;
        'learning_rate': 1e-3,&#13;
        'betas': [0.9, 0.999],&#13;
        'eps': 1e-8,&#13;
        'enable_compile': False,&#13;
        'weight_decay': 0.05,&#13;
        'max_steps': 500,&#13;
        'max_grad_norm': 1.0,&#13;
        'save_every': 100,&#13;
        'gradient_accumulation_steps': 1,&#13;
        'batch_size': 32&#13;
    }&#13;
    trainer = Cifer10Trainer(config)&#13;
    trainer.train()&#13;
    &#13;
if __name__ == '__main__':&#13;
    try:&#13;
        main()&#13;
    except KeyboardInterrupt:&#13;
        pass&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&gt; 代码使用cifer10数据集, resnet18作为模型训练的简单的流程&#13;
&#13;
有了流程接下来编写我们的训练代码&#13;
&#13;
&lt;details&gt;&lt;summary&gt;train.py(代码未整理完毕, 非初版代码, 仅供参考)&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
import os&#13;
import random&#13;
&#13;
import cv2&#13;
import numpy as np&#13;
from sklearn.metrics import f1_score&#13;
import torch&#13;
from PIL import Image&#13;
from torch.optim.lr_scheduler import LambdaLR&#13;
from torch.utils.data import DataLoader, Dataset&#13;
from datasets import load_dataset&#13;
from torchvision import transforms&#13;
&#13;
# from torchvision.models import resnet18&#13;
from model import convnext_base&#13;
from trainer import Trainer&#13;
&#13;
image_size = 224&#13;
batch_size = 32&#13;
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')&#13;
&#13;
&#13;
# def get_color_from_image(image_path):&#13;
#     '''&#13;
#     从纯色图片中获取RGB颜色值&#13;
#     返回: (R, G, B)元组&#13;
#     '''&#13;
#     # 读取图片&#13;
#     image = Image.open(image_path).convert('RGB')&#13;
#     # 转换为numpy数组&#13;
#     img_array = np.array(image)&#13;
    &#13;
#     # 获取图片中心点的颜色值&#13;
#     h, w = img_array.shape[:2]&#13;
#     center_color = img_array[h//2, w//2]&#13;
    &#13;
#     # 或者计算整个图片的平均颜色&#13;
#     average_color = img_array.mean(axis=(0,1)).astype(int)&#13;
    &#13;
#     return tuple(average_color)  # 或者 tuple(average_color)&#13;
&#13;
&#13;
# class AugmentationUtils:&#13;
#     @staticmethod&#13;
#     def add_color_mask(image, is_positive):&#13;
#         '''给图片添加颜色遮罩'''&#13;
#         # 转换为numpy数组并确保类型为uint8&#13;
#         image = np.array(image, dtype=np.uint8)&#13;
        &#13;
#         # 创建与图像相同大小的遮罩&#13;
#         mask = np.ones_like(image, dtype=np.uint8)&#13;
        &#13;
#         # 随机生成颜色&#13;
#         if is_positive:&#13;
#             color = [random.randint(0, 255) for _ in range(3)]&#13;
#         else:&#13;
#             color = get_color_from_image('22.png')&#13;
        &#13;
#         # 为遮罩赋予颜色    &#13;
#         for i in range(3):&#13;
#             mask[:, :, i] = color[i]&#13;
        &#13;
#         # 确保mask也是uint8类型&#13;
#         mask = mask.astype(np.uint8)&#13;
        &#13;
#         # 添加遮罩&#13;
#         alpha = 0.5  # 透明度&#13;
#         image = cv2.addWeighted(image, 1-alpha, mask, alpha, 0)&#13;
        &#13;
#         return Image.fromarray(image)&#13;
&#13;
#     @staticmethod&#13;
#     def embed_positive_in_negative(positive_img, negative_img):&#13;
#         '''在负样本中嵌入正样本'''&#13;
#         # 转换为numpy数组&#13;
#         pos_img = np.array(positive_img)&#13;
#         neg_img = np.array(negative_img)&#13;
        &#13;
#         # 确保图像是3通道的&#13;
#         if len(pos_img.shape) == 2:&#13;
#             pos_img = cv2.cvtColor(pos_img, cv2.COLOR_GRAY2BGR)&#13;
#         if len(neg_img.shape) == 2:&#13;
#             neg_img = cv2.cvtColor(neg_img, cv2.COLOR_GRAY2BGR)&#13;
        &#13;
#         # 获取负样本尺寸&#13;
#         h, w = neg_img.shape[:2]&#13;
#         pos_h, pos_w = pos_img.shape[:2]&#13;
        &#13;
#         # 计算合适的缩放比例&#13;
#         scale = min(&#13;
#             random.uniform(0.5, 0.8),&#13;
#             (w * 0.8) / pos_w,&#13;
#             (h * 0.8) / pos_h&#13;
#         )&#13;
        &#13;
#         # 缩放正样本&#13;
#         new_size = (int(pos_w * scale), int(pos_h * scale))&#13;
#         pos_img_resized = cv2.resize(pos_img, new_size)&#13;
        &#13;
#         # 确保有效的随机位置范围&#13;
#         max_x = max(0, w - new_size[0])&#13;
#         max_y = max(0, h - new_size[1])&#13;
        &#13;
#         # 随机选择插入位置&#13;
#         x = random.randint(0, max_x) if max_x &gt; 0 else 0&#13;
#         y = random.randint(0, max_y) if max_y &gt; 0 else 0&#13;
        &#13;
#         # 获取ROI区域并确保与缩放后的正样本具有相同的形状&#13;
#         roi = neg_img[y:y+new_size[1], x:x+new_size[0]]&#13;
        &#13;
#         # 确保ROI和pos_img_resized具有相同的形状和通道数&#13;
#         if roi.shape == pos_img_resized.shape:&#13;
#             # 混合图像&#13;
#             blended = cv2.addWeighted(roi, 0.3, pos_img_resized, 0.7, 0)&#13;
#             neg_img[y:y+new_size[1], x:x+new_size[0]] = blended&#13;
        &#13;
#         return Image.fromarray(neg_img)&#13;
    &#13;
#     @staticmethod&#13;
#     def embed_same(positive_img, negative_img):&#13;
#         '''在负样本中嵌入正样本'''&#13;
#         # 转换为numpy数组&#13;
#         pos_img = np.array(positive_img)&#13;
#         neg_img = np.array(negative_img)&#13;
        &#13;
#         # 确保图像是3通道的&#13;
#         if len(pos_img.shape) == 2:&#13;
#             pos_img = cv2.cvtColor(pos_img, cv2.COLOR_GRAY2BGR)&#13;
#         if len(neg_img.shape) == 2:&#13;
#             neg_img = cv2.cvtColor(neg_img, cv2.COLOR_GRAY2BGR)&#13;
        &#13;
#         # 获取负样本尺寸&#13;
#         h, w = neg_img.shape[:2]&#13;
#         pos_h, pos_w = pos_img.shape[:2]&#13;
        &#13;
#         # 计算合适的缩放比例&#13;
#         scale = min(&#13;
#             random.uniform(0.5, 0.8),&#13;
#             (w * 0.8) / pos_w,&#13;
#             (h * 0.8) / pos_h&#13;
#         )&#13;
        &#13;
#         # 缩放正样本&#13;
#         new_size = (int(pos_w * scale), int(pos_h * scale))&#13;
#         pos_img_resized = cv2.resize(pos_img, new_size)&#13;
        &#13;
#         # 确保有效的随机位置范围&#13;
#         max_x = max(0, w - new_size[0])&#13;
#         max_y = max(0, h - new_size[1])&#13;
        &#13;
#         # 随机选择插入位置&#13;
#         x = random.randint(0, max_x) if max_x &gt; 0 else 0&#13;
#         y = random.randint(0, max_y) if max_y &gt; 0 else 0&#13;
        &#13;
#         # 获取ROI区域并确保与缩放后的正样本具有相同的形状&#13;
#         roi = neg_img[y:y+new_size[1], x:x+new_size[0]]&#13;
        &#13;
#         # 确保ROI和pos_img_resized具有相同的形状和通道数&#13;
#         if roi.shape == pos_img_resized.shape:&#13;
#             # 混合图像&#13;
#             blended = cv2.addWeighted(roi, 0.3, pos_img_resized, 0.7, 0)&#13;
#             neg_img[y:y+new_size[1], x:x+new_size[0]] = blended&#13;
        &#13;
#         return Image.fromarray(neg_img)&#13;
&#13;
#     @staticmethod&#13;
#     def flip_image(image):&#13;
#         '''图片轴对称'''&#13;
#         return Image.fromarray(np.array(image)[:, ::-1])&#13;
    &#13;
#     @staticmethod&#13;
#     def mirror_half_image(image):&#13;
#         img_array = np.array(image)&#13;
    &#13;
#         # 获取图片尺寸&#13;
#         h, w = img_array.shape[:2]&#13;
        &#13;
#         # 取左半边&#13;
#         half_w = w // 2&#13;
#         left_half = img_array[:, :half_w]&#13;
        &#13;
#         # 水平翻转左半边得到右半边&#13;
#         right_half = left_half[:, ::-1]&#13;
        &#13;
#         # 拼接两个半边&#13;
#         mirrored = np.concatenate([left_half, right_half], axis=1)&#13;
        &#13;
#         return Image.fromarray(mirrored)&#13;
    &#13;
&#13;
# def augment_dataset(positive_images, negative_images):&#13;
#     aug_utils = AugmentationUtils()&#13;
#     augmented_data = []&#13;
    &#13;
#     # 增强正样本&#13;
#     for pos_img in positive_images:&#13;
#         img = Image.open(pos_img).convert('RGB')&#13;
#         # 原图&#13;
#         augmented_data.append((img, 1))&#13;
#         # 颜色遮罩&#13;
#         augmented_data.append((aug_utils.add_color_mask(img, True), 1))&#13;
#         # 轴对称&#13;
#         augmented_data.append((aug_utils.flip_image(img), 1))&#13;
#         # 镜像一半&#13;
#         augmented_data.append((aug_utils.mirror_half_image(img), 1))&#13;
#         # 嵌入相同&#13;
#         img_id = random.randint(0, len(positive_images)-1)&#13;
#         aaa = Image.open(positive_images[img_id]).convert('RGB')&#13;
#         augmented_data.append((aug_utils.embed_same(aaa, img), 1))&#13;
        &#13;
    &#13;
#     # 增强负样本&#13;
#     for i, neg_img in enumerate(negative_images):&#13;
#         img = Image.open(neg_img).convert('RGB')&#13;
#         # 原图&#13;
#         augmented_data.append((img, 0))&#13;
#         # 颜色遮罩&#13;
#         augmented_data.append((aug_utils.add_color_mask(img, False), 0))&#13;
#         # 镜像一半&#13;
#         augmented_data.append((aug_utils.mirror_half_image(img), 0))&#13;
#         # 嵌入正样本&#13;
#         pos_img = Image.open(positive_images[random.randint(0, len(positive_images)-1)]).convert('RGB')&#13;
#         augmented_data.append((aug_utils.embed_positive_in_negative(pos_img, img), 1))&#13;
#         # 嵌入相同&#13;
#         img_id = random.randint(0, len(negative_images)-1)&#13;
#         aaa = Image.open(negative_images[img_id]).convert('RGB')&#13;
#         augmented_data.append((aug_utils.embed_same(aaa, img), 0))&#13;
        &#13;
&#13;
        &#13;
#     # # 显示并保存&#13;
#     # for i, (img, label) in enumerate(augmented_data):&#13;
#     #     # img.show()&#13;
#     #     os.makedirs('aug_images', exist_ok=True)&#13;
#     #     img.save(f'aug_images/aug_{i}.jpg')&#13;
    &#13;
#     # 统计&#13;
#     print(f'Positive: {len([x for x, y in augmented_data if y == 1])}, Negative: {len([x for x, y in augmented_data if y == 0])}')&#13;
#     return augmented_data&#13;
&#13;
&#13;
class LinearWarmUpCosineAnnealingLR(LambdaLR):&#13;
    def __init__(self, optimizer, *, warmup_iters, max_learning_rate, min_lr, lr_decay_iters, **kwargs):&#13;
        self.warmup_iters = warmup_iters&#13;
        self.max_learning_rate = max_learning_rate&#13;
        self.lr_decay_iters = lr_decay_iters&#13;
        self.min_lr = min_lr&#13;
        kwargs['optimizer'] = optimizer&#13;
        kwargs['lr_lambda'] = self._step_inner&#13;
        super().__init__(**kwargs)&#13;
&#13;
    def _step_inner(self, steps):&#13;
        if steps &lt; self.warmup_iters:&#13;
            return self.max_learning_rate * steps / self.warmup_iters&#13;
        elif steps &lt; self.lr_decay_iters:&#13;
            return self.min_lr + 0.5 * (1.0 + np.cos((steps - self.warmup_iters) / (self.lr_decay_iters - self.warmup_iters)*np.pi)) * (self.max_learning_rate - self.min_lr)&#13;
        else:&#13;
            return self.min_lr&#13;
&#13;
&#13;
def transform_img(img):&#13;
    # 处理图片&#13;
    img_np = np.array(img)&#13;
    img_tensor = torch.from_numpy(img_np).permute(2, 0, 1)  # C, H, W&#13;
    img_tensor = torch.nn.functional.interpolate(img_tensor.unsqueeze(0), size=(image_size, image_size), mode='bilinear', align_corners=False).squeeze(0)&#13;
    # normalize&#13;
    normalized_img = img_tensor.float() / 255.0&#13;
    return normalized_img&#13;
&#13;
&#13;
transform = transforms.Compose([&#13;
    transforms.Resize((image_size, image_size)),&#13;
    transforms.ToTensor(),&#13;
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),&#13;
])&#13;
&#13;
&#13;
def transform_img_torchvision(data):&#13;
    data['x'] = [transform(img.convert('RGB')) for img in data['image']]&#13;
    return data&#13;
&#13;
&#13;
label_mapping = {&#13;
    'nailong': 0,&#13;
    'emoji': 1,&#13;
    'anime': 2,&#13;
    'others': 3,&#13;
    'long': 4&#13;
}&#13;
&#13;
def extract_datasets():&#13;
    ds = load_dataset('refoundd/NailongClassification', cache_dir='data', split='train')&#13;
    ds = ds.map(lambda x: {'label': label_mapping[x['label']]})&#13;
    ds = ds.map(transform_img_torchvision, remove_columns=['image'], batched=True)&#13;
    dataset = ds.train_test_split(test_size=0.2)&#13;
    return dataset&#13;
&#13;
dataset = extract_datasets()&#13;
&#13;
&#13;
class NaiLongDataset(Dataset):&#13;
    def __init__(self, mode='train'):&#13;
        assert mode in ['train', 'test']&#13;
        self.dataset = dataset[mode]&#13;
&#13;
    def __len__(self):&#13;
        return len(self.dataset)&#13;
&#13;
    def __getitem__(self, idx):&#13;
        item = self.dataset[idx]['x']&#13;
        label = self.dataset[idx]['label']&#13;
        return torch.tensor(item), torch.tensor(label)&#13;
&#13;
&#13;
&#13;
class NaiLongTrainer(Trainer):&#13;
    def __init__(self, config):&#13;
        super().__init__(config)&#13;
&#13;
    def build_model(self):&#13;
        # model = resnet18()&#13;
        # model.fc = torch.nn.Linear(model.fc.in_features, 2)&#13;
        # return model&#13;
        return convnext_base(pretrained=False, num_classes=5)&#13;
    &#13;
    def build_criterion(self):&#13;
        return torch.nn.CrossEntropyLoss()&#13;
    &#13;
    def build_scheduler(self):&#13;
        return LinearWarmUpCosineAnnealingLR(self.optimizer, warmup_iters=self.config['warmup_iters'], max_learning_rate=self.config['max_learning_rate'], min_lr=self.config['min_lr'], lr_decay_iters=self.config['lr_decay_iters'])&#13;
    &#13;
    def build_dataloader(self, mode='train'):&#13;
        dataset = NaiLongDataset(mode='train')&#13;
        return DataLoader(dataset, batch_size=batch_size, shuffle=True)&#13;
    &#13;
    def train_step(self, batch):&#13;
        x, y = batch&#13;
        x, y = x.to(device), y.to(device)&#13;
        return self.criterion(self.model(x), y)&#13;
    &#13;
    def validate(self):&#13;
        self.logger.info('Validating...')&#13;
        self.model.eval()&#13;
        dataloader = self.build_dataloader(mode='test')&#13;
        acc = []&#13;
        f1 = [[], []]&#13;
        with torch.no_grad(): &#13;
            for i, (x, y) in enumerate(dataloader):&#13;
                x, y = x.to(device), y.to(device)&#13;
                # print(f'Validation: {i}, {y}')&#13;
                y_hat = self.model(x)&#13;
                acc.append(torch.sum(torch.argmax(y_hat, dim=1) == y).item() / len(y))&#13;
                f1[0].extend(y.cpu().tolist())&#13;
                f1[1].extend(torch.argmax(y_hat, dim=1).cpu().tolist())&#13;
            f1_scores = f1_score(f1[0], f1[1], average='macro')&#13;
        return {'acc': sum(acc) / len(acc), 'f1': f1_scores}&#13;
&#13;
&#13;
def main():&#13;
    config = {  # test&#13;
        'model': 'convnext_tiny',&#13;
        'checkpoint_dir': './checkpoints',&#13;
        'tensorboard_dir': './tensorboard',&#13;
        'device': 'cuda',&#13;
        'enable_cudnn_benchmark': True,&#13;
        'enable_amp': False,&#13;
        'learning_rate': 1,  # 启动lr_scheduler 这里必须是1&#13;
        'betas': [0.9, 0.999],&#13;
        'eps': 1e-8,&#13;
        'enable_compile': False,&#13;
        'weight_decay': 0.0,&#13;
        'max_steps': 5000,&#13;
        'max_grad_norm': 1.0,&#13;
        'save_every': 500,&#13;
        'gradient_accumulation_steps': 1,&#13;
        'warmup_iters': 500,&#13;
        'max_learning_rate': 1e-3,&#13;
        'min_lr': 1e-4,&#13;
        'lr_decay_iters': 1000&#13;
    }&#13;
    os.makedirs(config['checkpoint_dir'], exist_ok=True)&#13;
    os.makedirs(config['tensorboard_dir'], exist_ok=True)&#13;
    trainer = NaiLongTrainer(config)&#13;
    trainer.train()&#13;
&#13;
if __name__ == '__main__':&#13;
    # 删除tensorboard下的文件, 但不删除文件夹&#13;
    for i in os.listdir('./tensorboard'):&#13;
        os.remove(os.path.join('./tensorboard', i))&#13;
    # 删除checkpoints下的文件&#13;
    for i in os.listdir('./checkpoints'):&#13;
        os.remove(os.path.join('./checkpoints', i))&#13;
    try:&#13;
        main()&#13;
    except KeyboardInterrupt:&#13;
        print('KeyboardInterrupt')&#13;
        &#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
### 数据增广&#13;
&#13;
原始数据集只有两百多张图片, 这个时候无法避免的要做数据增广, 扩展 nailong 标签的数据, 这里因为是初版方案, 也没有非常精细的增广方案, 这里使用了以下几种方式(代码在如上train.py中):&#13;
&#13;
- 给图片添加颜色遮罩&#13;
    让模型不要将遇到黄色的就判定为奶龙&#13;
- 在负样本中嵌入正样本&#13;
    很经典的增广数据的手法&#13;
- 图片轴对称&#13;
- 取图像的一半镜像翻转&#13;
&#13;
### 训练&#13;
&#13;
#### 参数搜索&#13;
&#13;
虽然是个人小项目, 简单的参数搜索不能少, 继续上面写的 `trainer.py`, 我也写了一个简单的 `hyperparameter_seacher.py` 来搜索超参&#13;
&#13;
&lt;details&gt;&lt;summary&gt;hyperparameter_seacher.py&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
from trainer import Trainer&#13;
import optuna&#13;
&#13;
&#13;
example_config = {&#13;
    'model': 'convnext_tiny',&#13;
    'checkpoint_dir': './checkpoints',&#13;
    'tensorboard_dir': './tensorboard',&#13;
    'device': 'cuda',&#13;
    'enable_cudnn_benchmark': True,&#13;
    'enable_amp': False,&#13;
    'learning_rate': 1e-4,&#13;
    'betas': [0.9, 0.999],&#13;
    'eps': 1e-8,&#13;
    'enable_compile': False,&#13;
    'weight_decay': 0.05,&#13;
    'max_steps': 100,&#13;
    'max_grad_norm': 1.0,&#13;
    'save_every': 1000000,  # 不保存&#13;
    'gradient_accumulation_steps': 4&#13;
}&#13;
&#13;
example_search_config = {&#13;
    'params': {&#13;
        'learning_rate': {&#13;
            'type': 'float',&#13;
            'range': [1e-5, 1e-2],&#13;
            'log': True&#13;
        },&#13;
        'gradient_accumulation_steps': {&#13;
            'type': 'int',&#13;
            'range': [1, 8],&#13;
            'log': False&#13;
        }&#13;
    },&#13;
    'if_save_info': False,&#13;
    'n_trials': 10&#13;
}&#13;
&#13;
class HyperparameterSearcher:&#13;
    def __init__(self, config, trainer):&#13;
        assert isinstance(trainer, Trainer), 'trainer must be an instance of Trainer'&#13;
        self.config = config&#13;
        self.trainer = trainer&#13;
        &#13;
    def objective(self, trial):&#13;
        search_params = self.config['params']&#13;
        &#13;
        for param_name, param_config in search_params.items():&#13;
            if param_config['type'] == 'float':&#13;
                self.trainer.config[param_name] = trial.suggest_float(&#13;
                    param_name,&#13;
                    param_config['range'][0],&#13;
                    param_config['range'][1],&#13;
                    log=param_config.get('log', False)&#13;
                )&#13;
            elif param_config['type'] == 'int':&#13;
                self.trainer.config[param_name] = trial.suggest_int(&#13;
                    param_name,&#13;
                    param_config['range'][0],&#13;
                    param_config['range'][1]&#13;
                )&#13;
            elif param_config['type'] == 'list':&#13;
                self.trainer.config[param_name] = trial.suggest_categorical(&#13;
                    param_name,&#13;
                    param_config['range']&#13;
                )&#13;
            else:&#13;
                raise ValueError(f'Unsupported parameter type: {param_config['type']}, only support float and int')&#13;
        &#13;
        self.trainer.setup_training()&#13;
        self.trainer.train()&#13;
        metric = self.trainer.validate()&#13;
        if 'acc' not in metric:&#13;
            raise ValueError('metric must contain 'acc'')&#13;
        return -metric['acc']  # only support maximizing acc&#13;
    &#13;
    def search(self):&#13;
        study = optuna.create_study(direction='maximize')&#13;
        study.optimize(self.objective, n_trials=self.config['n_trials'])&#13;
        print('Best params:', study.best_params)&#13;
        print('Best value:', -study.best_value)&#13;
        if self.config['if_save_info']:&#13;
            study.trials_dataframe().to_csv('./output/optuna_results.csv')&#13;
        return study.best_params&#13;
    &#13;
def main():&#13;
    &#13;
    pass&#13;
&#13;
if __name__ == '__main__':&#13;
    try:&#13;
        main()&#13;
    except KeyboardInterrupt:&#13;
        pass&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
超参搜索需要 trainer 的配合, 使用了与模型无关的 optuna 来跑给定范围的超参值, 这个时候可以给trainer一个比较容易训练的超参设置(短的epoch等), 同时关闭保存模式&#13;
&#13;
我也写了个简单的超参搜索的例子&#13;
&#13;
&lt;details&gt;&lt;summary&gt;hyperparameter_seacher使用案例&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
from example_trainer import Cifer10Trainer&#13;
from hyperparameter_seacher import HyperparameterSearcher&#13;
&#13;
class Cifer10HyperparameterSearcher(HyperparameterSearcher):&#13;
    def __init__(self, config, trainer):&#13;
        super().__init__(config, trainer)&#13;
&#13;
&#13;
def main():&#13;
    search_config = {&#13;
        'params': {&#13;
            'learning_rate': {&#13;
                'type': 'float',&#13;
                'range': [1e-5, 1e-2],&#13;
                'log': True&#13;
            },&#13;
            'gradient_accumulation_steps': {&#13;
                'type': 'int',&#13;
                'range': [1, 8],&#13;
                'log': False&#13;
            }&#13;
        },&#13;
        'if_save_info': True,&#13;
        'n_trials': 10&#13;
    }&#13;
&#13;
    trainer_config = {&#13;
        'model': 'resnet18',&#13;
        'checkpoint_dir': './checkpoints',&#13;
        'tensorboard_dir': './tensorboard',&#13;
        'device': 'cuda',&#13;
        'enable_cudnn_benchmark': True,&#13;
        'enable_amp': False,&#13;
        'learning_rate': 1e-3,&#13;
        'betas': [0.9, 0.999],&#13;
        'eps': 1e-8,&#13;
        'enable_compile': False,&#13;
        'weight_decay': 0.05,&#13;
        'max_steps': 500,&#13;
        'max_grad_norm': 1.0,&#13;
        'save_every': 10000,  # large than max_steps, no save&#13;
        'gradient_accumulation_steps': 4,&#13;
        'batch_size': 32&#13;
    }&#13;
    trainer = Cifer10Trainer(trainer_config)&#13;
    searcher = Cifer10HyperparameterSearcher(search_config, trainer)&#13;
    best_params = searcher.search()&#13;
    print(best_params)&#13;
&#13;
if __name__ == '__main__':&#13;
    main()&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&gt; 搜索上面那个例子中的合适的超参数&#13;
&#13;
在准备好这些后, 编写我们项目的超参搜索器&#13;
&#13;
&lt;details&gt;&lt;summary&gt;之后补充&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&gt; 搜索出来的最佳超参是一个很长的小数, 四舍五入合适的位数即可&#13;
&#13;
#### 第一次训练&#13;
&#13;
在准备好后, 开始第一次训练&#13;
&#13;
在较新的GPU下, 训练以前较小的模型可谓降维打击, 不到一个小时训练完毕&#13;
&#13;
然而, 第一个问题出来了&#13;
&#13;
acc很高, f1很低&#13;
&#13;
编写测试代码:&#13;
&#13;
&lt;details&gt;&lt;summary&gt;test.py(非初版代码, 仅供参考)&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
from sklearn.metrics import f1_score&#13;
import torch&#13;
&#13;
from model import convnext_base&#13;
from PIL import Image&#13;
import numpy as np&#13;
from glob import glob&#13;
from torchvision import transforms&#13;
&#13;
device = 'cuda'&#13;
image_size = 224&#13;
&#13;
model = convnext_base(pretrained=False, num_classes=5).to(device)&#13;
# model = resnet18()&#13;
# model.fc = torch.nn.Linear(model.fc.in_features, 2)&#13;
checkpoint = torch.load('./checkpoints/best.pt', map_location=device)&#13;
model.load_state_dict(checkpoint['model'])&#13;
&#13;
transform = transforms.Compose([&#13;
    transforms.Resize((image_size, image_size)),&#13;
    transforms.ToTensor(),&#13;
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),&#13;
])&#13;
&#13;
def transform_img(img):&#13;
    # 处理图片&#13;
    img_np = np.array(img)&#13;
    img_tensor = torch.from_numpy(img_np).permute(2, 0, 1)  # C, H, W&#13;
    img_tensor = torch.nn.functional.interpolate(img_tensor.unsqueeze(0), size=(image_size, image_size), mode='bilinear', align_corners=False).squeeze(0)&#13;
    # normalize&#13;
    normalized_img = img_tensor.float() / 255.0&#13;
    return normalized_img&#13;
&#13;
&#13;
def get_input_images(image_path):&#13;
    img = Image.open(image_path).convert('RGB')&#13;
    img = transform(img)&#13;
    return torch.tensor(img).to(device).unsqueeze(0)&#13;
&#13;
model.eval()&#13;
&#13;
# 导出onnx&#13;
input_names = ['input']&#13;
output_names = ['output']&#13;
dynamic_axes = {&#13;
    'input': {0: 'batch_size'},  # 输入的第一个维度是动态的&#13;
    'output': {0: 'batch_size'}  # 输出的第一个维度是动态的&#13;
}&#13;
torch.onnx.export(model, torch.randn(1, 3, 224, 224).to(device), 'model.onnx', input_names=input_names, output_names=output_names, dynamic_axes=dynamic_axes, opset_version=11)&#13;
&#13;
with torch.no_grad():&#13;
    # image = torch.randn(1, 3, 256, 256)&#13;
    print(torch.softmax(model(get_input_images('1.jpg')), dim=1))&#13;
    input()&#13;
    print(torch.softmax(model(get_input_images('3.jpg')), dim=1))&#13;
    input()&#13;
    acc = []&#13;
    f1 = [[], []]&#13;
    &#13;
    &#13;
    for file in glob('./datasets/nailong/*'):&#13;
        y_hat, y = model(get_input_images(file)), torch.tensor([0]).to(device)&#13;
        acc.append(torch.sum(torch.argmax(y_hat, dim=1) == y).item() / len(y))&#13;
        f1[0].append(y.cpu().tolist()[0])&#13;
        f1[1].append(torch.argmax(y_hat, dim=1).cpu().tolist()[0])&#13;
        &#13;
&#13;
    # for file in glob('./datasets/cifer10/*'):&#13;
    #     y_hat, y = model(get_input_images(file)), torch.tensor([3]).to(device)&#13;
    #     acc.append(torch.sum(torch.argmax(y_hat, dim=1) == y).item() / len(y))&#13;
    #     f1[0].append(y.cpu().tolist()[0])&#13;
    #     f1[1].append(torch.argmax(y_hat, dim=1).cpu().tolist()[0])&#13;
&#13;
print(sum(acc) / len(acc))&#13;
f1_scores = f1_score(f1[0], f1[1], average='macro')&#13;
print(f1_scores)&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&#13;
发现 acc 与 f1 的值非常低(百分之10到百分之20附近)&#13;
&#13;
这个时候查看模型的输出, 发现模型的输出接近初始化的输出(softmax后), 模型没有怎么被训练&#13;
&#13;
这个时候怀疑是训练代码出了问题, 然而 cifer10 的 训练并没有什么问题&#13;
检查数据增广代码, 查看增广后的图片, 发现增广做的不是很好, 正样本内嵌负样本没嵌好&#13;
&#13;
在经过修改后, 重启训练&#13;
然而, 问题变成了, 模型的输出接近(0.5, 0.5)(二分类任务)&#13;
&#13;
跟人讨论后, 认为是数据集难度太大, 检查表情包数据集, 都是一些分布与 nailong 数据差异很大的图片. &#13;
&#13;
&gt; 非严格推理, 纯脑测&#13;
模型发现, 给一张新的图片预测 nailong 类, 还是其他类, 都会导致loss上升, 于是干脆摆烂乱猜,  最终的概率分布会输出数据分布, 经过数据增广后的数据恰好是两类 1:1, 模型退化成统计数据集了&#13;
&#13;
导致这个的最直接原因是输入特征不够, 到图像分类就是模型找不到决定图片分类的模式&#13;
&#13;
于是, 第一阶段的训练结束了&#13;
&#13;
#### 第二次训练&#13;
&#13;
在数据集作者不断的努力下, nailong 数据集有了一些完善, 主要的完善点在于: &#13;
1. 新添更多 nailong&#13;
2. 不是二分类了, 新增了表情包分类, 动画分类等五分类, 不过不同类别的数据数量差异很大(两个数量级)&#13;
3. 加入了一些 corner case, 比如 藤田琴音等其他颜色为黄色的图像&#13;
&#13;
因为第一次训练代码已经写好了, 改起来也不是很麻烦, 只需要换个数据集定义与读取. 作者的数据集放在 huggingface 上, 于是我们使用 datasets 进行读取.&#13;
&#13;
&gt; 我也不知道是不是我写的问题, datasets读起来很慢, dataloader 后, 会把 label 自动变成torch.tensor格式, 但是 n, c, h, w 格式的图片只会把 w 维度变成 torch.tensor 格式, 其他维度还是 List, 需要在 dataset 类定义的时候使用 __getitem__() 将数据提前变为 torch.tensor&#13;
&gt; 然后不支持多线程读取(会卡住), 单线程读取读起来很慢, gpu 的 cuda 呈现尖刺状&#13;
&gt; 然后, dataset 的读取**要先**读取 id 再读取 x 跟 label&#13;
&gt; 没怎么用过 dataset, 这次属实是学到了&#13;
&#13;
修改好后数据加载的代码后并注释掉先前的数据增广代码后(后续研究), 第二次训练开始了&#13;
&#13;
这次结果好过头了&#13;
模型的 loss 收敛到了 $1e^{-5}$, acc跟f1更是到达了 $100\%$&#13;
&#13;
使用测试代码简单测试, 发现在数据集的数据都能完美分类, 不在数据集的分类只要分不出是奶龙即可. 检查模型输出权重, 也没啥问题, 看起来是完美了?&#13;
&#13;
然而 这张图还是给了模型一拳&#13;
&#13;
&lt;details&gt;&lt;summary&gt;图&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
![22](https://github.com/user-attachments/assets/3cb2b111-e01f-44dd-8e71-0509ab2bb6c0)&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&#13;
他会识别成 nailong, 不过我觉得问题不大(确实有人把他抽象的认成 nailong)&#13;
&#13;
### 部署&#13;
&#13;
上面的 `test.py` 中 写了onnx导出的代码, 支持任意 batch 的输入(解锁了 n, c, h, w 的 n 维度)&#13;
&#13;
简单编写onnx推理代码&#13;
&#13;
&lt;details&gt;&lt;summary&gt;onnx_inference.py&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
# from torchvision import transforms&#13;
import onnxruntime as ort&#13;
from PIL import Image&#13;
import numpy as np&#13;
&#13;
img_size = 224&#13;
&#13;
# transform = transforms.Compose([&#13;
#     transforms.Resize((img_size, img_size)),&#13;
#     transforms.ToTensor(),&#13;
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),&#13;
# ])&#13;
&#13;
def transform_img(img: Image, image_size=224, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):&#13;
    img = img.convert('RGB').resize((image_size, image_size), Image.Resampling.LANCZOS)&#13;
    img = np.array(img)&#13;
    img = (img / 255 - mean) / std&#13;
    img = img.transpose((2, 0, 1))&#13;
    img = np.expand_dims(img, axis=0)&#13;
    return img.astype(np.float32)&#13;
&#13;
&#13;
label_mapping = {&#13;
    'nailong': 0,&#13;
    'emoji': 1,&#13;
    'anime': 2,&#13;
    'others': 3,&#13;
    'long': 4&#13;
}&#13;
&#13;
reverse_label_mapping = {v: k for k, v in label_mapping.items()}&#13;
&#13;
model_path = 'model.onnx'&#13;
session = ort.InferenceSession(model_path)&#13;
&#13;
image_path = '3.jpg'&#13;
image = Image.open(image_path).convert('RGB')&#13;
# image = transform(image).unsqueeze(0).numpy()&#13;
image = transform_img(image)&#13;
&#13;
# 运行推理&#13;
input_name = session.get_inputs()[0].name&#13;
output_name = session.get_outputs()[0].name&#13;
outputs = session.run([output_name], {input_name: image})&#13;
&#13;
# 获取分类结果&#13;
output = outputs[0]&#13;
predicted_class = np.argmax(output, axis=1)&#13;
predicted_label = reverse_label_mapping[predicted_class[0]]&#13;
&#13;
print(f'Predicted class: {predicted_label}')&#13;
&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&gt; 训练的时候引入了 torchvision 的 transforms, 这里为了减少依赖, 选择手动实现, 有需要也可以自行取消注释并修改&#13;
。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/nailong-shu-ju-ji-%2C%20-jian-ce-nailong-de-mo-xing-%2C%20-xun-lian-yu-tui-li-%20%28-yi-%29.html</guid><pubDate>Tue, 26 Nov 2024 12:53:02 +0000</pubDate></item><item><title>使用 k3s 搭建 k8s 集群(使用国内镜像)</title><link>https://FairyOwO.github.io/post/shi-yong-%20k3s%20-da-jian-%20k8s%20-ji-qun-%28-shi-yong-guo-nei-jing-xiang-%29.html</link><description>&gt; 历史文章搬运&#13;
&#13;
&gt; 注: 此处k3s为具体的 Kubernetes 发行版, 后面的k8s为 Kubernetes 的缩写, Kubernetes 是开源容器编排平台&#13;
&#13;
需求之初是想对年抛机, 月抛机进行统一的管理, 方便部署相关镜像, 类似于史莱姆的结构(拿到新的机器, 加入集群, 机器时间过期, 自动离线, 伸缩重启分配全由集群本身管理)&#13;
&#13;
使用系统为 Debian&#13;
&#13;
## 服务器搭建&#13;
&#13;
### 搭建集群&#13;
&#13;
主 server sh脚本&#13;
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```sh&#13;
echo 'deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware&#13;
deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware&#13;
&#13;
deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware&#13;
deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware&#13;
&#13;
deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware&#13;
deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware&#13;
&#13;
# 以下安全更新软件源包含了官方源与镜像站配置，如有需要可自行修改注释切换&#13;
deb https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware&#13;
deb-src https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware' &gt; /etc/apt/sources.list&#13;
&#13;
apt update&#13;
&#13;
curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \&#13;
  INSTALL_K3S_MIRROR=cn \&#13;
  sh -s - server \&#13;
  --cluster-init \&#13;
  --system-default-registry=registry.cn-hangzhou.aliyuncs.com&#13;
&#13;
cat /var/lib/rancher/k3s/server/token&#13;
&#13;
cat &gt;&gt; /etc/rancher/k3s/registries.yaml &lt;&lt; EOF&#13;
mirrors:&#13;
  docker.io:&#13;
    endpoint:&#13;
      - 'https://dockerproxy.net'&#13;
      - 'https://registry.cn-hangzhou.aliyuncs.com/'&#13;
      - 'https://mirror.ccs.tencentyun.com'&#13;
  k8s.gcr.io:&#13;
    endpoint:&#13;
      - 'https://k8s.dockerproxy.net'&#13;
      - 'https://registry.aliyuncs.com/google_containers'&#13;
  ghcr.io:&#13;
    endpoint:&#13;
      - 'https://ghcr.dockerproxy.net'&#13;
      - 'https://ghcr.m.daocloud.io/'&#13;
  gcr.io:&#13;
    endpoint:&#13;
      - 'https://gcr.dockerproxy.net'&#13;
      - 'https://gcr.m.daocloud.io/'&#13;
  quay.io:&#13;
    endpoint:&#13;
      - 'https://quay.dockerproxy.net'&#13;
      - 'https://quay.tencentcloudcr.com/'&#13;
  registry.k8s.io:&#13;
    endpoint:&#13;
      - 'https://k8s.dockerproxy.net'&#13;
      - 'https://registry.aliyuncs.com/v2/google_containers'&#13;
EOF&#13;
systemctl restart k3s&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
副 server sh脚本&#13;
&#13;
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```sh&#13;
echo 'deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware&#13;
deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware&#13;
&#13;
deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware&#13;
deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware&#13;
&#13;
deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware&#13;
deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware&#13;
&#13;
# 以下安全更新软件源包含了官方源与镜像站配置，如有需要可自行修改注释切换&#13;
deb https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware&#13;
deb-src https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware' &gt; /etc/apt/sources.list&#13;
&#13;
apt update&#13;
&#13;
curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \&#13;
  INSTALL_K3S_MIRROR=cn \&#13;
  sh -s - server \&#13;
  --cluster-init \&#13;
  --system-default-registry=registry.cn-hangzhou.aliyuncs.com&#13;
&#13;
cat /var/lib/rancher/k3s/server/token&#13;
&#13;
cat &gt;&gt; /etc/rancher/k3s/registries.yaml &lt;&lt; EOF&#13;
mirrors:&#13;
  docker.io:&#13;
    endpoint:&#13;
      - 'https://dockerproxy.net'&#13;
      - 'https://registry.cn-hangzhou.aliyuncs.com/'&#13;
      - 'https://mirror.ccs.tencentyun.com'&#13;
  k8s.gcr.io:&#13;
    endpoint:&#13;
      - 'https://k8s.dockerproxy.net'&#13;
      - 'https://registry.aliyuncs.com/google_containers'&#13;
  ghcr.io:&#13;
    endpoint:&#13;
      - 'https://ghcr.dockerproxy.net'&#13;
      - 'https://ghcr.m.daocloud.io/'&#13;
  gcr.io:&#13;
    endpoint:&#13;
      - 'https://gcr.dockerproxy.net'&#13;
      - 'https://gcr.m.daocloud.io/'&#13;
  quay.io:&#13;
    endpoint:&#13;
      - 'https://quay.dockerproxy.net'&#13;
      - 'https://quay.tencentcloudcr.com/'&#13;
  registry.k8s.io:&#13;
    endpoint:&#13;
      - 'https://k8s.dockerproxy.net'&#13;
      - 'https://registry.aliyuncs.com/v2/google_containers'&#13;
EOF&#13;
systemctl restart k3s&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
client sh脚本&#13;
&#13;
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```sh&#13;
echo 'deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware&#13;
deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware&#13;
&#13;
deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware&#13;
deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware&#13;
&#13;
deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware&#13;
deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware&#13;
&#13;
# 以下安全更新软件源包含了官方源与镜像站配置，如有需要可自行修改注释切换&#13;
deb https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware&#13;
deb-src https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware' &gt; /etc/apt/sources.list&#13;
&#13;
apt update&#13;
&#13;
curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \&#13;
  INSTALL_K3S_MIRROR=cn \&#13;
  K3S_URL=https://ip:6443 \&#13;
  K3S_TOKEN=your_token \&#13;
  sh -&#13;
&#13;
mkdir -p /etc/rancher/k3s&#13;
cat &gt;&gt; /etc/rancher/k3s/registries.yaml &lt;&lt; EOF&#13;
mirrors:&#13;
  docker.io:&#13;
    endpoint:&#13;
      - 'https://dockerproxy.net'&#13;
      - 'https://registry.cn-hangzhou.aliyuncs.com/'&#13;
      - 'https://mirror.ccs.tencentyun.com'&#13;
  k8s.gcr.io:&#13;
    endpoint:&#13;
      - 'https://k8s.dockerproxy.net'&#13;
      - 'https://registry.aliyuncs.com/google_containers'&#13;
  ghcr.io:&#13;
    endpoint:&#13;
      - 'https://ghcr.dockerproxy.net'&#13;
      - 'https://ghcr.m.daocloud.io/'&#13;
  gcr.io:&#13;
    endpoint:&#13;
      - 'https://gcr.dockerproxy.net'&#13;
      - 'https://gcr.m.daocloud.io/'&#13;
  quay.io:&#13;
    endpoint:&#13;
      - 'https://quay.dockerproxy.net'&#13;
      - 'https://quay.tencentcloudcr.com/'&#13;
  registry.k8s.io:&#13;
    endpoint:&#13;
      - 'https://k8s.dockerproxy.net'&#13;
      - 'https://registry.aliyuncs.com/v2/google_containers'&#13;
EOF&#13;
systemctl restart k3s-agent&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&gt; 注: k3s 搭建集群的方案需要保证主服务器不离线, 否则整个集群会离线, 考虑到k3s占用低, 机器一般是性能不高的类型, 我也有长期续费的服务器, 故使用这个方案&#13;
&#13;
在主server服务器使用&#13;
&#13;
```sh&#13;
kubectl get nodes -A&#13;
```&#13;
出现每台机子的信息, 代表集群内部网络通信没问题&#13;
&#13;
在主server服务器使用&#13;
```sh&#13;
kubectl get pods --all-namespaces&#13;
```&#13;
在所有服务在 `RUNNING` 状态时, 为安装成功 (这些服务都是内部通信与均衡负载的镜像), 如果是卡在 `container creating`, 则安装失败, 原因是镜像没正确配置&#13;
&#13;
### 安装helm (虽然不知道干什么用, 集群内也自带一个helm)&#13;
&#13;
1. 手动安装&#13;
    1. 下载需要的版本 [下载地址](https://github.com/helm/helm/releases)&#13;
    2. 解压, 上传到服务器, chmod给执行权限&#13;
    3. 移动到环境变量的目录中&#13;
        ```sh&#13;
        mv helm /usr/local/bin/helm&#13;
        ```&#13;
2. 使用脚本安装&#13;
    ```sh&#13;
    https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash&#13;
    ```&#13;
&#13;
## 面板安装&#13;
&#13;
为了简单, 面板选择的是 kubepi&#13;
&#13;
[文档](https://github.com/1Panel-dev/KubePi/wiki/2%E3%80%81%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2#kubernetes)&#13;
&#13;
这里选择的是非持久化部署, 在直接部署在刚刚建好的集群之中&#13;
&#13;
&gt; 持久化部署会有莫名其妙的分配问题, 应该是跟分配本地空间有关系, 我也不需要持久化集群信息(因为只有一个集群), 所以没什么关系&#13;
&#13;
```sh&#13;
# 安装&#13;
sudo kubectl apply -f https://raw.githubusercontent.com/1Panel-dev/KubePi/master/docs/deploy/kubectl/kubepi.yaml&#13;
```&#13;
&#13;
安装完成后, 根据安装教程, 获取访问地址&#13;
&#13;
```sh&#13;
# 获取 NodeIp&#13;
export NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[0].address}')&#13;
# 获取 NodePort&#13;
export NODE_PORT=$(kubectl -n kube-system get services kubepi -o jsonpath='{.spec.ports[0].nodePort}')&#13;
# 获取 Address&#13;
echo http://$NODE_IP:$NODE_PORT&#13;
```&#13;
&#13;
&gt; 注: 内网组机子的时候这里会是内网地址, 需要使用端口转发转发到 `0.0.0.0` 之后才能外网访问&#13;
&gt; ```sh&#13;
&gt; kubectl port-forward --address 0.0.0.0 kubepi-d8477f9d8-drthz -n kube-system 2999:80&#13;
&gt; ```&#13;
&gt; 此命令不会中断, 会持续运行, 需要把这条命令中的 `kubepi-d8477f9d8-drthz` 换成实际名字&#13;
&#13;
登陆系统&#13;
&#13;
```text&#13;
地址: http://$NODE_IP:$NODE_PORT&#13;
用户名: admin&#13;
密码: kubepi&#13;
```&#13;
&#13;
登陆后记得修改密码&#13;
&#13;
导入集群&#13;
&#13;
在主服务器, 获取&#13;
&#13;
```sh&#13;
cd /etc/rancher/k3s&#13;
cat k3s.yaml&#13;
```&#13;
在 kubepi 导入集群, 认证模式选择 kubeconfig文件, 把这个文件复制进去&#13;
&#13;
在集群配置中, 配置一下网络, 使之可以直接通过外网端口访问&#13;
&#13;
具体配置流程忘了, 此方法由同事指点&#13;
&#13;
## 部署项目&#13;
&#13;
在 kubepi 中 选择集群, 应用市场, chart 仓库, 填入相关信息, 这里我使用的是:&#13;
```text&#13;
开源社: http://mirror.kaiyuanshe.cn/kubernetes/charts/&#13;
开源应用市场: https://charts.grapps.cn&#13;
```&#13;
&#13;
点开应用就有很多项目跳出来可以部署。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/shi-yong-%20k3s%20-da-jian-%20k8s%20-ji-qun-%28-shi-yong-guo-nei-jing-xiang-%29.html</guid><pubDate>Fri, 22 Nov 2024 09:34:51 +0000</pubDate></item><item><title>待阅读</title><link>https://FairyOwO.github.io/post/dai-yue-du.html</link><description>## 对强化学习进行系统性的学习&#13;
&gt; 此前因为对一些强化学习项目感兴趣, 而草草学习了一部分(感谢 ai, 解释了大部分内容), 这里准备系统性学习一份&#13;
1. [Reinforcement Learning: An Introduction](https://rl.qiwihui.com/zh-cn/latest/) 强化学习导论&#13;
2. [Openai Spinning Up](https://spinningup.qiwihui.com/zh-cn/latest/) OpenAI 深度强化学习&#13;
&#13;
## 知乎 大模型相关&#13;
&#13;
## 苏剑林博客&#13;
&#13;
## 图像生成, llm对prompt优化&#13;
&gt; demo需要&#13;
&#13;
## 图像生成模型 2024年 训练方法。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/dai-yue-du.html</guid><pubDate>Fri, 22 Nov 2024 08:44:27 +0000</pubDate></item><item><title>Test</title><link>https://FairyOwO.github.io/post/Test.html</link><description>这是一个测试!&#13;
This is a test!。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/Test.html</guid><pubDate>Fri, 22 Nov 2024 03:26:34 +0000</pubDate></item></channel></rss>