<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>FairyOwO 的 Blog</title><link>https://FairyOwO.github.io</link><description>记录一些琐事</description><copyright>FairyOwO 的 Blog</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://github.githubassets.com/favicons/favicon.svg</url><title>avatar</title><link>https://FairyOwO.github.io</link></image><lastBuildDate>Wed, 12 Feb 2025 01:22:32 +0000</lastBuildDate><managingEditor>FairyOwO 的 Blog</managingEditor><ttl>60</ttl><webMaster>FairyOwO 的 Blog</webMaster><item><title>基于 deepseek 的 r1 复现, 对传统强化学习的思考 (挖坑)</title><link>https://FairyOwO.github.io/post/ji-yu-%20deepseek%20-de-%20r1%20-fu-xian-%2C%20-dui-chuan-tong-qiang-hua-xue-xi-de-si-kao-%20%28-wa-keng-%29.html</link><description>&gt; 本文存在大量口齿不清

deepseek-r1持续火热, 已有大量的复现训练过程

1. [open-r1](https://github.com/huggingface/open-r1)
2. [mini-deepseek-r1](https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/mini-deepseek-r1-aha-grpo.ipynb)
3. [open-r1-multimodal](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal)
4. [open-thoughts](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal)
5. [TinyZero](https://github.com/Jiayi-Pan/TinyZero)
6. [simpleRL-reason](https://github.com/hkust-nlp/simpleRL-reason)
7. [RAGEN](https://github.com/ZihanWang314/RAGEN)
8. [unsloth r1-reasoning](https://unsloth.ai/blog/r1-reasoning)
9. [oat-zero](https://github.com/sail-sg/oat-zero)
10. [Logic-RL](https://github.com/Unakar/Logic-RL)

具体效果可以看具体的仓库与其中的论文或者博客, 这里主要想法是, 是否可以使用这一训练范式, 来为传统强化学习任务, 带来可对人带来参考的思维过程

例如, 希望给围棋使用此范式, 一个可能的 pipeline 是:
1. sft, 为模型带入围棋知识
    &gt; 我认为是必要的, 因为通用型大模型此类知识较少, 如果没有的话, 搜索解空间太大, 不容易搜索到正确的 token, 可以使用其他强力模型或者程序进行辅助构建
2. 设定一个通用格式, 继续sft, 让模型输出正确格式(非必要, 在上述复现中, 无需特意sft也可以让模型输出正确格式)
3. rule based RL
    &gt; 考虑到围棋中间的reward非常难量化, 可以使用专业围棋模型进行反馈
    &gt; 让大模型进行一定的思考, 输出一个答案, 之后与围棋模型进行比对, 前几选 reward +1, 其他 reward -1
    &gt; 如果是简单的任务, 则可以通过env反馈奖励(与r1相同), 参考 [RAGEN](https://github.com/ZihanWang314/RAGEN)


。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/ji-yu-%20deepseek%20-de-%20r1%20-fu-xian-%2C%20-dui-chuan-tong-qiang-hua-xue-xi-de-si-kao-%20%28-wa-keng-%29.html</guid><pubDate>Tue, 11 Feb 2025 07:47:26 +0000</pubDate></item><item><title>幻兽帕鲁繁殖树搜索, 用于搜索从A帕鲁到B帕鲁的所有最短路径</title><link>https://FairyOwO.github.io/post/huan-shou-pa-lu-fan-zhi-shu-sou-suo-%2C%20-yong-yu-sou-suo-cong-A-pa-lu-dao-B-pa-lu-de-suo-you-zui-duan-lu-jing.html</link><description>## 相关原理

详见[这里](https://paldb.cc/cn/Breeding_Farm#BreedingFarm)

简单来说, 子帕鲁是两个父帕鲁繁殖力与1的和整除2, 特殊配方需要按照专有配方才可以繁殖

剪枝的 bfs 即可完成

## 准备数据

你需要从[帕鲁编年史](https://paldb.cc)中, 自行整理 `a.tsv` (所有帕鲁的CombiRank 繁殖力 IndexOrder 索引), `b.tsv` (特殊繁殖配方)
&gt; 此索引与帕鲁id不同
&lt;details&gt;&lt;summary&gt;两个表的格式示例&lt;/summary&gt;
&lt;p&gt;

## a.tsv

[帕鲁编年史 Breed Combi](https://paldb.cc/cn/Breeding_Farm)

```tsv
name	CombiRank	IndexOrder
皮皮鸡	1500	66
壶小象	1490	17
喵丝特	1480	7
```

&gt; 注意中间是 `\t` 制表符

## b.tsv

[帕鲁编年史 Breed Unique](https://paldb.cc/cn/Breeding_Farm#BreedUnique)
[帕鲁编年史 Breed Self](https://paldb.cc/cn/Breeding_Farm#BreedSelf)


```tsv
parent	孵化完成
佩克龙  伏特喵	派克龙
炎魔羊  噬魂兽	暗魔羊
喵丝特  企丸丸	冰丝特
```
&gt; 注意中间是 `\t` 制表符, parent中间是两个空格

&lt;/p&gt;
&lt;/details&gt; 

&gt; 注意语言

## 代码

&lt;details&gt;&lt;summary&gt;未整理代码&lt;/summary&gt;
&lt;p&gt;

```python
from collections import deque
import csv

def load_data():
    data = {}
    with open(r'a.tsv', 'r', newline='', encoding='utf-8') as f:
        reader = csv.reader(f, delimiter='\t')
        next(reader)
        for row in reader:
            data[row[0]] = {'fertility': int(row[1]), 'index_order': int(row[2])}

    data2 = {}
    exclusive = set()

    with open(r'b.tsv', 'r', newline='', encoding='utf-8') as f:
        reader = csv.reader(f, delimiter='\t')
        next(reader)
        for row in reader:
            parents = tuple(sorted(row[0].split('  ')))
            child = row[1]
            data2[parents] = child
            exclusive.add(child)

    return data, data2, exclusive

pal_data, special_breeding, exclusive_pals = load_data()

def breed(parent1, parent2):
    key = tuple(sorted([parent1, parent2]))
    if key in special_breeding:
        return special_breeding[key]
    
    # 塔主处理
    p1_fertility = pal_data[parent1]['fertility']
    p2_fertility = pal_data[parent2]['fertility']
    if p1_fertility == 9999 or p2_fertility == 9999:
        return '皮皮鸡'
    
    value = (p1_fertility + p2_fertility + 1) // 2
    
    valid_pals = [
        (name, info) for name, info in pal_data.items() 
        if name not in exclusive_pals
    ]
    
    closest_pal = min(
        valid_pals,
        key=lambda x: (abs(x[1]['fertility'] - value), x[1]['index_order'])
    )
    return closest_pal[0]

def find_breeding_path(start, end):
    if start not in pal_data or end not in pal_data:
        return []
    
    target_fertility = pal_data[end]['fertility']
    visited = {}
    queue = deque([(start, [start])])
    results = []
    min_steps = None

    while queue:
        level_size = len(queue)
        found = False
        
        for _ in range(level_size):
            current_pal, path = queue.popleft()
            
            if min_steps and len(path) &gt; min_steps:
                continue
                
            if current_pal == end:
                if not min_steps:
                    min_steps = len(path)
                if len(path) == min_steps:
                    results.append(path)
                found = True
                continue
                
            # 获取当前帕鲁繁殖力
            current_fertility = pal_data[current_pal]['fertility']
            
            for mate in pal_data:
                if mate == current_pal:
                    continue
                
                key = tuple(sorted([current_pal, mate]))
                
                # 特殊繁殖直接处理
                if key in special_breeding:
                    child = special_breeding[key]
                    new_path = path + [mate, child]
                    if child not in visited or len(new_path) &lt; visited.get(child, float('inf')):
                        visited[child] = len(new_path)
                        queue.append((child, new_path))
                    continue
                
                mate_fertility = pal_data[mate]['fertility']
                
                # 塔主特殊处理
                if current_fertility == 9999 or mate_fertility == 9999:
                    child = '皮皮鸡'
                else:
                    if current_fertility &lt; target_fertility and mate_fertility &lt; current_fertility:
                        continue  # 配偶生育力更低，子代只会更小
                    if current_fertility &gt; target_fertility and mate_fertility &gt; current_fertility:
                        continue  # 配偶生育力更高，子代只会更大
                    
                    child = breed(current_pal, mate)
                
                new_path = path + [mate, child]
                
                # 如果子代生育力偏离目标方向
                child_fertility = pal_data[child]['fertility']
                if (target_fertility &gt; current_fertility and child_fertility &lt; current_fertility) or \
                   (target_fertility &lt; current_fertility and child_fertility &gt; current_fertility):
                    continue
                
                if child not in visited or len(new_path) &lt; visited[child]:
                    visited[child] = len(new_path)
                    queue.append((child, new_path))
                elif len(new_path) == visited[child]:
                    queue.append((child, new_path))
        
        if found:
            break

    final_results = [p for p in results if len(p) == min_steps]
    return final_results if final_results else []

start_pal = '混沌骑士'
end_pal = '八云犬'
paths = find_breeding_path(start_pal, end_pal)

if paths:
    print(f'找到{len(paths)}条最短繁殖路径（步数：{(len(paths[0])-1)//2}）:')
    for i, path in enumerate(paths, 1):
        print(f'\n路径{i}:')
        for j in range(0, len(path)-1, 2):
            print(f'{path[j]} + {path[j+1]} → {path[j+2]}')
else:
    print('无可行繁殖路径')

```

&lt;/p&gt;
&lt;/details&gt; 

&gt; 不知道为什么, 相对于帕鲁编年史的实现, 我的实现慢一点, 估计是他预计算好了。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/huan-shou-pa-lu-fan-zhi-shu-sou-suo-%2C%20-yong-yu-sou-suo-cong-A-pa-lu-dao-B-pa-lu-de-suo-you-zui-duan-lu-jing.html</guid><pubDate>Wed, 05 Feb 2025 03:03:11 +0000</pubDate></item><item><title>2024 Advent of Code 复盘与答案 (二)</title><link>https://FairyOwO.github.io/post/2024%20Advent%20of%20Code%20-fu-pan-yu-da-an-%20%28-er-%29.html</link><description>[Advent of Code](https://adventofcode.com/)&#13;
&#13;
TODO, 每天更新一题一周前的题目&#13;
&#13;
使用 python 编写, 没有整理代码, 所以非常乱(变量乱取名, 没有注释, 逻辑奇怪, 并非最佳实现)&#13;
可以使用 gpt 相关工具辅助查看&#13;
&#13;
&gt; 如果没有特意说明, 变量 a 统一存放所有原始字符串&#13;
&#13;
## 第九题&#13;
&#13;
[https://adventofcode.com/2024/day/9](https://adventofcode.com/2024/day/9)&#13;
&#13;
### 1题&#13;
&#13;
一个磁盘中的文件表示方法, 每两位数字各表示, x块文件, x块空的区域&#13;
例如&#13;
`12345`&#13;
表示 一块文件 两块空文件 三块文件 四块空文件 五块文件&#13;
每个文件从前往后的id为他们在磁盘中的顺序, 例如上述可以表示为&#13;
`0..111....22222`&#13;
&#13;
希望从后往前的将文件塞入从前往后中空的地方&#13;
&#13;
例如&#13;
```text&#13;
0..111....22222&#13;
02.111....2222.&#13;
022111....222..&#13;
0221112...22...&#13;
02211122..2....&#13;
022111222......&#13;
```&#13;
&#13;
最后输出每个块位置*id号之和&#13;
&#13;
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
# 构建&#13;
t = []&#13;
flag = True&#13;
file_id = 0&#13;
for i in list(a):&#13;
    if flag:&#13;
        for _ in range(int(i)):&#13;
            t.append(file_id)&#13;
        flag = False&#13;
        file_id += 1&#13;
    else:&#13;
        for _ in range(int(i)):&#13;
            t.append('.')&#13;
        flag = True&#13;
&#13;
i = 0&#13;
j = len(t) - 1&#13;
&#13;
while i &lt; j:&#13;
    if t[i] == '.':&#13;
        if t[j] != '.':&#13;
            t[i], t[j] = t[j], t[i]&#13;
        else:&#13;
            i -= 1&#13;
        j -= 1&#13;
    i += 1&#13;
&#13;
ans = 0&#13;
for i, j in enumerate(t):&#13;
    if j != '.':&#13;
        ans += i * j&#13;
print(ans)&#13;
&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
### 2题&#13;
&#13;
在一题的基础上, 修改从后往前放入从前往后空区域的方法, 从单个块移动转换为整个文件移动, 例如&#13;
```text&#13;
00...111...2...333.44.5555.6666.777.888899&#13;
0099.111...2...333.44.5555.6666.777.8888..&#13;
0099.1117772...333.44.5555.6666.....8888..&#13;
0099.111777244.333....5555.6666.....8888..&#13;
00992111777.44.333....5555.6666.....8888..&#13;
```&#13;
&#13;
最后输出每个块位置*id号之和&#13;
&#13;
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
# 构建&#13;
t = []&#13;
flag = True&#13;
file_id = 0&#13;
for i in list(a):&#13;
    if flag:&#13;
        for _ in range(int(i)):&#13;
            t.append(file_id)&#13;
        flag = False&#13;
        file_id += 1&#13;
    else:&#13;
        for _ in range(int(i)):&#13;
            t.append('.')&#13;
        flag = True&#13;
&#13;
&#13;
def get_file_size(file_id):&#13;
    return t.count(file_id)&#13;
&#13;
for i in range(file_id - 1, -1, -1):&#13;
    file_size = get_file_size(i)&#13;
    file_index = t.index(i)&#13;
    flag = False&#13;
    size = 0&#13;
    for id, j in enumerate(t):&#13;
        if j == '.':&#13;
            flag = True&#13;
        else:&#13;
            flag = False&#13;
            size = 0&#13;
        &#13;
        if flag:&#13;
            size += 1&#13;
            if size == file_size:&#13;
                # change位置&#13;
                for k in range(id, id - file_size, -1):&#13;
                    t[k], t[file_index] = t[file_index], t[k]&#13;
                    file_index += 1&#13;
&#13;
                break&#13;
        if id &gt;= file_index:&#13;
            break&#13;
&#13;
ans = 0&#13;
for i, j in enumerate(t):&#13;
    if j != '.':&#13;
        ans += i * j&#13;
print(ans)&#13;
&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
## 第十题&#13;
&#13;
[https://adventofcode.com/2024/day/10](https://adventofcode.com/2024/day/10)&#13;
&#13;
### 1题&#13;
&#13;
给定一幅由数字组成的地图, 从 0 开始, 一步一步上下左右移动到 9&#13;
求一副图中, 每一个 0 能到达的 9 有多少个, 输出他们的和&#13;
&#13;
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
aaa = []&#13;
for i in a.splitlines():&#13;
    aaa.append(list(map(int, list(i))))&#13;
&#13;
# print(aaa)&#13;
&#13;
# 获取所有 0 跟 9 的位置&#13;
pos_0 = []&#13;
pos_9 = []&#13;
for i in range(len(aaa)):&#13;
    for j in range(len(aaa[i])):&#13;
        if aaa[i][j] == 0:&#13;
            pos_0.append((i, j))&#13;
        elif aaa[i][j] == 9:&#13;
            pos_9.append((i, j))&#13;
&#13;
def get_allow_pos(pos):&#13;
    for i in [(0, 1), (1, 0), (-1, 0), (0, -1)]:&#13;
        next_step = (pos[0] + i[0], pos[1] + i[1])&#13;
        if len(aaa) &gt; next_step[0] &gt;= 0 and len(aaa[0]) &gt; next_step[1] &gt;= 0:&#13;
            if aaa[next_step[0]][next_step[1]] == aaa[pos[0]][pos[1]] + 1:&#13;
                yield (pos[0] + i[0], pos[1] + i[1])&#13;
&#13;
&#13;
def dfs(pos, pos_9):&#13;
    if pos == pos_9:&#13;
        return 1&#13;
    aa = 0&#13;
    for p in get_allow_pos(pos):&#13;
        aa = dfs(p, pos_9)&#13;
        if aa &gt; 0:&#13;
            return aa&#13;
    return aa&#13;
&#13;
ans = 0&#13;
for i in pos_0:&#13;
    for j in pos_9:&#13;
        ans += dfs(i, j)&#13;
&#13;
print(ans)&#13;
&#13;
``` &#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
### 2题&#13;
&#13;
求 0 到每一个 9 有多少种不同的走法 (注意与第一题的区别, 第一题只要求到达, 第二题需要找到所有路线)&#13;
&#13;
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
aaa = []&#13;
for i in a.splitlines():&#13;
    aaa.append(list(map(int, list(i))))&#13;
&#13;
# print(aaa)&#13;
&#13;
# 获取所有 0 跟 9 的位置&#13;
pos_0 = []&#13;
pos_9 = []&#13;
for i in range(len(aaa)):&#13;
    for j in range(len(aaa[i])):&#13;
        if aaa[i][j] == 0:&#13;
            pos_0.append((i, j))&#13;
        elif aaa[i][j] == 9:&#13;
            pos_9.append((i, j))&#13;
&#13;
def get_allow_pos(pos):&#13;
    for i in [(0, 1), (1, 0), (-1, 0), (0, -1)]:&#13;
        next_step = (pos[0] + i[0], pos[1] + i[1])&#13;
        if len(aaa) &gt; next_step[0] &gt;= 0 and len(aaa[0]) &gt; next_step[1] &gt;= 0:&#13;
            if aaa[next_step[0]][next_step[1]] == aaa[pos[0]][pos[1]] + 1:&#13;
                yield (pos[0] + i[0], pos[1] + i[1])&#13;
&#13;
&#13;
def dfs(pos, pos_9):&#13;
    if pos == pos_9:&#13;
        return 1&#13;
    aa = 0&#13;
    for p in get_allow_pos(pos):&#13;
        aa = dfs(p, pos_9)&#13;
        if aa &gt; 0:&#13;
            return aa&#13;
    return aa&#13;
&#13;
ans = 0&#13;
for i in pos_0:&#13;
    for j in pos_9:&#13;
        ans += dfs(i, j)&#13;
&#13;
print(ans)&#13;
&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&gt; 在一题的基础上, dfs固定返回1 变成 dfs 返回上一次dfs + 1&#13;
&#13;
## 第十一题&#13;
&#13;
[https://adventofcode.com/2024/day/11](https://adventofcode.com/2024/day/11)&#13;
&#13;
## 1题&#13;
&#13;
给定一串数字, 根据以下规则变换&#13;
&#13;
1. 如果数字是0, 则数字是1&#13;
2. 如果数字是偶数位, 则数字变成两个数字, 左半跟右半, 例如 1000 变成 10 跟 00, 不保留前导0, 00变成0&#13;
3. 如果没有碰到前两条规则, 则数字=数字*2024&#13;
&#13;
顺序都会被保留 (但是这题没有用到, 而且会误导第二题)&#13;
模拟上述规则25次&#13;
&#13;
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
aa = a.split(' ')&#13;
&#13;
for _ in range(25):&#13;
    i = 0&#13;
    while i &lt; len(aa):&#13;
        if aa[i] == '0':&#13;
            aa[i] = '1'&#13;
        elif len(aa[i]) % 2 == 0:&#13;
            left = aa[i][:len(aa[i]) // 2]&#13;
            left = str(int(left))&#13;
            right = aa[i][len(aa[i]) // 2:]&#13;
            right = str(int(right))&#13;
            aa.insert(i+1, right)&#13;
            aa[i] = left&#13;
            i += 1&#13;
        else:&#13;
            aa[i] = str(int(aa[i]) * 2024)&#13;
        &#13;
        i += 1&#13;
&#13;
print(len(aa))&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&gt; 模拟即可, 25次蛮少的可以直接出来&#13;
&#13;
### 2题&#13;
&#13;
在1题的基础上, 模拟75次&#13;
&#13;
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
from collections import defaultdict&#13;
from tqdm import tqdm&#13;
aa = list(map(int, a.split(' ')))&#13;
&#13;
def get_length(num):&#13;
    i = 0&#13;
    while num &gt; 0:&#13;
        num //= 10&#13;
        i += 1&#13;
    return i&#13;
&#13;
t = defaultdict(int)&#13;
&#13;
for i in aa:&#13;
    t[i] += 1&#13;
&#13;
for _ in tqdm(range(75)):&#13;
    tt = defaultdict(int)&#13;
    for i, j in t.items():&#13;
        length = get_length(i)&#13;
        if i == 0:&#13;
            tt[1] += j&#13;
        elif length % 2 == 0:&#13;
            tt[i // 10 ** (length // 2)] += j&#13;
            tt[i % 10 ** (length // 2)] += j&#13;
        else:&#13;
            tt[i * 2024] += j&#13;
        &#13;
        t = tt&#13;
&#13;
print(sum(t.values()))&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&gt; `tqdm` 是为了监控速度, 非必要引入&#13;
&gt; 与第一题不同, 这题指数爆炸, 75次会超时, 因为答案不要求顺序, 所以可以用缓存&#13;
&#13;
## 第十二题&#13;
&#13;
[https://adventofcode.com/2024/day/12](https://adventofcode.com/2024/day/12)&#13;
&#13;
### 1题&#13;
&#13;
划分区域&#13;
&#13;
```text&#13;
AAAA&#13;
BBCD&#13;
BBCC&#13;
EEEC&#13;
```&#13;
&#13;
划分成&#13;
&#13;
```text&#13;
+-+-+-+-+&#13;
|A A A A|&#13;
+-+-+-+-+     +-+&#13;
              |D|&#13;
+-+-+   +-+   +-+&#13;
|B B|   |C|&#13;
+   +   + +-+&#13;
|B B|   |C C|&#13;
+-+-+   +-+ +&#13;
          |C|&#13;
+-+-+-+   +-+&#13;
|E E E|&#13;
+-+-+-+&#13;
```&#13;
&#13;
分为五个区域, &#13;
计算每个区域的周长*面积之和&#13;
&#13;
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
&#13;
aa = []&#13;
for i in a.splitlines():&#13;
    aa.append(list(i))&#13;
&#13;
def get_perimeter(arr):&#13;
    perimeter = 0&#13;
    for i in range(len(arr)):&#13;
        for j in range(len(arr[0])):&#13;
            if arr[i][j] == 1:&#13;
                perimeter += 4&#13;
                if i &gt; 0 and arr[i - 1][j] == 1:&#13;
                    perimeter -= 1&#13;
                if j &gt; 0 and arr[i][j - 1] == 1:&#13;
                    perimeter -= 1&#13;
                if i &lt; len(arr) - 1 and arr[i + 1][j] == 1:&#13;
                    perimeter -= 1&#13;
                if j &lt; len(arr[0]) - 1 and arr[i][j + 1] == 1:&#13;
                    perimeter -= 1&#13;
    &#13;
    return perimeter&#13;
&#13;
def get_area(arr):&#13;
    area = 0&#13;
    for i in range(len(arr)):&#13;
        for j in range(len(arr[0])):&#13;
            if arr[i][j] == 1:&#13;
                area += 1&#13;
    return area&#13;
&#13;
def get_allow_pos(pos):&#13;
    allow_pos = [(1, 0), (0, 1), (-1, 0), (0, -1)]&#13;
    char = aa[pos[0]][pos[1]]&#13;
    for i in allow_pos:&#13;
        next_pos = (pos[0] + i[0], pos[1] + i[1])&#13;
        if len(aa) &gt; next_pos[0] &gt;= 0 and len(aa[0]) &gt; next_pos[1] &gt;= 0:&#13;
            if aa[next_pos[0]][next_pos[1]] == char:&#13;
                yield next_pos&#13;
&#13;
&#13;
ans = 0&#13;
&#13;
already_visited = np.zeros((len(aa), len(aa[0])))&#13;
for i in range(len(aa)):&#13;
    for j in range(len(aa[i])):&#13;
        if already_visited[i][j] == 0:&#13;
            # print(aa[i][j])&#13;
            array_ = np.zeros((len(aa), len(aa[0])))&#13;
            already_visited[i][j] = 1&#13;
            array_[i][j] = 1&#13;
            def dfs(pos):&#13;
                for next_pos in get_allow_pos(pos):&#13;
                    if already_visited[next_pos[0]][next_pos[1]] == 0:&#13;
                        already_visited[next_pos[0]][next_pos[1]] = 1&#13;
                        array_[next_pos[0]][next_pos[1]] = 1&#13;
                        dfs(next_pos)&#13;
            &#13;
            dfs((i, j))&#13;
            area = get_area(array_)&#13;
            perimeter = get_perimeter(array_)&#13;
            ans += area * perimeter&#13;
        &#13;
print(ans)&#13;
&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&gt; 加一个正方形边长+4, 如果旁边每有一个正方形就-1的边长&#13;
&#13;
### 2题&#13;
&#13;
1题的边长变成边的数量&#13;
&#13;
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
&#13;
aa = []&#13;
for i in a.splitlines():&#13;
    aa.append(list(i))&#13;
&#13;
&#13;
&#13;
def get_area(arr):&#13;
    area = 0&#13;
    for i in range(len(arr)):&#13;
        for j in range(len(arr[0])):&#13;
            if arr[i][j] == 1:&#13;
                area += 1&#13;
    return area&#13;
&#13;
def get_perimeter(region):&#13;
        &#13;
    max_x = len(region) - 1&#13;
    max_y = len(region[0]) - 1&#13;
    min_x = min_y = 0&#13;
    &#13;
    def state(x, y):&#13;
        if x &lt; 0 or x &gt; max_x or y &lt; 0 or y &gt; max_y:&#13;
            return False&#13;
        return region[x][y]&#13;
    &#13;
    perimeter = 0&#13;
    &#13;
    # 垂直方向扫描&#13;
    for i in range(max_x + 1):&#13;
        st = state(i, -1)&#13;
        for j in range(max_y + 2):&#13;
            if st != state(i, j):&#13;
                if st != state(i-1, j-1) or st == state(i-1, j):&#13;
                    perimeter += 1&#13;
                if st != state(i+1, j-1) or st == state(i+1, j):&#13;
                    perimeter += 1&#13;
                st = not st&#13;
    &#13;
    # 水平方向扫描&#13;
    for j in range(max_y + 1):&#13;
        st = state(-1, j)&#13;
        for i in range(max_x + 2):&#13;
            if st != state(i, j):&#13;
                if st != state(i-1, j-1) or st == state(i, j-1):&#13;
                    perimeter += 1&#13;
                if st != state(i-1, j+1) or st == state(i, j+1):&#13;
                    perimeter += 1&#13;
                st = not st&#13;
    &#13;
    return perimeter // 2&#13;
&#13;
def get_allow_pos(pos):&#13;
    allow_pos = [(1, 0), (0, 1), (-1, 0), (0, -1)]&#13;
    char = aa[pos[0]][pos[1]]&#13;
    for i in allow_pos:&#13;
        next_pos = (pos[0] + i[0], pos[1] + i[1])&#13;
        if len(aa) &gt; next_pos[0] &gt;= 0 and len(aa[0]) &gt; next_pos[1] &gt;= 0:&#13;
            if aa[next_pos[0]][next_pos[1]] == char:&#13;
                yield next_pos&#13;
&#13;
ans = 0&#13;
&#13;
already_visited = np.zeros((len(aa), len(aa[0])))&#13;
for i in range(len(aa)):&#13;
    for j in range(len(aa[i])):&#13;
        if already_visited[i][j] == 0:&#13;
            # print(aa[i][j])&#13;
            array_ = np.zeros((len(aa), len(aa[0])))&#13;
            already_visited[i][j] = 1&#13;
            array_[i][j] = 1&#13;
            def dfs(pos):&#13;
                for next_pos in get_allow_pos(pos):&#13;
                    if already_visited[next_pos[0]][next_pos[1]] == 0:&#13;
                        already_visited[next_pos[0]][next_pos[1]] = 1&#13;
                        array_[next_pos[0]][next_pos[1]] = 1&#13;
                        &#13;
                        dfs(next_pos)&#13;
            &#13;
            dfs((i, j))&#13;
            area = get_area(array_)&#13;
            perimeter = get_perimeter(array_)&#13;
            # print(perimeter)&#13;
            ans += area * perimeter&#13;
        &#13;
print(ans)&#13;
&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&gt; 一条边有两个角, 找到所有角之后整除2即可&#13;
&#13;
## 第十三题&#13;
&#13;
[https://adventofcode.com/2024/day/13](https://adventofcode.com/2024/day/13)&#13;
&#13;
### 1题&#13;
&#13;
按下按钮A或B, 移动老虎爪, 让老虎爪移动到指定的位置, 按下A需要三块钱, B需要一块钱&#13;
有可能无解(无法移动到指定位置)&#13;
&#13;
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
&#13;
aa = a.split('\n\n')&#13;
from sympy import *&#13;
&#13;
ans = 0&#13;
for i in aa:&#13;
    inputs = i.split('\n')&#13;
    # 提取输入&#13;
    ax, ay = inputs[0].split(': ')[1].split(', ')&#13;
    bx, by = inputs[1].split(': ')[1].split(', ')&#13;
    t1, t2 = inputs[2].split(': ')[1].split(', ')&#13;
    ax, ay = int(ax[2:]), int(ay[2:])&#13;
    bx, by = int(bx[2:]), int(by[2:])&#13;
    t1, t2 = int(t1[2:]), int(t2[2:])&#13;
    m = Symbol('m')&#13;
    n = Symbol('n')&#13;
    temp = solve([m * ax + n * bx - t1, m * ay + n * by - t2], [m, n])&#13;
    # print(temp)&#13;
    if temp[m].is_Integer and temp[n].is_Integer:&#13;
        ans += int(temp[m]) * 3 + int(temp[n])&#13;
&#13;
print(ans)&#13;
&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&gt; 二元一次方程组的整数解&#13;
&#13;
### 2题&#13;
&#13;
在一题的基础上, X轴和Y轴上都高出10000000000000(大数)&#13;
&#13;
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
&#13;
aa = a.split('\n\n')&#13;
from sympy import *&#13;
&#13;
ans = 0&#13;
for i in aa:&#13;
    inputs = i.split('\n')&#13;
    # 提取输入&#13;
    ax, ay = inputs[0].split(': ')[1].split(', ')&#13;
    bx, by = inputs[1].split(': ')[1].split(', ')&#13;
    t1, t2 = inputs[2].split(': ')[1].split(', ')&#13;
    ax, ay = int(ax[2:]), int(ay[2:])&#13;
    bx, by = int(bx[2:]), int(by[2:])&#13;
    t1, t2 = int(t1[2:]) + 10000000000000, int(t2[2:]) + 10000000000000&#13;
    m = Symbol('m')&#13;
    n = Symbol('n')&#13;
    temp = solve([m * ax + n * bx - t1, m * ay + n * by - t2], [m, n])&#13;
    # print(temp)&#13;
    if temp[m].is_Integer and temp[n].is_Integer:&#13;
        ans += int(temp[m]) * 3 + int(temp[n])&#13;
&#13;
print(ans)&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&gt; python无限精度整数, 不需要额外处理&#13;
&#13;
## 第十四题&#13;
&#13;
[https://adventofcode.com/2024/day/14](https://adventofcode.com/2024/day/14)&#13;
&#13;
## 1题&#13;
&#13;
给定点坐标, 点的移动速度, 大小固定的图&#13;
&#13;
求 四个象限内点的数量 之积&#13;
象限是去掉最中间一列与一行使得图分成四个区域&#13;
点到边界会传送到另一侧 (mod)&#13;
&#13;
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
&#13;
def main():&#13;
    lines = a.splitlines()&#13;
&#13;
    robots = []&#13;
    for line in lines:&#13;
        p_part, v_part = line.strip().split()&#13;
        p_x, p_y = map(int, p_part[2:].split(','))&#13;
        v_x, v_y = map(int, v_part[2:].split(','))&#13;
        robots.append({'p': (p_x, p_y), 'v': (v_x, v_y)})&#13;
&#13;
    positions = {}&#13;
    for robot in robots:&#13;
        x = (robot['p'][0] + robot['v'][0] * 100) % 101&#13;
        y = (robot['p'][1] + robot['v'][1] * 100) % 103&#13;
        positions[(x, y)] = positions.get((x, y), 0) + 1&#13;
&#13;
    q1 = q2 = q3 = q4 = 0&#13;
    for (x, y), count in positions.items():&#13;
        if x &lt; 50 and y &lt; 51:&#13;
            q1 += count&#13;
        elif x &gt; 50 and y &lt; 51:&#13;
            q2 += count&#13;
        elif x &gt; 50 and y &gt; 51:&#13;
            q3 += count&#13;
        elif x &lt; 50 and y &gt; 51:&#13;
            q4 += count&#13;
&#13;
    safety_factor = q1 * q2 * q3 * q4&#13;
    print(safety_factor)&#13;
&#13;
if __name__ == '__main__':&#13;
    main()&#13;
&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
### 2题&#13;
&#13;
直接贴原题, 因为原题是阅读理解&#13;
&#13;
During the bathroom break, someone notices that these robots seem awfully similar to ones built and used at the North Pole. If they're the same type of robots, they should have a hard-coded Easter egg: very rarely, most of the robots should arrange themselves into a picture of a Christmas tree.&#13;
&gt; 在上厕所的时候，有人注意到这些机器人看起来与在北极建造和使用的机器人非常相似。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/2024%20Advent%20of%20Code%20-fu-pan-yu-da-an-%20%28-er-%29.html</guid><pubDate>Wed, 18 Dec 2024 02:03:04 +0000</pubDate></item><item><title>免训练的RAG pipeline</title><link>https://FairyOwO.github.io/post/mian-xun-lian-de-RAG%20pipeline.html</link><description>&gt; 本文的框架是 `llama_index`&#13;
&#13;
&gt; 整理参考自&#13;
&gt; [EasyRAG](https://github.com/BUAADreamer/EasyRAG)&#13;
&gt; [RAG 最佳实践](https://zhuanlan.zhihu.com/p/8861103446)&#13;
&#13;
## 数据读取与处理&#13;
&#13;
将其他类型转换为几种基础类型, 再将对基础类型进行解析&#13;
&#13;
使用到的基础类型为: `markdown`, `html`, `pdf`&#13;
&#13;
使用到的库为: [dify_rag](https://github.com/hustyichi/dify-rag)&#13;
是一个不错的基础类型解析方案, 解析成 `langchain` 的 `Document` 形式&#13;
&gt; 尽管他是给 `dify` 设计, 但他是通用式设计, 可以用在其他地方&#13;
&#13;
```sh&#13;
pip install dify_rag&#13;
```&#13;
&#13;
```python&#13;
from dify_rag.extractor.html_extractor import HtmlExtractor&#13;
from dify_rag.extractor.markdown_extractor import MarkdownExtractor&#13;
&#13;
documents = HtmlExtractor(r'path/to/data.html').extract()  # MarkdownExtractor(r'path/to/data.md').extract()&#13;
&#13;
# 转换成 llama_index 的 Document 格式&#13;
docs = []&#13;
for i in documents:&#13;
    i.metadata['titles'] = str(i.metadata['titles'])  # 兼容性问题, llama_index 不支持 titles 里用list[str]&#13;
    docs.append(Document(text=i.page_content, metadata=i.metadata))&#13;
&#13;
```&#13;
&#13;
## 切分文档&#13;
&#13;
`chunk_size` 在 256 512 1024 中选择, 这里选择的是 512&#13;
`chunk_overlap` 视存储成本, 这里选择是 40&#13;
&#13;
### 父子切分&#13;
&#13;
&gt; 将一份 Document 中的文档, 切分成父子的形式, 检索到子节点的时候, 使用父节点返回, 来拓展上下文&#13;
&#13;
```python&#13;
node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=[512 * 4, 512], chunk_overlap=40)&#13;
nodes = node_parser.get_nodes_from_documents(docs)&#13;
```&#13;
&#13;
### 普通切分&#13;
&#13;
```python&#13;
node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=40)&#13;
nodes = node_parser.get_nodes_from_documents(docs)&#13;
```&#13;
&#13;
### embedding&#13;
&#13;
最好的仍然是使用 `llm` 调整成的 `embedding`, 不过这里考虑到推理成本, 这里使用的是[BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)&#13;
&#13;
```python&#13;
embed_model = HuggingFaceEmbedding(model_name='BAAI/bge-m3', cache_folder='cache')&#13;
```&#13;
&#13;
TODO。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/mian-xun-lian-de-RAG%20pipeline.html</guid><pubDate>Fri, 13 Dec 2024 09:52:55 +0000</pubDate></item><item><title>2024 Advent of Code 复盘与答案 (一) 1-8题</title><link>https://FairyOwO.github.io/post/2024%20Advent%20of%20Code%20-fu-pan-yu-da-an-%20%28-yi-%29%201-8-ti.html</link><description>[Advent of Code](https://adventofcode.com/)&#13;
&#13;
使用 python 编写, 没有整理代码, 所以非常乱(变量乱取名, 没有注释, 逻辑奇怪, 并非最佳实现)&#13;
可以使用 gpt 相关工具辅助查看&#13;
&#13;
&gt; 如果没有特意说明, 变量 a 统一存放所有原始字符串&#13;
&#13;
## 第一题&#13;
&#13;
[https://adventofcode.com/2024/day/1](https://adventofcode.com/2024/day/1)&#13;
&#13;
### 1题&#13;
&#13;
请将数字配对并测量它们之间的距离。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/2024%20Advent%20of%20Code%20-fu-pan-yu-da-an-%20%28-yi-%29%201-8-ti.html</guid><pubDate>Mon, 02 Dec 2024 07:16:27 +0000</pubDate></item><item><title>使用k3s搭建的k8s集群搭建一些简单的应用</title><link>https://FairyOwO.github.io/post/shi-yong-k3s-da-jian-de-k8s-ji-qun-da-jian-yi-xie-jian-dan-de-ying-yong.html</link><description>&gt; 可能并非最佳实现, 没有进行系统性学习, 欢迎交流&#13;
&#13;
## 镜像&#13;
&#13;
### 镜像管理&#13;
&#13;
安装镜像管理平台 Harbor, 来为集群提供镜像源&#13;
&#13;
在[上一篇](https://fairyowo.github.io/post/shi-yong-%20k3s%20-da-jian-%20k8s%20-ji-qun-%28-shi-yong-guo-nei-jing-xiang-%29.html), 介绍了集群的搭建, 在那个时候配置了集群的镜像源, 也就是说, 这个集群有简单的拉镜像能力, 其次, 安装了 helm, 可以根据 helm 仓库拉取应用&#13;
&#13;
这里介绍拉取原始chart仓库的相关文件的方法安装&#13;
&#13;
1. 获取chart仓库的文件&#13;
  前往 [harbor-helm](harbor-helm) 的 github 仓库&#13;
  前往 release, 下载源代码, 解压之后拿到其中的 `templates` 文件夹, `Chart.yaml` 文件, `values.yaml` 文件&#13;
2. 修改 `values.yaml` 文件&#13;
  详见 附录一&#13;
    &gt; 我们的小集群既没有https, 又没有持久化存储, 所以这些都不需要写在配置文件中, 直接关掉这些功能即可&#13;
    &gt; admin 的初始化密码在网页登陆后也能改&#13;
3. 创建 harbor 专属的 namespace&#13;
  ```sh&#13;
  kubectl create namespace harbor&#13;
  ```&#13;
4. 启动&#13;
  ```sh&#13;
  cd /path/to/harbor&#13;
  helm --namespace harbor install harbor .&#13;
  ```&#13;
  如果对配置项后悔, 使用 以下命令更新&#13;
  ```sh&#13;
  helm --namespace harbor upgrade harbor .&#13;
  ```&#13;
&#13;
到 kubepi 中, 即可看到 harbor 的相关镜像在拉取了, 如果镜像配置正确的话, 过一段时间就会拉取成功&#13;
&gt; 在 kubepi 中他会不断重试拉取, 实际上会缓慢拉取成功&#13;
&gt; 我是2M小服务器拉了半个小时+&#13;
&#13;
在拉取成功之后, 第一时间在刚刚 `values.yaml` 中配置的url中登录, 修改admin密码(如果没有修改默认密码)&#13;
&#13;
### 向 Harbor 上传镜像&#13;
&#13;
需要一台使用 docker 的机器, 向 Harbor 上传镜像 `docker pull`&#13;
&gt; 这里我在安装好后才意识到, 我的集群使用的容器运行时 containerd, 好像没有能力对镜像进行修改, 只能对镜像进行部署, 所以令起了一台机子, 安装 docker, 配置镜像源(TODO)&#13;
&gt; 另一种方案是重装集群, 使用 docker 作为 容器运行时, 但我没有选择这个方案&#13;
&#13;
做好准备后 需要对 harbor 进行信任&#13;
&gt; 没有 https 导致的, 有 https 可以跳过这一步&#13;
&#13;
向 `/etc/docker/daemon.json` 写入&#13;
```text&#13;
'insecure-registries': ['harbor_ip:port']&#13;
```&#13;
&#13;
&gt; 你需要自行处理他的json语法&#13;
&#13;
后, 使用&#13;
&#13;
```sh&#13;
systemctl daemon-reload&#13;
systemctl restart docker&#13;
```&#13;
&#13;
重启 docker&#13;
&#13;
在配置好后, 可以推一个简单的 helloworld镜像&#13;
&#13;
```sh&#13;
docker login harbor_ip:port&#13;
# 这里填入你的账号与密码. 我这里是 admin 与相对于的密码&#13;
&#13;
docker run hello-world:latest&#13;
# 修改tag&#13;
docker tag hello-world:latest harbor_ip:port/library/hello-world:latest&#13;
docker push harbor_ip:port/library/hello-world:latest&#13;
```&#13;
&#13;
登录到 harbor 控制台, 即可看到刚刚推送上来的镜像&#13;
&#13;
## 集群拉取镜像&#13;
&#13;
&gt; 如果你没有 https, 则需要以下额外一步, 如果有, 则保证集群可以连接到 harbor 即可&#13;
&#13;
根据不同的集群搭建方法(这里是k3s), 将 harbor 添加进集群可以拉取的镜像源&#13;
&#13;
与普通添加镜像一致, 首先需要到 `/etc/rancher/k3s` 目录下&#13;
&#13;
向其中的 `registries.yaml` 添加内容&#13;
&#13;
```yaml&#13;
mirrors:&#13;
  harbor_ip:port:&#13;
    endpoint:&#13;
     - http://harbor_ip:port&#13;
&#13;
configs:&#13;
  harbor_ip:port:&#13;
    auth:&#13;
      username: 你的 harbor 账号&#13;
      password: 你的 harbor 密码&#13;
```&#13;
&#13;
&gt; 这里可能 `registries.yaml` 已经有内容了(配置过镜像源), 你需要根据 yaml 的语法自行处理他们的关系&#13;
&gt; 每一台集群都要这么做&#13;
&#13;
之后, 重启即可&#13;
```sh&#13;
systemctl restart k3s  # systemctl restart k3s-agent if agent&#13;
```&#13;
&#13;
在编写好 kubectl 使用的 yaml 后, 即可从 harbor 拉取镜像&#13;
&gt; 感谢 claude-sonnet 帮我写yaml&#13;
&#13;
## 实战&#13;
&#13;
这里搭建了一个求生之路2的服务器&#13;
&#13;
&gt; 这里应该有一个求生之路2的简单介绍&#13;
&#13;
### docker机子拉取镜像&#13;
&#13;
这里使用的是 [HoshinoRei/l4d2server-docker](https://github.com/HoshinoRei/l4d2server-docker)&#13;
&#13;
```sh&#13;
docker pull hoshinorei/l4d2server:edge&#13;
```&#13;
&#13;
### 提交镜像&#13;
&#13;
```sh&#13;
docker tag hoshinorei/l4d2server:edge harbor_ip:port/library/hoshinorei/l4d2server:edge&#13;
docker push harbor_ip:port/library/hoshinorei/l4d2server:edge&#13;
```&#13;
&#13;
### 编写 kubectl 使用的 yaml&#13;
&#13;
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```yaml&#13;
apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  name: l4d2server&#13;
spec:&#13;
  replicas: 1&#13;
  selector:&#13;
    matchLabels:&#13;
      app: l4d2server&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: l4d2server&#13;
    spec:&#13;
      containers:&#13;
      - name: l4d2server&#13;
        image: harbor_ip:port/library/hoshinorei/l4d2server:edge&#13;
        command: ['/home/steam/l4d2server/srcds_run', '-game left4dead2', '-secure', '+exec', 'server.cfg', '+map', 'c1m1_hotel', '-port', '27015', '-tickrate 60', '+sv_setmax 31']&#13;
        ports:&#13;
        - containerPort: 27015&#13;
          name: tcp-game&#13;
        - containerPort: 27015&#13;
          protocol: UDP&#13;
          name: udp-game&#13;
        volumeMounts:&#13;
        - name: addons&#13;
          mountPath: /home/steam/l4d2server/left4dead2/addons/&#13;
        - name: server-config&#13;
          mountPath: /home/steam/l4d2server/left4dead2/cfg/server.cfg&#13;
          subPath: server.cfg&#13;
        - name: host-file&#13;
          mountPath: /home/steam/l4d2server/left4dead2/host.txt&#13;
          subPath: host.txt&#13;
        - name: motd-file&#13;
          mountPath: /home/steam/l4d2server/left4dead2/motd.txt&#13;
          subPath: motd.txt&#13;
        - name: cfg&#13;
          mountPath: /home/steam/l4d2server/left4dead2/cfg/&#13;
        &#13;
      volumes:&#13;
        - name: addons&#13;
          hostPath:&#13;
            path: /root/l4d2/addons/&#13;
            type: Directory&#13;
        - name: server-config&#13;
          hostPath:&#13;
            path: /root/l4d2/cfg/&#13;
        - name: host-file&#13;
          configMap:&#13;
            name: l4d2server-host&#13;
        - name: motd-file&#13;
          configMap:&#13;
            name: l4d2server-motd&#13;
        - name: cfg&#13;
          hostPath:&#13;
            path: /root/l4d2/cfg/&#13;
            type: Directory&#13;
&#13;
---&#13;
apiVersion: v1&#13;
kind: Service&#13;
metadata:&#13;
  name: l4d2server&#13;
spec:&#13;
  type: NodePort&#13;
  ports:&#13;
  - name: tcp-game&#13;
    port: 27015&#13;
    targetPort: 27015&#13;
    protocol: TCP&#13;
    nodePort: 30015&#13;
  - name: udp-game&#13;
    port: 27015&#13;
    targetPort: 27015&#13;
    protocol: UDP&#13;
    nodePort: 30016&#13;
  selector:&#13;
    app: l4d2server&#13;
&#13;
---&#13;
apiVersion: v1&#13;
kind: ConfigMap&#13;
metadata:&#13;
  name: l4d2server-host&#13;
data:&#13;
  host.txt: |&#13;
    # 这里放置 host.txt 的内容&#13;
&#13;
---&#13;
apiVersion: v1&#13;
kind: ConfigMap&#13;
metadata:&#13;
  name: l4d2server-motd&#13;
data:&#13;
  motd.txt: |&#13;
    # 这里放置 motd.txt 的内容&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&gt; 具体的参数细节参考 [这里](https://www.bilibili.com/opus/736922474423255104)&#13;
&gt; 需要在当前目录创建 `host.txt` 与 `motd.txt`, 并在里面输入内容, 在服务器进入的时候会显示这些内容&#13;
&#13;
执行&#13;
&#13;
```sh&#13;
kubectl apply -f l4d2server.yaml&#13;
```&#13;
&#13;
### 其他&#13;
&#13;
&gt; 挂载了宿主机的目录来放置 l4d2 mod&#13;
&#13;
进入 kubepi 中, 查看具体被分配到了哪台机子上, 然后去那台机子的 ~/cfg 中, 放置 原始 l4d2 服务器的 cfg (可以通过刚开始的 docker 机子, 进入 docke 容器取得), 之后在 kubepi 重启即可&#13;
&#13;
如果需要添加 mods, 则将 mod 移动到 addons 跟 cfg 中即可(注意 linux 兼容性), 然后重启, 如果需要更改启动命令则需要修改原 yaml&#13;
&#13;
## 附录&#13;
&#13;
### 一&#13;
这里给出常用的 harbor 的 values.yaml 的选项, 复制自 [(https://blog.starry-s.moe/posts/2023/harbor-helm-chart/)](https://blog.starry-s.moe/posts/2023/harbor-helm-chart/)&#13;
&#13;
&lt;details&gt;&lt;summary&gt;修改的选项&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```yaml&#13;
expose:&#13;
# expose type, 可以设置为 ingress, clusterIP, nodePort, nodeBalancer，区分大小写&#13;
# 默认为 ingress（如果不想使用 80/443 标准端口，可以设置为 nodePort，端口为高位 3000X）&#13;
type: ingress&#13;
tls:&#13;
  # 是否启用 TLS (HTTPS)，建议启用&#13;
  enabled: true&#13;
  # TLS Certificate 的来源，可以为 auto, secret 或 none&#13;
  # 如果为 secret，需要在安装 Chart 之前先创建 TLS Secret&#13;
  # 1) auto: generate the tls certificate automatically&#13;
  # 2) secret: read the tls certificate from the specified secret.&#13;
  # The tls certificate can be generated manually or by cert manager&#13;
  # 3) none: configure no tls certificate for the ingress. If the default&#13;
  # tls certificate is configured in the ingress controller, choose this option&#13;
  certSource: secret&#13;
  secret:&#13;
    # The name of secret which contains keys named:&#13;
    # 'tls.crt' - the certificate&#13;
    # 'tls.key' - the private key&#13;
    secretName: 'harbor-tls'&#13;
    # Only needed when the 'expose.type' is 'ingress'.&#13;
    notarySecretName: 'harbor-tls'&#13;
ingress:&#13;
  hosts:&#13;
    # Ingress Host，如果需要允许任意域名/IP 都能访问，将其设置为空字符串（不建议）&#13;
    # 这里填写的域名务必能解析到当前集群&#13;
    core: harbor.example.com&#13;
    notary: notary.example.com&#13;
&#13;
# Harbor external URL&#13;
# 与 Ingress Host 相对应，如果启用了 TLS，那就是 https://&lt;domain&gt;&#13;
# 如果没启用 TLS，那就是 http://&lt;domain&gt;&#13;
# 如果 expose type 为 nodePort，则填写 http(s)://&lt;IP_ADDRESS&gt;:3000X (端口号不能丢)&#13;
externalURL: https://harbor.example.com&#13;
&#13;
# 持久卷配置，默认为 true，如果是测试环境可以设置为 enabled: false (重新安装 Chart 时仓库里所有的数据都会丢失，不建议！)&#13;
# 如果需要启用持久卷，可以在安装 Chart 之前提前创建好 PVC，并配置 subPath&#13;
persistence:&#13;
enabled: true&#13;
resourcePolicy: 'keep'&#13;
persistentVolumeClaim:&#13;
  registry:&#13;
    # 填写已经创建好的 PVC&#13;
    existingClaim: 'harbor-pvc'&#13;
    storageClass: ''&#13;
    # 如果共用一个 PVC，需要设置子目录&#13;
    subPath: 'registry'&#13;
    accessMode: ReadWriteOnce&#13;
    size: 5Gi&#13;
    annotations: {}&#13;
  jobservice:&#13;
    jobLog:&#13;
      existingClaim: 'harbor-pvc'&#13;
      storageClass: ''&#13;
      subPath: 'jobservice'&#13;
      accessMode: ReadWriteOnce&#13;
      size: 1Gi&#13;
      annotations: {}&#13;
  database:&#13;
    existingClaim: 'harbor-pvc'&#13;
    storageClass: ''&#13;
    subPath: 'database'&#13;
    accessMode: ReadWriteOnce&#13;
    size: 1Gi&#13;
    annotations: {}&#13;
  redis:&#13;
    existingClaim: 'harbor-pvc'&#13;
    storageClass: ''&#13;
    subPath: 'redis'&#13;
    accessMode: ReadWriteOnce&#13;
    size: 1Gi&#13;
    annotations: {}&#13;
  trivy:&#13;
    existingClaim: 'harbor-pvc'&#13;
    storageClass: ''&#13;
    subPath: 'trivy'&#13;
    accessMode: ReadWriteOnce&#13;
    size: 5Gi&#13;
    annotations: {}&#13;
&#13;
# Admin 初始密码&#13;
harborAdminPassword: 'Harbor12345'&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/shi-yong-k3s-da-jian-de-k8s-ji-qun-da-jian-yi-xie-jian-dan-de-ying-yong.html</guid><pubDate>Mon, 02 Dec 2024 07:13:01 +0000</pubDate></item><item><title>nailong数据集, 检测nailong的模型, 训练与推理 (一)</title><link>https://FairyOwO.github.io/post/nailong-shu-ju-ji-%2C%20-jian-ce-nailong-de-mo-xing-%2C%20-xun-lian-yu-tui-li-%20%28-yi-%29.html</link><description>&gt; 回归 老本行&#13;
&#13;
偶然得到 nailong 数据集, 分为两块, 一种是给[分类模型使用的数据集](https://huggingface.co/datasets/refoundd/NailongClassification), 另一种是给[目标检测模型使用的数据集](https://huggingface.co/datasets/refoundd/NailongDetection)&#13;
&#13;
&gt; 后者的数据量不是非常多(6张), 等到有足够多的数据或者我一时兴起手动标注在进行研究&#13;
&#13;
## 初版方案&#13;
&#13;
### 数据集选择&#13;
&#13;
在我刚接触这个数据集的时候, 数据集是只有奶龙的(无其他标签的数据), 这个时候第一个想法就是引入其他分类, 这里采用cifer10数据集的数据, 对数据集进行增广&#13;
&#13;
然而, cifer10 的数据分布毕竟与常见群聊内发送的图片不同, 我觉得会影响最终能力, 应该有选择性而不是随意添加其他类型的图片, 在一番搜索之后, 选中了 [表情包数据集](https://github.com/LLM-Red-Team/emo-visual-data)&#13;
&#13;
虽然这个数据集的原计划是用来检测 VLLM 的能力, 但我认为在我们这个任务中也可以使用&#13;
&#13;
### 模型&#13;
&#13;
在敲定数据集之后, 就开始挑选模型了, 因为是个人小项目, 这里采用我个人喜好的模型选择, 使用了 [convnext 系模型](https://github.com/facebookresearch/ConvNeXt)&#13;
&#13;
这个模型的论文是一篇非常经典的实验文, 里面大量探索了一些技巧对模型能力的影响 (各类消融实验), 虽然他是 2020 年推出, 但他对现在的卷积网络的训练技巧的指引很大&#13;
&#13;
具体细节可以搜索相关的模型解析, 这里不再赘述&#13;
&#13;
&lt;details&gt;&lt;summary&gt;model.py&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
# copy from facebook/ConvNeXt&#13;
import torch&#13;
import torch.nn as nn&#13;
import torch.nn.functional as F&#13;
from timm.models.layers import trunc_normal_, DropPath&#13;
from timm.models.registry import register_model&#13;
&#13;
class Block(nn.Module):&#13;
    r''' ConvNeXt Block. There are two equivalent implementations:&#13;
    (1) DwConv -&gt; LayerNorm (channels_first) -&gt; 1x1 Conv -&gt; GELU -&gt; 1x1 Conv; all in (N, C, H, W)&#13;
    (2) DwConv -&gt; Permute to (N, H, W, C); LayerNorm (channels_last) -&gt; Linear -&gt; GELU -&gt; Linear; Permute back&#13;
    We use (2) as we find it slightly faster in PyTorch&#13;
    &#13;
    Args:&#13;
        dim (int): Number of input channels.&#13;
        drop_path (float): Stochastic depth rate. Default: 0.0&#13;
        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.&#13;
    '''&#13;
    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):&#13;
        super().__init__()&#13;
        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv&#13;
        self.norm = LayerNorm(dim, eps=1e-6)&#13;
        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers&#13;
        self.act = nn.GELU()&#13;
        self.pwconv2 = nn.Linear(4 * dim, dim)&#13;
        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), &#13;
                                    requires_grad=True) if layer_scale_init_value &gt; 0 else None&#13;
        self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity()&#13;
&#13;
    def forward(self, x):&#13;
        input = x&#13;
        x = self.dwconv(x)&#13;
        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -&gt; (N, H, W, C)&#13;
        x = self.norm(x)&#13;
        x = self.pwconv1(x)&#13;
        x = self.act(x)&#13;
        x = self.pwconv2(x)&#13;
        if self.gamma is not None:&#13;
            x = self.gamma * x&#13;
        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -&gt; (N, C, H, W)&#13;
&#13;
        x = input + self.drop_path(x)&#13;
        return x&#13;
&#13;
class ConvNeXt(nn.Module):&#13;
    r''' ConvNeXt&#13;
        A PyTorch impl of : `A ConvNet for the 2020s`  -&#13;
          https://arxiv.org/pdf/2201.03545.pdf&#13;
&#13;
    Args:&#13;
        in_chans (int): Number of input image channels. Default: 3&#13;
        num_classes (int): Number of classes for classification head. Default: 1000&#13;
        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]&#13;
        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]&#13;
        drop_path_rate (float): Stochastic depth rate. Default: 0.&#13;
        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.&#13;
        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.&#13;
    '''&#13;
    def __init__(self, in_chans=3, num_classes=1000, &#13;
                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0., &#13;
                 layer_scale_init_value=1e-6, head_init_scale=1.,&#13;
                 ):&#13;
        super().__init__()&#13;
&#13;
        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers&#13;
        stem = nn.Sequential(&#13;
            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),&#13;
            LayerNorm(dims[0], eps=1e-6, data_format='channels_first')&#13;
        )&#13;
        self.downsample_layers.append(stem)&#13;
        for i in range(3):&#13;
            downsample_layer = nn.Sequential(&#13;
                    LayerNorm(dims[i], eps=1e-6, data_format='channels_first'),&#13;
                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),&#13;
            )&#13;
            self.downsample_layers.append(downsample_layer)&#13;
&#13;
        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks&#13;
        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] &#13;
        cur = 0&#13;
        for i in range(4):&#13;
            stage = nn.Sequential(&#13;
                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], &#13;
                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]&#13;
            )&#13;
            self.stages.append(stage)&#13;
            cur += depths[i]&#13;
&#13;
        self.norm = nn.LayerNorm(dims[-1], eps=1e-6) # final norm layer&#13;
        self.head = nn.Linear(dims[-1], num_classes)&#13;
&#13;
        self.apply(self._init_weights)&#13;
        self.head.weight.data.mul_(head_init_scale)&#13;
        self.head.bias.data.mul_(head_init_scale)&#13;
&#13;
    def _init_weights(self, m):&#13;
        if isinstance(m, (nn.Conv2d, nn.Linear)):&#13;
            trunc_normal_(m.weight, std=.02)&#13;
            nn.init.constant_(m.bias, 0)&#13;
&#13;
    def forward_features(self, x):&#13;
        for i in range(4):&#13;
            x = self.downsample_layers[i](x)&#13;
            x = self.stages[i](x)&#13;
        return self.norm(x.mean([-2, -1])) # global average pooling, (N, C, H, W) -&gt; (N, C)&#13;
&#13;
    def forward(self, x):&#13;
        x = self.forward_features(x)&#13;
        x = self.head(x)&#13;
        return x&#13;
&#13;
class LayerNorm(nn.Module):&#13;
    r''' LayerNorm that supports two data formats: channels_last (default) or channels_first. &#13;
    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with &#13;
    shape (batch_size, height, width, channels) while channels_first corresponds to inputs &#13;
    with shape (batch_size, channels, height, width).&#13;
    '''&#13;
    def __init__(self, normalized_shape, eps=1e-6, data_format='channels_last'):&#13;
        super().__init__()&#13;
        self.weight = nn.Parameter(torch.ones(normalized_shape))&#13;
        self.bias = nn.Parameter(torch.zeros(normalized_shape))&#13;
        self.eps = eps&#13;
        self.data_format = data_format&#13;
        if self.data_format not in ['channels_last', 'channels_first']:&#13;
            raise NotImplementedError &#13;
        self.normalized_shape = (normalized_shape, )&#13;
    &#13;
    def forward(self, x):&#13;
        if self.data_format == 'channels_last':&#13;
            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)&#13;
        elif self.data_format == 'channels_first':&#13;
            u = x.mean(1, keepdim=True)&#13;
            s = (x - u).pow(2).mean(1, keepdim=True)&#13;
            x = (x - u) / torch.sqrt(s + self.eps)&#13;
            x = self.weight[:, None, None] * x + self.bias[:, None, None]&#13;
            return x&#13;
&#13;
&#13;
model_urls = {&#13;
    'convnext_tiny_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth',&#13;
    'convnext_small_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth',&#13;
    'convnext_base_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth',&#13;
    'convnext_large_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth',&#13;
    'convnext_tiny_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth',&#13;
    'convnext_small_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth',&#13;
    'convnext_base_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth',&#13;
    'convnext_large_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth',&#13;
    'convnext_xlarge_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth',&#13;
}&#13;
&#13;
@register_model&#13;
def convnext_tiny(pretrained=False,in_22k=False, **kwargs):&#13;
    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)&#13;
    if pretrained:&#13;
        url = model_urls['convnext_tiny_22k'] if in_22k else model_urls['convnext_tiny_1k']&#13;
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu', check_hash=True)&#13;
        model.load_state_dict(checkpoint['model'])&#13;
    return model&#13;
&#13;
@register_model&#13;
def convnext_small(pretrained=False,in_22k=False, **kwargs):&#13;
    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], **kwargs)&#13;
    if pretrained:&#13;
        url = model_urls['convnext_small_22k'] if in_22k else model_urls['convnext_small_1k']&#13;
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')&#13;
        model.load_state_dict(checkpoint['model'])&#13;
    return model&#13;
&#13;
@register_model&#13;
def convnext_base(pretrained=False, in_22k=False, **kwargs):&#13;
    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], **kwargs)&#13;
    if pretrained:&#13;
        url = model_urls['convnext_base_22k'] if in_22k else model_urls['convnext_base_1k']&#13;
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')&#13;
        model.load_state_dict(checkpoint['model'])&#13;
    return model&#13;
&#13;
@register_model&#13;
def convnext_large(pretrained=False, in_22k=False, **kwargs):&#13;
    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], **kwargs)&#13;
    if pretrained:&#13;
        url = model_urls['convnext_large_22k'] if in_22k else model_urls['convnext_large_1k']&#13;
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')&#13;
        model.load_state_dict(checkpoint['model'])&#13;
    return model&#13;
&#13;
@register_model&#13;
def convnext_xlarge(pretrained=False, in_22k=False, **kwargs):&#13;
    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048], **kwargs)&#13;
    if pretrained:&#13;
        assert in_22k, 'only ImageNet-22K pre-trained ConvNeXt-XL is available; please set in_22k=True'&#13;
        url = model_urls['convnext_xlarge_22k']&#13;
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')&#13;
        model.load_state_dict(checkpoint['model'])&#13;
    return model&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&#13;
### 代码&#13;
&#13;
训练代码大部分都是模板, 不过, 我发现我还没有一个属于自己的 trainer, 趁着这次训练模型的时候补充一个&#13;
&#13;
使用别人的 trainer 难免会遇到 debug, 而代码不熟的情况, 自己写的 trainer 可以掌握各种细节&#13;
&#13;
在整理了一些以前代码后, 总结出了覆盖许多训练模型情况的流程, 趁着这个时候测试一下现在ai编码的能力, 将流程发给 claude-sonnet 后, 输出了一版代码, 在我的一些小修小补(补充日志)后, 就可以跑起来了&#13;
&#13;
&lt;details&gt;&lt;summary&gt;trainer.py&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
import gc&#13;
import json&#13;
import logging&#13;
import os&#13;
import shutil&#13;
&#13;
import torch&#13;
from torch import optim&#13;
from torch.amp import GradScaler&#13;
from torch.nn.utils import clip_grad_norm_&#13;
from torch.utils.tensorboard import SummaryWriter&#13;
from tqdm import tqdm&#13;
&#13;
example_config = {&#13;
    'model': 'model_name',&#13;
    'checkpoint_dir': './checkpoints',&#13;
    'tensorboard_dir': './tensorboard',&#13;
    'device': 'cuda',&#13;
    'enable_cudnn_benchmark': True,&#13;
    'enable_amp': False,&#13;
    'learning_rate': 1e-4,&#13;
    'betas': [0.9, 0.999],&#13;
    'eps': 1e-8,&#13;
    'enable_compile': False,&#13;
    'weight_decay': 0.05,&#13;
    'max_steps': 100000,&#13;
    'max_grad_norm': 1.0,&#13;
    'save_every': 10000,&#13;
    'gradient_accumulation_steps': 4&#13;
}&#13;
&#13;
&#13;
class Trainer:&#13;
    def __init__(self, config):&#13;
        self.config = config&#13;
        self.setup_logging()&#13;
        self.setup_device()&#13;
        self.setup_model()&#13;
        self.setup_training()&#13;
        &#13;
    def setup_logging(self):&#13;
        '''设置日志'''&#13;
        logging.basicConfig(&#13;
            level=logging.INFO,&#13;
            format='%(asctime)s %(levelname)s %(message)s'&#13;
        )&#13;
        self.logger = logging.getLogger(__name__)&#13;
        self.writer = SummaryWriter(self.config['tensorboard_dir'])&#13;
        &#13;
    def setup_device(self):&#13;
        '''设置设备'''&#13;
        self.device = torch.device(self.config['device'])&#13;
        torch.backends.cudnn.benchmark = self.config.get('enable_cudnn_benchmark', True)&#13;
        if self.device.type == 'cuda':&#13;
            self.logger.info(f'Using device: {self.device} ({torch.cuda.get_device_name()})')&#13;
        else:&#13;
            self.logger.info(f'Using device: {self.device}')&#13;
            &#13;
    def setup_model(self):&#13;
        '''设置模型、损失函数等'''&#13;
        self.model = self.build_model().to(self.device)&#13;
        if self.config.get('enable_compile', False):&#13;
            self.model.compile()&#13;
        self.criterion = self.build_criterion()&#13;
        &#13;
        # 打印模型信息&#13;
        n_parameters = sum(p.numel() for p in self.model.parameters())&#13;
        self.logger.info(f'Number of parameters: {n_parameters:,}')&#13;
        &#13;
    def setup_training(self):&#13;
        '''设置训练相关组件'''&#13;
        # 优化器&#13;
        self.optimizer = self.build_optimizer()&#13;
        &#13;
        # 学习率调度器&#13;
        self.scheduler = self.build_scheduler()&#13;
        &#13;
        # 梯度缩放器(用于混合精度训练)&#13;
        self.scaler = GradScaler(&#13;
            enabled=self.config.get('enable_amp', False)&#13;
        )&#13;
        self.gradient_accumulation_steps = self.config.get('gradient_accumulation_steps', 1)&#13;
        &#13;
        # 加载检查点&#13;
        self.steps = 0&#13;
        self.best_metric = {}&#13;
        self.load_checkpoint()&#13;
        &#13;
    def build_model(self):&#13;
        '''构建模型(需要子类实现)'''&#13;
        raise NotImplementedError&#13;
        &#13;
    def build_criterion(self):&#13;
        '''构建损失函数(需要子类实现)'''&#13;
        raise NotImplementedError&#13;
        &#13;
    def build_optimizer(self):&#13;
        '''构建优化器'''&#13;
        # 区分需要和不需要weight decay的参数&#13;
        decay_params = []&#13;
        no_decay_params = []&#13;
        for name, param in self.model.named_parameters():&#13;
            if 'bias' in name or 'norm' in name:&#13;
                no_decay_params.append(param)&#13;
            else:&#13;
                decay_params.append(param)&#13;
                &#13;
        opt_params = [&#13;
            {'params': decay_params, 'weight_decay': self.config['weight_decay']},&#13;
            {'params': no_decay_params, 'weight_decay': 0.0}&#13;
        ]&#13;
        &#13;
        return optim.AdamW(&#13;
            opt_params,&#13;
            lr=self.config['learning_rate'],&#13;
            betas=self.config.get('betas', (0.9, 0.999)),&#13;
            eps=self.config.get('eps', 1e-8)&#13;
        )&#13;
        &#13;
    def build_scheduler(self):&#13;
        '''构建学习率调度器(需要子类实现)'''&#13;
        return NotImplementedError&#13;
        &#13;
    def build_dataloader(self):&#13;
        '''构建数据加载器(需要子类实现)'''&#13;
        raise NotImplementedError&#13;
        &#13;
    def train_step(self, batch):&#13;
        '''单步训练(需要子类实现)'''&#13;
        raise NotImplementedError&#13;
        &#13;
    def validate(self):&#13;
        '''验证(需要子类实现)'''&#13;
        raise NotImplementedError&#13;
        &#13;
    def save_checkpoint(self, is_best=False):&#13;
        '''保存检查点'''&#13;
        state = {&#13;
            'model': self.model.state_dict(),&#13;
            'optimizer': self.optimizer.state_dict(),&#13;
            'scheduler': self.scheduler.state_dict(),&#13;
            'scaler': self.scaler.state_dict(),&#13;
            'steps': self.steps,&#13;
            'best_metric': self.best_metric,&#13;
            'config': self.config&#13;
        }&#13;
        &#13;
        # 保存最新检查点&#13;
        torch.save(&#13;
            state,&#13;
            os.path.join(self.config['checkpoint_dir'], 'latest.pt')&#13;
        )&#13;
        &#13;
        # 保存最佳检查点&#13;
        if is_best:&#13;
            shutil.copy(&#13;
                os.path.join(self.config['checkpoint_dir'], 'latest.pt'),&#13;
                os.path.join(self.config['checkpoint_dir'], 'best.pt')&#13;
            )&#13;
            &#13;
    def load_checkpoint(self):&#13;
        '''加载检查点'''&#13;
        checkpoint_path = os.path.join(&#13;
            self.config['checkpoint_dir'],&#13;
            'latest.pt'&#13;
        )&#13;
        &#13;
        if os.path.exists(checkpoint_path):&#13;
            checkpoint = torch.load(&#13;
                checkpoint_path,&#13;
                map_location=self.device&#13;
            )&#13;
            &#13;
            self.model.load_state_dict(checkpoint['model'])&#13;
            self.optimizer.load_state_dict(checkpoint['optimizer'])&#13;
            self.scheduler.load_state_dict(checkpoint['scheduler'])&#13;
            self.scaler.load_state_dict(checkpoint['scaler'])&#13;
            self.steps = checkpoint['steps']&#13;
            self.best_metric = checkpoint['best_metric']&#13;
            &#13;
            self.logger.info(f'Loaded checkpoint from {checkpoint_path}')&#13;
            self.logger.info(f'Training will resume from step {self.steps}')&#13;
    &#13;
    @staticmethod&#13;
    def is_better_performance(baseline_dict, compare_dict):&#13;
        '''&#13;
        判断compare_dict中的指标是否全面超过baseline_dict&#13;
        &#13;
        Args:&#13;
            baseline_dict: 基准字典,格式为 {指标名: 值}&#13;
            compare_dict: 比较字典,格式为 {指标名: 值} &#13;
        &#13;
        Returns:&#13;
            bool: 如果compare_dict中所有指标都严格大于baseline_dict则返回True,否则返回False&#13;
        '''&#13;
        if not baseline_dict:&#13;
            return True&#13;
        &#13;
        # 检查两个字典的键是否一致&#13;
        if set(baseline_dict.keys()) != set(compare_dict.keys()):&#13;
            return False&#13;
            &#13;
        # 检查每个指标是否都有提升&#13;
        for metric in baseline_dict:&#13;
            if compare_dict[metric] &lt;= baseline_dict[metric]:&#13;
                return False&#13;
                &#13;
        return True&#13;
            &#13;
    def train(self):&#13;
        '''训练流程'''&#13;
        train_loader = self.build_dataloader()&#13;
        self.model.train()&#13;
        &#13;
        self.logger.info('Start training...')&#13;
        pbar = tqdm(total=self.config['max_steps'], initial=self.steps)&#13;
        &#13;
        while self.steps &lt; self.config['max_steps']:&#13;
            for batch in train_loader:&#13;
                # 训练一步&#13;
                with torch.autocast(device_type=self.config['device'], enabled=self.config.get('enable_amp', False)):&#13;
                    loss = self.train_step(batch)&#13;
                self.scaler.scale(loss / self.gradient_accumulation_steps).backward()&#13;
                &#13;
                if (self.steps + 1) % self.gradient_accumulation_steps == 0:&#13;
                    # 梯度裁剪&#13;
                    if self.config.get('max_grad_norm', 0) &gt; 0:&#13;
                        self.scaler.unscale_(self.optimizer)&#13;
                        clip_grad_norm_(&#13;
                            self.model.parameters(),&#13;
                            self.config['max_grad_norm']&#13;
                        )&#13;
&#13;
                    # 优化器步进&#13;
                    self.scaler.step(self.optimizer)&#13;
                    self.scaler.update()&#13;
                    self.optimizer.zero_grad(set_to_none=True)&#13;
                self.scheduler.step()&#13;
                &#13;
                # 记录&#13;
                self.writer.add_scalar('train/loss', loss, self.steps)&#13;
                self.writer.add_scalar(&#13;
                    'train/lr',&#13;
                    self.scheduler.get_last_lr()[0],&#13;
                    self.steps&#13;
                )&#13;
                &#13;
                self.steps += 1&#13;
                pbar.update(1)&#13;
                &#13;
                # 验证和保存&#13;
                if self.steps % self.config['save_every'] == 0:&#13;
                    metric = self.validate()&#13;
                    for i in metric:&#13;
                        self.logger.info(f'Validation {i}: {metric[i]}')&#13;
                        self.writer.add_scalar(f'val/{i}', metric[i], self.steps)&#13;
                    &#13;
                    is_best = self.is_better_performance(self.best_metric, metric)&#13;
                    if is_best:&#13;
                        self.best_metric = metric&#13;
&#13;
                    self.model.train()&#13;
                    self.save_checkpoint(is_best)&#13;
                    &#13;
                if self.steps &gt;= self.config['max_steps']:&#13;
                    break&#13;
                &#13;
            gc.collect()&#13;
            torch.cuda.empty_cache()&#13;
                    &#13;
        pbar.close()&#13;
        self.logger.info('Training finished!')&#13;
&#13;
&#13;
def main():&#13;
    '''主函数'''&#13;
    # 加载配置&#13;
    with open('config.json') as f:&#13;
        config = json.load(f)&#13;
        &#13;
    # 创建输出目录&#13;
    os.makedirs(config['checkpoint_dir'], exist_ok=True)&#13;
    os.makedirs(config['tensorboard_dir'], exist_ok=True)&#13;
    &#13;
    # 训练&#13;
    trainer = Trainer(config)&#13;
    trainer.train()&#13;
    &#13;
if __name__ == '__main__':&#13;
    try:&#13;
        main()&#13;
    except KeyboardInterrupt:&#13;
        pass&#13;
``` &#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
`trainer.py` 简单整合了几个常用的训练手段, 比如混合精度训练, 梯度裁剪, 梯度累计, weight_decay(写死了), tensorboard的记录, 断点续训等操作, 需要注意的是, `trainer.py` 没有使用 epoch 作为训练进度, 而是用了更精细的 step (每次迭代参数即为一个step), 使用的时候需要自行实现模型构建, 损失函数构建 学习率调度器 数据集加载器, 单步训练, 验证的流程的子类实现&#13;
&#13;
然后将一些配置放到config中便于读取, 其中有一些配置是必须的, 其他则是子类实现的时候需要的&#13;
&#13;
听起来可能有点抽象, 下面是一个简单的trainer使用案例&#13;
&#13;
&lt;details&gt;&lt;summary&gt;trainer使用案例&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
import torchvision&#13;
import torch&#13;
from trainer import Trainer&#13;
from torchvision.models import resnet18&#13;
from torch.optim.lr_scheduler import LambdaLR&#13;
&#13;
&#13;
&#13;
transform = torchvision.transforms.Compose([&#13;
    torchvision.transforms.ToTensor(),&#13;
    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))&#13;
])&#13;
&#13;
class ConstantLambdaLR(LambdaLR):&#13;
    def __init__(self, optimizer, **kwargs):&#13;
        kwargs['optimizer'] = optimizer&#13;
        kwargs['lr_lambda'] = self._step_inner&#13;
        super().__init__(**kwargs)&#13;
&#13;
    def _step_inner(self, steps):&#13;
        return 1&#13;
&#13;
&#13;
class Cifer10Trainer(Trainer):&#13;
    def __init__(self, config):&#13;
        super().__init__(config)&#13;
&#13;
    def build_model(self):&#13;
        model = resnet18()&#13;
        model.fc = torch.nn.Linear(model.fc.in_features, 10)&#13;
        return model&#13;
    &#13;
    def build_criterion(self):&#13;
        return torch.nn.CrossEntropyLoss()&#13;
    &#13;
    def build_scheduler(self):&#13;
        return ConstantLambdaLR(self.optimizer)&#13;
    &#13;
    def build_dataloader(self):&#13;
        train_dataset = torchvision.datasets.CIFAR10(root='./temp', train=True, download=True, transform=transform)&#13;
        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.config['batch_size'], shuffle=True, num_workers=1)&#13;
        return train_loader&#13;
    &#13;
    def train_step(self, batch):&#13;
        inputs, labels = batch&#13;
        inputs, labels = inputs.to(self.device), labels.to(self.device)&#13;
        outputs = self.model(inputs)&#13;
        loss = self.criterion(outputs, labels)&#13;
        return loss&#13;
    &#13;
    def validate(self):&#13;
        self.model.eval()&#13;
        test_dataset = torchvision.datasets.CIFAR10(root='./temp', train=False, download=True, transform=transform)&#13;
        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=self.config['batch_size'], shuffle=False, num_workers=1)&#13;
        acc = []&#13;
        with torch.inference_mode():&#13;
            for batch in test_loader:&#13;
                inputs, labels = batch&#13;
                inputs, labels = inputs.to(self.device), labels.to(self.device)&#13;
                y_hat = self.model(inputs)&#13;
                acc.append((y_hat.argmax(dim=1) == labels).sum().item() / labels.size(0))&#13;
                &#13;
        return {'acc': sum(acc) / len(acc)}&#13;
                &#13;
&#13;
def main():&#13;
    config = {&#13;
        'model': 'resnet18',&#13;
        'checkpoint_dir': './checkpoints',&#13;
        'tensorboard_dir': './tensorboard',&#13;
        'device': 'cuda',&#13;
        'enable_cudnn_benchmark': True,&#13;
        'enable_amp': False,&#13;
        'learning_rate': 1e-3,&#13;
        'betas': [0.9, 0.999],&#13;
        'eps': 1e-8,&#13;
        'enable_compile': False,&#13;
        'weight_decay': 0.05,&#13;
        'max_steps': 500,&#13;
        'max_grad_norm': 1.0,&#13;
        'save_every': 100,&#13;
        'gradient_accumulation_steps': 1,&#13;
        'batch_size': 32&#13;
    }&#13;
    trainer = Cifer10Trainer(config)&#13;
    trainer.train()&#13;
    &#13;
if __name__ == '__main__':&#13;
    try:&#13;
        main()&#13;
    except KeyboardInterrupt:&#13;
        pass&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&gt; 代码使用cifer10数据集, resnet18作为模型训练的简单的流程&#13;
&#13;
有了流程接下来编写我们的训练代码&#13;
&#13;
&lt;details&gt;&lt;summary&gt;train.py(代码未整理完毕, 非初版代码, 仅供参考)&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
import os&#13;
import random&#13;
&#13;
import cv2&#13;
import numpy as np&#13;
from sklearn.metrics import f1_score&#13;
import torch&#13;
from PIL import Image&#13;
from torch.optim.lr_scheduler import LambdaLR&#13;
from torch.utils.data import DataLoader, Dataset&#13;
from datasets import load_dataset&#13;
from torchvision import transforms&#13;
&#13;
# from torchvision.models import resnet18&#13;
from model import convnext_base&#13;
from trainer import Trainer&#13;
&#13;
image_size = 224&#13;
batch_size = 32&#13;
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')&#13;
&#13;
&#13;
# def get_color_from_image(image_path):&#13;
#     '''&#13;
#     从纯色图片中获取RGB颜色值&#13;
#     返回: (R, G, B)元组&#13;
#     '''&#13;
#     # 读取图片&#13;
#     image = Image.open(image_path).convert('RGB')&#13;
#     # 转换为numpy数组&#13;
#     img_array = np.array(image)&#13;
    &#13;
#     # 获取图片中心点的颜色值&#13;
#     h, w = img_array.shape[:2]&#13;
#     center_color = img_array[h//2, w//2]&#13;
    &#13;
#     # 或者计算整个图片的平均颜色&#13;
#     average_color = img_array.mean(axis=(0,1)).astype(int)&#13;
    &#13;
#     return tuple(average_color)  # 或者 tuple(average_color)&#13;
&#13;
&#13;
# class AugmentationUtils:&#13;
#     @staticmethod&#13;
#     def add_color_mask(image, is_positive):&#13;
#         '''给图片添加颜色遮罩'''&#13;
#         # 转换为numpy数组并确保类型为uint8&#13;
#         image = np.array(image, dtype=np.uint8)&#13;
        &#13;
#         # 创建与图像相同大小的遮罩&#13;
#         mask = np.ones_like(image, dtype=np.uint8)&#13;
        &#13;
#         # 随机生成颜色&#13;
#         if is_positive:&#13;
#             color = [random.randint(0, 255) for _ in range(3)]&#13;
#         else:&#13;
#             color = get_color_from_image('22.png')&#13;
        &#13;
#         # 为遮罩赋予颜色    &#13;
#         for i in range(3):&#13;
#             mask[:, :, i] = color[i]&#13;
        &#13;
#         # 确保mask也是uint8类型&#13;
#         mask = mask.astype(np.uint8)&#13;
        &#13;
#         # 添加遮罩&#13;
#         alpha = 0.5  # 透明度&#13;
#         image = cv2.addWeighted(image, 1-alpha, mask, alpha, 0)&#13;
        &#13;
#         return Image.fromarray(image)&#13;
&#13;
#     @staticmethod&#13;
#     def embed_positive_in_negative(positive_img, negative_img):&#13;
#         '''在负样本中嵌入正样本'''&#13;
#         # 转换为numpy数组&#13;
#         pos_img = np.array(positive_img)&#13;
#         neg_img = np.array(negative_img)&#13;
        &#13;
#         # 确保图像是3通道的&#13;
#         if len(pos_img.shape) == 2:&#13;
#             pos_img = cv2.cvtColor(pos_img, cv2.COLOR_GRAY2BGR)&#13;
#         if len(neg_img.shape) == 2:&#13;
#             neg_img = cv2.cvtColor(neg_img, cv2.COLOR_GRAY2BGR)&#13;
        &#13;
#         # 获取负样本尺寸&#13;
#         h, w = neg_img.shape[:2]&#13;
#         pos_h, pos_w = pos_img.shape[:2]&#13;
        &#13;
#         # 计算合适的缩放比例&#13;
#         scale = min(&#13;
#             random.uniform(0.5, 0.8),&#13;
#             (w * 0.8) / pos_w,&#13;
#             (h * 0.8) / pos_h&#13;
#         )&#13;
        &#13;
#         # 缩放正样本&#13;
#         new_size = (int(pos_w * scale), int(pos_h * scale))&#13;
#         pos_img_resized = cv2.resize(pos_img, new_size)&#13;
        &#13;
#         # 确保有效的随机位置范围&#13;
#         max_x = max(0, w - new_size[0])&#13;
#         max_y = max(0, h - new_size[1])&#13;
        &#13;
#         # 随机选择插入位置&#13;
#         x = random.randint(0, max_x) if max_x &gt; 0 else 0&#13;
#         y = random.randint(0, max_y) if max_y &gt; 0 else 0&#13;
        &#13;
#         # 获取ROI区域并确保与缩放后的正样本具有相同的形状&#13;
#         roi = neg_img[y:y+new_size[1], x:x+new_size[0]]&#13;
        &#13;
#         # 确保ROI和pos_img_resized具有相同的形状和通道数&#13;
#         if roi.shape == pos_img_resized.shape:&#13;
#             # 混合图像&#13;
#             blended = cv2.addWeighted(roi, 0.3, pos_img_resized, 0.7, 0)&#13;
#             neg_img[y:y+new_size[1], x:x+new_size[0]] = blended&#13;
        &#13;
#         return Image.fromarray(neg_img)&#13;
    &#13;
#     @staticmethod&#13;
#     def embed_same(positive_img, negative_img):&#13;
#         '''在负样本中嵌入正样本'''&#13;
#         # 转换为numpy数组&#13;
#         pos_img = np.array(positive_img)&#13;
#         neg_img = np.array(negative_img)&#13;
        &#13;
#         # 确保图像是3通道的&#13;
#         if len(pos_img.shape) == 2:&#13;
#             pos_img = cv2.cvtColor(pos_img, cv2.COLOR_GRAY2BGR)&#13;
#         if len(neg_img.shape) == 2:&#13;
#             neg_img = cv2.cvtColor(neg_img, cv2.COLOR_GRAY2BGR)&#13;
        &#13;
#         # 获取负样本尺寸&#13;
#         h, w = neg_img.shape[:2]&#13;
#         pos_h, pos_w = pos_img.shape[:2]&#13;
        &#13;
#         # 计算合适的缩放比例&#13;
#         scale = min(&#13;
#             random.uniform(0.5, 0.8),&#13;
#             (w * 0.8) / pos_w,&#13;
#             (h * 0.8) / pos_h&#13;
#         )&#13;
        &#13;
#         # 缩放正样本&#13;
#         new_size = (int(pos_w * scale), int(pos_h * scale))&#13;
#         pos_img_resized = cv2.resize(pos_img, new_size)&#13;
        &#13;
#         # 确保有效的随机位置范围&#13;
#         max_x = max(0, w - new_size[0])&#13;
#         max_y = max(0, h - new_size[1])&#13;
        &#13;
#         # 随机选择插入位置&#13;
#         x = random.randint(0, max_x) if max_x &gt; 0 else 0&#13;
#         y = random.randint(0, max_y) if max_y &gt; 0 else 0&#13;
        &#13;
#         # 获取ROI区域并确保与缩放后的正样本具有相同的形状&#13;
#         roi = neg_img[y:y+new_size[1], x:x+new_size[0]]&#13;
        &#13;
#         # 确保ROI和pos_img_resized具有相同的形状和通道数&#13;
#         if roi.shape == pos_img_resized.shape:&#13;
#             # 混合图像&#13;
#             blended = cv2.addWeighted(roi, 0.3, pos_img_resized, 0.7, 0)&#13;
#             neg_img[y:y+new_size[1], x:x+new_size[0]] = blended&#13;
        &#13;
#         return Image.fromarray(neg_img)&#13;
&#13;
#     @staticmethod&#13;
#     def flip_image(image):&#13;
#         '''图片轴对称'''&#13;
#         return Image.fromarray(np.array(image)[:, ::-1])&#13;
    &#13;
#     @staticmethod&#13;
#     def mirror_half_image(image):&#13;
#         img_array = np.array(image)&#13;
    &#13;
#         # 获取图片尺寸&#13;
#         h, w = img_array.shape[:2]&#13;
        &#13;
#         # 取左半边&#13;
#         half_w = w // 2&#13;
#         left_half = img_array[:, :half_w]&#13;
        &#13;
#         # 水平翻转左半边得到右半边&#13;
#         right_half = left_half[:, ::-1]&#13;
        &#13;
#         # 拼接两个半边&#13;
#         mirrored = np.concatenate([left_half, right_half], axis=1)&#13;
        &#13;
#         return Image.fromarray(mirrored)&#13;
    &#13;
&#13;
# def augment_dataset(positive_images, negative_images):&#13;
#     aug_utils = AugmentationUtils()&#13;
#     augmented_data = []&#13;
    &#13;
#     # 增强正样本&#13;
#     for pos_img in positive_images:&#13;
#         img = Image.open(pos_img).convert('RGB')&#13;
#         # 原图&#13;
#         augmented_data.append((img, 1))&#13;
#         # 颜色遮罩&#13;
#         augmented_data.append((aug_utils.add_color_mask(img, True), 1))&#13;
#         # 轴对称&#13;
#         augmented_data.append((aug_utils.flip_image(img), 1))&#13;
#         # 镜像一半&#13;
#         augmented_data.append((aug_utils.mirror_half_image(img), 1))&#13;
#         # 嵌入相同&#13;
#         img_id = random.randint(0, len(positive_images)-1)&#13;
#         aaa = Image.open(positive_images[img_id]).convert('RGB')&#13;
#         augmented_data.append((aug_utils.embed_same(aaa, img), 1))&#13;
        &#13;
    &#13;
#     # 增强负样本&#13;
#     for i, neg_img in enumerate(negative_images):&#13;
#         img = Image.open(neg_img).convert('RGB')&#13;
#         # 原图&#13;
#         augmented_data.append((img, 0))&#13;
#         # 颜色遮罩&#13;
#         augmented_data.append((aug_utils.add_color_mask(img, False), 0))&#13;
#         # 镜像一半&#13;
#         augmented_data.append((aug_utils.mirror_half_image(img), 0))&#13;
#         # 嵌入正样本&#13;
#         pos_img = Image.open(positive_images[random.randint(0, len(positive_images)-1)]).convert('RGB')&#13;
#         augmented_data.append((aug_utils.embed_positive_in_negative(pos_img, img), 1))&#13;
#         # 嵌入相同&#13;
#         img_id = random.randint(0, len(negative_images)-1)&#13;
#         aaa = Image.open(negative_images[img_id]).convert('RGB')&#13;
#         augmented_data.append((aug_utils.embed_same(aaa, img), 0))&#13;
        &#13;
&#13;
        &#13;
#     # # 显示并保存&#13;
#     # for i, (img, label) in enumerate(augmented_data):&#13;
#     #     # img.show()&#13;
#     #     os.makedirs('aug_images', exist_ok=True)&#13;
#     #     img.save(f'aug_images/aug_{i}.jpg')&#13;
    &#13;
#     # 统计&#13;
#     print(f'Positive: {len([x for x, y in augmented_data if y == 1])}, Negative: {len([x for x, y in augmented_data if y == 0])}')&#13;
#     return augmented_data&#13;
&#13;
&#13;
class LinearWarmUpCosineAnnealingLR(LambdaLR):&#13;
    def __init__(self, optimizer, *, warmup_iters, max_learning_rate, min_lr, lr_decay_iters, **kwargs):&#13;
        self.warmup_iters = warmup_iters&#13;
        self.max_learning_rate = max_learning_rate&#13;
        self.lr_decay_iters = lr_decay_iters&#13;
        self.min_lr = min_lr&#13;
        kwargs['optimizer'] = optimizer&#13;
        kwargs['lr_lambda'] = self._step_inner&#13;
        super().__init__(**kwargs)&#13;
&#13;
    def _step_inner(self, steps):&#13;
        if steps &lt; self.warmup_iters:&#13;
            return self.max_learning_rate * steps / self.warmup_iters&#13;
        elif steps &lt; self.lr_decay_iters:&#13;
            return self.min_lr + 0.5 * (1.0 + np.cos((steps - self.warmup_iters) / (self.lr_decay_iters - self.warmup_iters)*np.pi)) * (self.max_learning_rate - self.min_lr)&#13;
        else:&#13;
            return self.min_lr&#13;
&#13;
&#13;
def transform_img(img):&#13;
    # 处理图片&#13;
    img_np = np.array(img)&#13;
    img_tensor = torch.from_numpy(img_np).permute(2, 0, 1)  # C, H, W&#13;
    img_tensor = torch.nn.functional.interpolate(img_tensor.unsqueeze(0), size=(image_size, image_size), mode='bilinear', align_corners=False).squeeze(0)&#13;
    # normalize&#13;
    normalized_img = img_tensor.float() / 255.0&#13;
    return normalized_img&#13;
&#13;
&#13;
transform = transforms.Compose([&#13;
    transforms.Resize((image_size, image_size)),&#13;
    transforms.ToTensor(),&#13;
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),&#13;
])&#13;
&#13;
&#13;
def transform_img_torchvision(data):&#13;
    data['x'] = [transform(img.convert('RGB')) for img in data['image']]&#13;
    return data&#13;
&#13;
&#13;
label_mapping = {&#13;
    'nailong': 0,&#13;
    'emoji': 1,&#13;
    'anime': 2,&#13;
    'others': 3,&#13;
    'long': 4&#13;
}&#13;
&#13;
def extract_datasets():&#13;
    ds = load_dataset('refoundd/NailongClassification', cache_dir='data', split='train')&#13;
    ds = ds.map(lambda x: {'label': label_mapping[x['label']]})&#13;
    ds = ds.map(transform_img_torchvision, remove_columns=['image'], batched=True)&#13;
    dataset = ds.train_test_split(test_size=0.2)&#13;
    return dataset&#13;
&#13;
dataset = extract_datasets()&#13;
&#13;
&#13;
class NaiLongDataset(Dataset):&#13;
    def __init__(self, mode='train'):&#13;
        assert mode in ['train', 'test']&#13;
        self.dataset = dataset[mode]&#13;
&#13;
    def __len__(self):&#13;
        return len(self.dataset)&#13;
&#13;
    def __getitem__(self, idx):&#13;
        item = self.dataset[idx]['x']&#13;
        label = self.dataset[idx]['label']&#13;
        return torch.tensor(item), torch.tensor(label)&#13;
&#13;
&#13;
&#13;
class NaiLongTrainer(Trainer):&#13;
    def __init__(self, config):&#13;
        super().__init__(config)&#13;
&#13;
    def build_model(self):&#13;
        # model = resnet18()&#13;
        # model.fc = torch.nn.Linear(model.fc.in_features, 2)&#13;
        # return model&#13;
        return convnext_base(pretrained=False, num_classes=5)&#13;
    &#13;
    def build_criterion(self):&#13;
        return torch.nn.CrossEntropyLoss()&#13;
    &#13;
    def build_scheduler(self):&#13;
        return LinearWarmUpCosineAnnealingLR(self.optimizer, warmup_iters=self.config['warmup_iters'], max_learning_rate=self.config['max_learning_rate'], min_lr=self.config['min_lr'], lr_decay_iters=self.config['lr_decay_iters'])&#13;
    &#13;
    def build_dataloader(self, mode='train'):&#13;
        dataset = NaiLongDataset(mode='train')&#13;
        return DataLoader(dataset, batch_size=batch_size, shuffle=True)&#13;
    &#13;
    def train_step(self, batch):&#13;
        x, y = batch&#13;
        x, y = x.to(device), y.to(device)&#13;
        return self.criterion(self.model(x), y)&#13;
    &#13;
    def validate(self):&#13;
        self.logger.info('Validating...')&#13;
        self.model.eval()&#13;
        dataloader = self.build_dataloader(mode='test')&#13;
        acc = []&#13;
        f1 = [[], []]&#13;
        with torch.no_grad(): &#13;
            for i, (x, y) in enumerate(dataloader):&#13;
                x, y = x.to(device), y.to(device)&#13;
                # print(f'Validation: {i}, {y}')&#13;
                y_hat = self.model(x)&#13;
                acc.append(torch.sum(torch.argmax(y_hat, dim=1) == y).item() / len(y))&#13;
                f1[0].extend(y.cpu().tolist())&#13;
                f1[1].extend(torch.argmax(y_hat, dim=1).cpu().tolist())&#13;
            f1_scores = f1_score(f1[0], f1[1], average='macro')&#13;
        return {'acc': sum(acc) / len(acc), 'f1': f1_scores}&#13;
&#13;
&#13;
def main():&#13;
    config = {  # test&#13;
        'model': 'convnext_tiny',&#13;
        'checkpoint_dir': './checkpoints',&#13;
        'tensorboard_dir': './tensorboard',&#13;
        'device': 'cuda',&#13;
        'enable_cudnn_benchmark': True,&#13;
        'enable_amp': False,&#13;
        'learning_rate': 1,  # 启动lr_scheduler 这里必须是1&#13;
        'betas': [0.9, 0.999],&#13;
        'eps': 1e-8,&#13;
        'enable_compile': False,&#13;
        'weight_decay': 0.0,&#13;
        'max_steps': 5000,&#13;
        'max_grad_norm': 1.0,&#13;
        'save_every': 500,&#13;
        'gradient_accumulation_steps': 1,&#13;
        'warmup_iters': 500,&#13;
        'max_learning_rate': 1e-3,&#13;
        'min_lr': 1e-4,&#13;
        'lr_decay_iters': 1000&#13;
    }&#13;
    os.makedirs(config['checkpoint_dir'], exist_ok=True)&#13;
    os.makedirs(config['tensorboard_dir'], exist_ok=True)&#13;
    trainer = NaiLongTrainer(config)&#13;
    trainer.train()&#13;
&#13;
if __name__ == '__main__':&#13;
    # 删除tensorboard下的文件, 但不删除文件夹&#13;
    for i in os.listdir('./tensorboard'):&#13;
        os.remove(os.path.join('./tensorboard', i))&#13;
    # 删除checkpoints下的文件&#13;
    for i in os.listdir('./checkpoints'):&#13;
        os.remove(os.path.join('./checkpoints', i))&#13;
    try:&#13;
        main()&#13;
    except KeyboardInterrupt:&#13;
        print('KeyboardInterrupt')&#13;
        &#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
### 数据增广&#13;
&#13;
原始数据集只有两百多张图片, 这个时候无法避免的要做数据增广, 扩展 nailong 标签的数据, 这里因为是初版方案, 也没有非常精细的增广方案, 这里使用了以下几种方式(代码在如上train.py中):&#13;
&#13;
- 给图片添加颜色遮罩&#13;
    让模型不要将遇到黄色的就判定为奶龙&#13;
- 在负样本中嵌入正样本&#13;
    很经典的增广数据的手法&#13;
- 图片轴对称&#13;
- 取图像的一半镜像翻转&#13;
&#13;
### 训练&#13;
&#13;
#### 参数搜索&#13;
&#13;
虽然是个人小项目, 简单的参数搜索不能少, 继续上面写的 `trainer.py`, 我也写了一个简单的 `hyperparameter_seacher.py` 来搜索超参&#13;
&#13;
&lt;details&gt;&lt;summary&gt;hyperparameter_seacher.py&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
from trainer import Trainer&#13;
import optuna&#13;
&#13;
&#13;
example_config = {&#13;
    'model': 'convnext_tiny',&#13;
    'checkpoint_dir': './checkpoints',&#13;
    'tensorboard_dir': './tensorboard',&#13;
    'device': 'cuda',&#13;
    'enable_cudnn_benchmark': True,&#13;
    'enable_amp': False,&#13;
    'learning_rate': 1e-4,&#13;
    'betas': [0.9, 0.999],&#13;
    'eps': 1e-8,&#13;
    'enable_compile': False,&#13;
    'weight_decay': 0.05,&#13;
    'max_steps': 100,&#13;
    'max_grad_norm': 1.0,&#13;
    'save_every': 1000000,  # 不保存&#13;
    'gradient_accumulation_steps': 4&#13;
}&#13;
&#13;
example_search_config = {&#13;
    'params': {&#13;
        'learning_rate': {&#13;
            'type': 'float',&#13;
            'range': [1e-5, 1e-2],&#13;
            'log': True&#13;
        },&#13;
        'gradient_accumulation_steps': {&#13;
            'type': 'int',&#13;
            'range': [1, 8],&#13;
            'log': False&#13;
        }&#13;
    },&#13;
    'if_save_info': False,&#13;
    'n_trials': 10&#13;
}&#13;
&#13;
class HyperparameterSearcher:&#13;
    def __init__(self, config, trainer):&#13;
        assert isinstance(trainer, Trainer), 'trainer must be an instance of Trainer'&#13;
        self.config = config&#13;
        self.trainer = trainer&#13;
        &#13;
    def objective(self, trial):&#13;
        search_params = self.config['params']&#13;
        &#13;
        for param_name, param_config in search_params.items():&#13;
            if param_config['type'] == 'float':&#13;
                self.trainer.config[param_name] = trial.suggest_float(&#13;
                    param_name,&#13;
                    param_config['range'][0],&#13;
                    param_config['range'][1],&#13;
                    log=param_config.get('log', False)&#13;
                )&#13;
            elif param_config['type'] == 'int':&#13;
                self.trainer.config[param_name] = trial.suggest_int(&#13;
                    param_name,&#13;
                    param_config['range'][0],&#13;
                    param_config['range'][1]&#13;
                )&#13;
            elif param_config['type'] == 'list':&#13;
                self.trainer.config[param_name] = trial.suggest_categorical(&#13;
                    param_name,&#13;
                    param_config['range']&#13;
                )&#13;
            else:&#13;
                raise ValueError(f'Unsupported parameter type: {param_config['type']}, only support float and int')&#13;
        &#13;
        self.trainer.setup_training()&#13;
        self.trainer.train()&#13;
        metric = self.trainer.validate()&#13;
        if 'acc' not in metric:&#13;
            raise ValueError('metric must contain 'acc'')&#13;
        return -metric['acc']  # only support maximizing acc&#13;
    &#13;
    def search(self):&#13;
        study = optuna.create_study(direction='maximize')&#13;
        study.optimize(self.objective, n_trials=self.config['n_trials'])&#13;
        print('Best params:', study.best_params)&#13;
        print('Best value:', -study.best_value)&#13;
        if self.config['if_save_info']:&#13;
            study.trials_dataframe().to_csv('./output/optuna_results.csv')&#13;
        return study.best_params&#13;
    &#13;
def main():&#13;
    &#13;
    pass&#13;
&#13;
if __name__ == '__main__':&#13;
    try:&#13;
        main()&#13;
    except KeyboardInterrupt:&#13;
        pass&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
超参搜索需要 trainer 的配合, 使用了与模型无关的 optuna 来跑给定范围的超参值, 这个时候可以给trainer一个比较容易训练的超参设置(短的epoch等), 同时关闭保存模式&#13;
&#13;
我也写了个简单的超参搜索的例子&#13;
&#13;
&lt;details&gt;&lt;summary&gt;hyperparameter_seacher使用案例&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
from example_trainer import Cifer10Trainer&#13;
from hyperparameter_seacher import HyperparameterSearcher&#13;
&#13;
class Cifer10HyperparameterSearcher(HyperparameterSearcher):&#13;
    def __init__(self, config, trainer):&#13;
        super().__init__(config, trainer)&#13;
&#13;
&#13;
def main():&#13;
    search_config = {&#13;
        'params': {&#13;
            'learning_rate': {&#13;
                'type': 'float',&#13;
                'range': [1e-5, 1e-2],&#13;
                'log': True&#13;
            },&#13;
            'gradient_accumulation_steps': {&#13;
                'type': 'int',&#13;
                'range': [1, 8],&#13;
                'log': False&#13;
            }&#13;
        },&#13;
        'if_save_info': True,&#13;
        'n_trials': 10&#13;
    }&#13;
&#13;
    trainer_config = {&#13;
        'model': 'resnet18',&#13;
        'checkpoint_dir': './checkpoints',&#13;
        'tensorboard_dir': './tensorboard',&#13;
        'device': 'cuda',&#13;
        'enable_cudnn_benchmark': True,&#13;
        'enable_amp': False,&#13;
        'learning_rate': 1e-3,&#13;
        'betas': [0.9, 0.999],&#13;
        'eps': 1e-8,&#13;
        'enable_compile': False,&#13;
        'weight_decay': 0.05,&#13;
        'max_steps': 500,&#13;
        'max_grad_norm': 1.0,&#13;
        'save_every': 10000,  # large than max_steps, no save&#13;
        'gradient_accumulation_steps': 4,&#13;
        'batch_size': 32&#13;
    }&#13;
    trainer = Cifer10Trainer(trainer_config)&#13;
    searcher = Cifer10HyperparameterSearcher(search_config, trainer)&#13;
    best_params = searcher.search()&#13;
    print(best_params)&#13;
&#13;
if __name__ == '__main__':&#13;
    main()&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&gt; 搜索上面那个例子中的合适的超参数&#13;
&#13;
在准备好这些后, 编写我们项目的超参搜索器&#13;
&#13;
&lt;details&gt;&lt;summary&gt;之后补充&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&gt; 搜索出来的最佳超参是一个很长的小数, 四舍五入合适的位数即可&#13;
&#13;
#### 第一次训练&#13;
&#13;
在准备好后, 开始第一次训练&#13;
&#13;
在较新的GPU下, 训练以前较小的模型可谓降维打击, 不到一个小时训练完毕&#13;
&#13;
然而, 第一个问题出来了&#13;
&#13;
acc很高, f1很低&#13;
&#13;
编写测试代码:&#13;
&#13;
&lt;details&gt;&lt;summary&gt;test.py(非初版代码, 仅供参考)&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
from sklearn.metrics import f1_score&#13;
import torch&#13;
&#13;
from model import convnext_base&#13;
from PIL import Image&#13;
import numpy as np&#13;
from glob import glob&#13;
from torchvision import transforms&#13;
&#13;
device = 'cuda'&#13;
image_size = 224&#13;
&#13;
model = convnext_base(pretrained=False, num_classes=5).to(device)&#13;
# model = resnet18()&#13;
# model.fc = torch.nn.Linear(model.fc.in_features, 2)&#13;
checkpoint = torch.load('./checkpoints/best.pt', map_location=device)&#13;
model.load_state_dict(checkpoint['model'])&#13;
&#13;
transform = transforms.Compose([&#13;
    transforms.Resize((image_size, image_size)),&#13;
    transforms.ToTensor(),&#13;
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),&#13;
])&#13;
&#13;
def transform_img(img):&#13;
    # 处理图片&#13;
    img_np = np.array(img)&#13;
    img_tensor = torch.from_numpy(img_np).permute(2, 0, 1)  # C, H, W&#13;
    img_tensor = torch.nn.functional.interpolate(img_tensor.unsqueeze(0), size=(image_size, image_size), mode='bilinear', align_corners=False).squeeze(0)&#13;
    # normalize&#13;
    normalized_img = img_tensor.float() / 255.0&#13;
    return normalized_img&#13;
&#13;
&#13;
def get_input_images(image_path):&#13;
    img = Image.open(image_path).convert('RGB')&#13;
    img = transform(img)&#13;
    return torch.tensor(img).to(device).unsqueeze(0)&#13;
&#13;
model.eval()&#13;
&#13;
# 导出onnx&#13;
input_names = ['input']&#13;
output_names = ['output']&#13;
dynamic_axes = {&#13;
    'input': {0: 'batch_size'},  # 输入的第一个维度是动态的&#13;
    'output': {0: 'batch_size'}  # 输出的第一个维度是动态的&#13;
}&#13;
torch.onnx.export(model, torch.randn(1, 3, 224, 224).to(device), 'model.onnx', input_names=input_names, output_names=output_names, dynamic_axes=dynamic_axes, opset_version=11)&#13;
&#13;
with torch.no_grad():&#13;
    # image = torch.randn(1, 3, 256, 256)&#13;
    print(torch.softmax(model(get_input_images('1.jpg')), dim=1))&#13;
    input()&#13;
    print(torch.softmax(model(get_input_images('3.jpg')), dim=1))&#13;
    input()&#13;
    acc = []&#13;
    f1 = [[], []]&#13;
    &#13;
    &#13;
    for file in glob('./datasets/nailong/*'):&#13;
        y_hat, y = model(get_input_images(file)), torch.tensor([0]).to(device)&#13;
        acc.append(torch.sum(torch.argmax(y_hat, dim=1) == y).item() / len(y))&#13;
        f1[0].append(y.cpu().tolist()[0])&#13;
        f1[1].append(torch.argmax(y_hat, dim=1).cpu().tolist()[0])&#13;
        &#13;
&#13;
    # for file in glob('./datasets/cifer10/*'):&#13;
    #     y_hat, y = model(get_input_images(file)), torch.tensor([3]).to(device)&#13;
    #     acc.append(torch.sum(torch.argmax(y_hat, dim=1) == y).item() / len(y))&#13;
    #     f1[0].append(y.cpu().tolist()[0])&#13;
    #     f1[1].append(torch.argmax(y_hat, dim=1).cpu().tolist()[0])&#13;
&#13;
print(sum(acc) / len(acc))&#13;
f1_scores = f1_score(f1[0], f1[1], average='macro')&#13;
print(f1_scores)&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&#13;
发现 acc 与 f1 的值非常低(百分之10到百分之20附近)&#13;
&#13;
这个时候查看模型的输出, 发现模型的输出接近初始化的输出(softmax后), 模型没有怎么被训练&#13;
&#13;
这个时候怀疑是训练代码出了问题, 然而 cifer10 的 训练并没有什么问题&#13;
检查数据增广代码, 查看增广后的图片, 发现增广做的不是很好, 正样本内嵌负样本没嵌好&#13;
&#13;
在经过修改后, 重启训练&#13;
然而, 问题变成了, 模型的输出接近(0.5, 0.5)(二分类任务)&#13;
&#13;
跟人讨论后, 认为是数据集难度太大, 检查表情包数据集, 都是一些分布与 nailong 数据差异很大的图片. &#13;
&#13;
&gt; 非严格推理, 纯脑测&#13;
模型发现, 给一张新的图片预测 nailong 类, 还是其他类, 都会导致loss上升, 于是干脆摆烂乱猜,  最终的概率分布会输出数据分布, 经过数据增广后的数据恰好是两类 1:1, 模型退化成统计数据集了&#13;
&#13;
导致这个的最直接原因是输入特征不够, 到图像分类就是模型找不到决定图片分类的模式&#13;
&#13;
于是, 第一阶段的训练结束了&#13;
&#13;
#### 第二次训练&#13;
&#13;
在数据集作者不断的努力下, nailong 数据集有了一些完善, 主要的完善点在于: &#13;
1. 新添更多 nailong&#13;
2. 不是二分类了, 新增了表情包分类, 动画分类等五分类, 不过不同类别的数据数量差异很大(两个数量级)&#13;
3. 加入了一些 corner case, 比如 藤田琴音等其他颜色为黄色的图像&#13;
&#13;
因为第一次训练代码已经写好了, 改起来也不是很麻烦, 只需要换个数据集定义与读取. 作者的数据集放在 huggingface 上, 于是我们使用 datasets 进行读取.&#13;
&#13;
&gt; 我也不知道是不是我写的问题, datasets读起来很慢, dataloader 后, 会把 label 自动变成torch.tensor格式, 但是 n, c, h, w 格式的图片只会把 w 维度变成 torch.tensor 格式, 其他维度还是 List, 需要在 dataset 类定义的时候使用 __getitem__() 将数据提前变为 torch.tensor&#13;
&gt; 然后不支持多线程读取(会卡住), 单线程读取读起来很慢, gpu 的 cuda 呈现尖刺状&#13;
&gt; 然后, dataset 的读取**要先**读取 id 再读取 x 跟 label&#13;
&gt; 没怎么用过 dataset, 这次属实是学到了&#13;
&#13;
修改好后数据加载的代码后并注释掉先前的数据增广代码后(后续研究), 第二次训练开始了&#13;
&#13;
这次结果好过头了&#13;
模型的 loss 收敛到了 $1e^{-5}$, acc跟f1更是到达了 $100\%$&#13;
&#13;
使用测试代码简单测试, 发现在数据集的数据都能完美分类, 不在数据集的分类只要分不出是奶龙即可. 检查模型输出权重, 也没啥问题, 看起来是完美了?&#13;
&#13;
然而 这张图还是给了模型一拳&#13;
&#13;
&lt;details&gt;&lt;summary&gt;图&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
![22](https://github.com/user-attachments/assets/3cb2b111-e01f-44dd-8e71-0509ab2bb6c0)&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&#13;
他会识别成 nailong, 不过我觉得问题不大(确实有人把他抽象的认成 nailong)&#13;
&#13;
### 部署&#13;
&#13;
上面的 `test.py` 中 写了onnx导出的代码, 支持任意 batch 的输入(解锁了 n, c, h, w 的 n 维度)&#13;
&#13;
简单编写onnx推理代码&#13;
&#13;
&lt;details&gt;&lt;summary&gt;onnx_inference.py&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```python&#13;
# from torchvision import transforms&#13;
import onnxruntime as ort&#13;
from PIL import Image&#13;
import numpy as np&#13;
&#13;
img_size = 224&#13;
&#13;
# transform = transforms.Compose([&#13;
#     transforms.Resize((img_size, img_size)),&#13;
#     transforms.ToTensor(),&#13;
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),&#13;
# ])&#13;
&#13;
def transform_img(img: Image, image_size=224, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):&#13;
    img = img.convert('RGB').resize((image_size, image_size), Image.Resampling.LANCZOS)&#13;
    img = np.array(img)&#13;
    img = (img / 255 - mean) / std&#13;
    img = img.transpose((2, 0, 1))&#13;
    img = np.expand_dims(img, axis=0)&#13;
    return img.astype(np.float32)&#13;
&#13;
&#13;
label_mapping = {&#13;
    'nailong': 0,&#13;
    'emoji': 1,&#13;
    'anime': 2,&#13;
    'others': 3,&#13;
    'long': 4&#13;
}&#13;
&#13;
reverse_label_mapping = {v: k for k, v in label_mapping.items()}&#13;
&#13;
model_path = 'model.onnx'&#13;
session = ort.InferenceSession(model_path)&#13;
&#13;
image_path = '3.jpg'&#13;
image = Image.open(image_path).convert('RGB')&#13;
# image = transform(image).unsqueeze(0).numpy()&#13;
image = transform_img(image)&#13;
&#13;
# 运行推理&#13;
input_name = session.get_inputs()[0].name&#13;
output_name = session.get_outputs()[0].name&#13;
outputs = session.run([output_name], {input_name: image})&#13;
&#13;
# 获取分类结果&#13;
output = outputs[0]&#13;
predicted_class = np.argmax(output, axis=1)&#13;
predicted_label = reverse_label_mapping[predicted_class[0]]&#13;
&#13;
print(f'Predicted class: {predicted_label}')&#13;
&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&gt; 训练的时候引入了 torchvision 的 transforms, 这里为了减少依赖, 选择手动实现, 有需要也可以自行取消注释并修改&#13;
。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/nailong-shu-ju-ji-%2C%20-jian-ce-nailong-de-mo-xing-%2C%20-xun-lian-yu-tui-li-%20%28-yi-%29.html</guid><pubDate>Tue, 26 Nov 2024 12:53:02 +0000</pubDate></item><item><title>使用 k3s 搭建 k8s 集群(使用国内镜像)</title><link>https://FairyOwO.github.io/post/shi-yong-%20k3s%20-da-jian-%20k8s%20-ji-qun-%28-shi-yong-guo-nei-jing-xiang-%29.html</link><description>&gt; 历史文章搬运&#13;
&#13;
&gt; 注: 此处k3s为具体的 Kubernetes 发行版, 后面的k8s为 Kubernetes 的缩写, Kubernetes 是开源容器编排平台&#13;
&#13;
需求之初是想对年抛机, 月抛机进行统一的管理, 方便部署相关镜像, 类似于史莱姆的结构(拿到新的机器, 加入集群, 机器时间过期, 自动离线, 伸缩重启分配全由集群本身管理)&#13;
&#13;
使用系统为 Debian&#13;
&#13;
## 服务器搭建&#13;
&#13;
### 搭建集群&#13;
&#13;
主 server sh脚本&#13;
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```sh&#13;
echo 'deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware&#13;
deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware&#13;
&#13;
deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware&#13;
deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware&#13;
&#13;
deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware&#13;
deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware&#13;
&#13;
# 以下安全更新软件源包含了官方源与镜像站配置，如有需要可自行修改注释切换&#13;
deb https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware&#13;
deb-src https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware' &gt; /etc/apt/sources.list&#13;
&#13;
apt update&#13;
&#13;
curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \&#13;
  INSTALL_K3S_MIRROR=cn \&#13;
  sh -s - server \&#13;
  --cluster-init \&#13;
  --system-default-registry=registry.cn-hangzhou.aliyuncs.com&#13;
&#13;
cat /var/lib/rancher/k3s/server/token&#13;
&#13;
cat &gt;&gt; /etc/rancher/k3s/registries.yaml &lt;&lt; EOF&#13;
mirrors:&#13;
  docker.io:&#13;
    endpoint:&#13;
      - 'https://dockerproxy.net'&#13;
      - 'https://registry.cn-hangzhou.aliyuncs.com/'&#13;
      - 'https://mirror.ccs.tencentyun.com'&#13;
  k8s.gcr.io:&#13;
    endpoint:&#13;
      - 'https://k8s.dockerproxy.net'&#13;
      - 'https://registry.aliyuncs.com/google_containers'&#13;
  ghcr.io:&#13;
    endpoint:&#13;
      - 'https://ghcr.dockerproxy.net'&#13;
      - 'https://ghcr.m.daocloud.io/'&#13;
  gcr.io:&#13;
    endpoint:&#13;
      - 'https://gcr.dockerproxy.net'&#13;
      - 'https://gcr.m.daocloud.io/'&#13;
  quay.io:&#13;
    endpoint:&#13;
      - 'https://quay.dockerproxy.net'&#13;
      - 'https://quay.tencentcloudcr.com/'&#13;
  registry.k8s.io:&#13;
    endpoint:&#13;
      - 'https://k8s.dockerproxy.net'&#13;
      - 'https://registry.aliyuncs.com/v2/google_containers'&#13;
EOF&#13;
systemctl restart k3s&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
副 server sh脚本&#13;
&#13;
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```sh&#13;
echo 'deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware&#13;
deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware&#13;
&#13;
deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware&#13;
deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware&#13;
&#13;
deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware&#13;
deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware&#13;
&#13;
# 以下安全更新软件源包含了官方源与镜像站配置，如有需要可自行修改注释切换&#13;
deb https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware&#13;
deb-src https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware' &gt; /etc/apt/sources.list&#13;
&#13;
apt update&#13;
&#13;
curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \&#13;
  INSTALL_K3S_MIRROR=cn \&#13;
  sh -s - server \&#13;
  --cluster-init \&#13;
  --system-default-registry=registry.cn-hangzhou.aliyuncs.com&#13;
&#13;
cat /var/lib/rancher/k3s/server/token&#13;
&#13;
cat &gt;&gt; /etc/rancher/k3s/registries.yaml &lt;&lt; EOF&#13;
mirrors:&#13;
  docker.io:&#13;
    endpoint:&#13;
      - 'https://dockerproxy.net'&#13;
      - 'https://registry.cn-hangzhou.aliyuncs.com/'&#13;
      - 'https://mirror.ccs.tencentyun.com'&#13;
  k8s.gcr.io:&#13;
    endpoint:&#13;
      - 'https://k8s.dockerproxy.net'&#13;
      - 'https://registry.aliyuncs.com/google_containers'&#13;
  ghcr.io:&#13;
    endpoint:&#13;
      - 'https://ghcr.dockerproxy.net'&#13;
      - 'https://ghcr.m.daocloud.io/'&#13;
  gcr.io:&#13;
    endpoint:&#13;
      - 'https://gcr.dockerproxy.net'&#13;
      - 'https://gcr.m.daocloud.io/'&#13;
  quay.io:&#13;
    endpoint:&#13;
      - 'https://quay.dockerproxy.net'&#13;
      - 'https://quay.tencentcloudcr.com/'&#13;
  registry.k8s.io:&#13;
    endpoint:&#13;
      - 'https://k8s.dockerproxy.net'&#13;
      - 'https://registry.aliyuncs.com/v2/google_containers'&#13;
EOF&#13;
systemctl restart k3s&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
client sh脚本&#13;
&#13;
&lt;details&gt;&lt;summary&gt;Details&lt;/summary&gt;&#13;
&lt;p&gt;&#13;
&#13;
```sh&#13;
echo 'deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware&#13;
deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware&#13;
&#13;
deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware&#13;
deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware&#13;
&#13;
deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware&#13;
deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware&#13;
&#13;
# 以下安全更新软件源包含了官方源与镜像站配置，如有需要可自行修改注释切换&#13;
deb https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware&#13;
deb-src https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware' &gt; /etc/apt/sources.list&#13;
&#13;
apt update&#13;
&#13;
curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \&#13;
  INSTALL_K3S_MIRROR=cn \&#13;
  K3S_URL=https://ip:6443 \&#13;
  K3S_TOKEN=your_token \&#13;
  sh -&#13;
&#13;
mkdir -p /etc/rancher/k3s&#13;
cat &gt;&gt; /etc/rancher/k3s/registries.yaml &lt;&lt; EOF&#13;
mirrors:&#13;
  docker.io:&#13;
    endpoint:&#13;
      - 'https://dockerproxy.net'&#13;
      - 'https://registry.cn-hangzhou.aliyuncs.com/'&#13;
      - 'https://mirror.ccs.tencentyun.com'&#13;
  k8s.gcr.io:&#13;
    endpoint:&#13;
      - 'https://k8s.dockerproxy.net'&#13;
      - 'https://registry.aliyuncs.com/google_containers'&#13;
  ghcr.io:&#13;
    endpoint:&#13;
      - 'https://ghcr.dockerproxy.net'&#13;
      - 'https://ghcr.m.daocloud.io/'&#13;
  gcr.io:&#13;
    endpoint:&#13;
      - 'https://gcr.dockerproxy.net'&#13;
      - 'https://gcr.m.daocloud.io/'&#13;
  quay.io:&#13;
    endpoint:&#13;
      - 'https://quay.dockerproxy.net'&#13;
      - 'https://quay.tencentcloudcr.com/'&#13;
  registry.k8s.io:&#13;
    endpoint:&#13;
      - 'https://k8s.dockerproxy.net'&#13;
      - 'https://registry.aliyuncs.com/v2/google_containers'&#13;
EOF&#13;
systemctl restart k3s-agent&#13;
```&#13;
&#13;
&lt;/p&gt;&#13;
&lt;/details&gt; &#13;
&#13;
&gt; 注: k3s 搭建集群的方案需要保证主服务器不离线, 否则整个集群会离线, 考虑到k3s占用低, 机器一般是性能不高的类型, 我也有长期续费的服务器, 故使用这个方案&#13;
&#13;
在主server服务器使用&#13;
&#13;
```sh&#13;
kubectl get nodes -A&#13;
```&#13;
出现每台机子的信息, 代表集群内部网络通信没问题&#13;
&#13;
在主server服务器使用&#13;
```sh&#13;
kubectl get pods --all-namespaces&#13;
```&#13;
在所有服务在 `RUNNING` 状态时, 为安装成功 (这些服务都是内部通信与均衡负载的镜像), 如果是卡在 `container creating`, 则安装失败, 原因是镜像没正确配置&#13;
&#13;
### 安装helm (虽然不知道干什么用, 集群内也自带一个helm)&#13;
&#13;
1. 手动安装&#13;
    1. 下载需要的版本 [下载地址](https://github.com/helm/helm/releases)&#13;
    2. 解压, 上传到服务器, chmod给执行权限&#13;
    3. 移动到环境变量的目录中&#13;
        ```sh&#13;
        mv helm /usr/local/bin/helm&#13;
        ```&#13;
2. 使用脚本安装&#13;
    ```sh&#13;
    https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash&#13;
    ```&#13;
&#13;
## 面板安装&#13;
&#13;
为了简单, 面板选择的是 kubepi&#13;
&#13;
[文档](https://github.com/1Panel-dev/KubePi/wiki/2%E3%80%81%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2#kubernetes)&#13;
&#13;
这里选择的是非持久化部署, 在直接部署在刚刚建好的集群之中&#13;
&#13;
&gt; 持久化部署会有莫名其妙的分配问题, 应该是跟分配本地空间有关系, 我也不需要持久化集群信息(因为只有一个集群), 所以没什么关系&#13;
&#13;
```sh&#13;
# 安装&#13;
sudo kubectl apply -f https://raw.githubusercontent.com/1Panel-dev/KubePi/master/docs/deploy/kubectl/kubepi.yaml&#13;
```&#13;
&#13;
安装完成后, 根据安装教程, 获取访问地址&#13;
&#13;
```sh&#13;
# 获取 NodeIp&#13;
export NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[0].address}')&#13;
# 获取 NodePort&#13;
export NODE_PORT=$(kubectl -n kube-system get services kubepi -o jsonpath='{.spec.ports[0].nodePort}')&#13;
# 获取 Address&#13;
echo http://$NODE_IP:$NODE_PORT&#13;
```&#13;
&#13;
&gt; 注: 内网组机子的时候这里会是内网地址, 需要使用端口转发转发到 `0.0.0.0` 之后才能外网访问&#13;
&gt; ```sh&#13;
&gt; kubectl port-forward --address 0.0.0.0 kubepi-d8477f9d8-drthz -n kube-system 2999:80&#13;
&gt; ```&#13;
&gt; 此命令不会中断, 会持续运行, 需要把这条命令中的 `kubepi-d8477f9d8-drthz` 换成实际名字&#13;
&#13;
登陆系统&#13;
&#13;
```text&#13;
地址: http://$NODE_IP:$NODE_PORT&#13;
用户名: admin&#13;
密码: kubepi&#13;
```&#13;
&#13;
登陆后记得修改密码&#13;
&#13;
导入集群&#13;
&#13;
在主服务器, 获取&#13;
&#13;
```sh&#13;
cd /etc/rancher/k3s&#13;
cat k3s.yaml&#13;
```&#13;
在 kubepi 导入集群, 认证模式选择 kubeconfig文件, 把这个文件复制进去&#13;
&#13;
在集群配置中, 配置一下网络, 使之可以直接通过外网端口访问&#13;
&#13;
具体配置流程忘了, 此方法由同事指点&#13;
&#13;
## 部署项目&#13;
&#13;
在 kubepi 中 选择集群, 应用市场, chart 仓库, 填入相关信息, 这里我使用的是:&#13;
```text&#13;
开源社: http://mirror.kaiyuanshe.cn/kubernetes/charts/&#13;
开源应用市场: https://charts.grapps.cn&#13;
```&#13;
&#13;
点开应用就有很多项目跳出来可以部署。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/shi-yong-%20k3s%20-da-jian-%20k8s%20-ji-qun-%28-shi-yong-guo-nei-jing-xiang-%29.html</guid><pubDate>Fri, 22 Nov 2024 09:34:51 +0000</pubDate></item><item><title>待阅读</title><link>https://FairyOwO.github.io/post/dai-yue-du.html</link><description>## 对强化学习进行系统性的学习&#13;
&gt; 此前因为对一些强化学习项目感兴趣, 而草草学习了一部分(感谢 ai, 解释了大部分内容), 这里准备系统性学习一份&#13;
1. [Reinforcement Learning: An Introduction](https://rl.qiwihui.com/zh-cn/latest/) 强化学习导论&#13;
2. [Openai Spinning Up](https://spinningup.qiwihui.com/zh-cn/latest/) OpenAI 深度强化学习&#13;
&#13;
## 知乎 大模型相关&#13;
&#13;
## 苏剑林博客&#13;
&#13;
## 图像生成, llm对prompt优化&#13;
&gt; demo需要&#13;
&#13;
## 图像生成模型 2024年 训练方法。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/dai-yue-du.html</guid><pubDate>Fri, 22 Nov 2024 08:44:27 +0000</pubDate></item><item><title>Test</title><link>https://FairyOwO.github.io/post/Test.html</link><description>这是一个测试!&#13;
This is a test!。</description><guid isPermaLink="true">https://FairyOwO.github.io/post/Test.html</guid><pubDate>Fri, 22 Nov 2024 03:26:34 +0000</pubDate></item></channel></rss>