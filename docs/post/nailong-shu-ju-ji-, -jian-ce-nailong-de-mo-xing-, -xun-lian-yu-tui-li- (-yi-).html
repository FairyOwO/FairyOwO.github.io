<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark_colorblind" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    
    <link rel="icon" href="https://github.githubassets.com/favicons/favicon.svg"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="> 回归 老本行

偶然得到 nailong 数据集, 分为两块, 一种是给[分类模型使用的数据集](https://huggingface.co/datasets/refoundd/NailongClassification), 另一种是给[目标检测模型使用的数据集](https://huggingface.co/datasets/refoundd/NailongDetection)

> 后者的数据量不是非常多(6张), 等到有足够多的数据或者我一时兴起手动标注在进行研究

## 初版方案

### 数据集选择

在我刚接触这个数据集的时候, 数据集是只有奶龙的(无其他标签的数据), 这个时候第一个想法就是引入其他分类, 这里采用cifer10数据集的数据, 对数据集进行增广

然而, cifer10 的数据分布毕竟与常见群聊内发送的图片不同, 我觉得会影响最终能力, 应该有选择性而不是随意添加其他类型的图片, 在一番搜索之后, 选中了 [表情包数据集](https://github.com/LLM-Red-Team/emo-visual-data)

虽然这个数据集的原计划是用来检测 VLLM 的能力, 但我认为在我们这个任务中也可以使用

### 模型

在敲定数据集之后, 就开始挑选模型了, 因为是个人小项目, 这里采用我个人喜好的模型选择, 使用了 [convnext 系模型](https://github.com/facebookresearch/ConvNeXt)

这个模型的论文是一篇非常经典的实验文, 里面大量探索了一些技巧对模型能力的影响 (各类消融实验), 虽然他是 2020 年推出, 但他对现在的卷积网络的训练技巧的指引很大

具体细节可以搜索相关的模型解析, 这里不再赘述

<details><summary>model.py</summary>
<p>

```python
# copy from facebook/ConvNeXt
import torch
import torch.nn as nn
import torch.nn.functional as F
from timm.models.layers import trunc_normal_, DropPath
from timm.models.registry import register_model

class Block(nn.Module):
    r''' ConvNeXt Block. There are two equivalent implementations:
    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)
    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back
    We use (2) as we find it slightly faster in PyTorch
    
    Args:
        dim (int): Number of input channels.
        drop_path (float): Stochastic depth rate. Default: 0.0
        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.
    '''
    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):
        super().__init__()
        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv
        self.norm = LayerNorm(dim, eps=1e-6)
        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers
        self.act = nn.GELU()
        self.pwconv2 = nn.Linear(4 * dim, dim)
        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), 
                                    requires_grad=True) if layer_scale_init_value > 0 else None
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()

    def forward(self, x):
        input = x
        x = self.dwconv(x)
        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)
        x = self.norm(x)
        x = self.pwconv1(x)
        x = self.act(x)
        x = self.pwconv2(x)
        if self.gamma is not None:
            x = self.gamma * x
        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)

        x = input + self.drop_path(x)
        return x

class ConvNeXt(nn.Module):
    r''' ConvNeXt
        A PyTorch impl of : `A ConvNet for the 2020s`  -
          https://arxiv.org/pdf/2201.03545.pdf

    Args:
        in_chans (int): Number of input image channels. Default: 3
        num_classes (int): Number of classes for classification head. Default: 1000
        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]
        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]
        drop_path_rate (float): Stochastic depth rate. Default: 0.
        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.
        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.
    '''
    def __init__(self, in_chans=3, num_classes=1000, 
                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0., 
                 layer_scale_init_value=1e-6, head_init_scale=1.,
                 ):
        super().__init__()

        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers
        stem = nn.Sequential(
            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),
            LayerNorm(dims[0], eps=1e-6, data_format='channels_first')
        )
        self.downsample_layers.append(stem)
        for i in range(3):
            downsample_layer = nn.Sequential(
                    LayerNorm(dims[i], eps=1e-6, data_format='channels_first'),
                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),
            )
            self.downsample_layers.append(downsample_layer)

        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks
        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] 
        cur = 0
        for i in range(4):
            stage = nn.Sequential(
                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], 
                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]
            )
            self.stages.append(stage)
            cur += depths[i]

        self.norm = nn.LayerNorm(dims[-1], eps=1e-6) # final norm layer
        self.head = nn.Linear(dims[-1], num_classes)

        self.apply(self._init_weights)
        self.head.weight.data.mul_(head_init_scale)
        self.head.bias.data.mul_(head_init_scale)

    def _init_weights(self, m):
        if isinstance(m, (nn.Conv2d, nn.Linear)):
            trunc_normal_(m.weight, std=.02)
            nn.init.constant_(m.bias, 0)

    def forward_features(self, x):
        for i in range(4):
            x = self.downsample_layers[i](x)
            x = self.stages[i](x)
        return self.norm(x.mean([-2, -1])) # global average pooling, (N, C, H, W) -> (N, C)

    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x)
        return x

class LayerNorm(nn.Module):
    r''' LayerNorm that supports two data formats: channels_last (default) or channels_first. 
    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with 
    shape (batch_size, height, width, channels) while channels_first corresponds to inputs 
    with shape (batch_size, channels, height, width).
    '''
    def __init__(self, normalized_shape, eps=1e-6, data_format='channels_last'):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.eps = eps
        self.data_format = data_format
        if self.data_format not in ['channels_last', 'channels_first']:
            raise NotImplementedError 
        self.normalized_shape = (normalized_shape, )
    
    def forward(self, x):
        if self.data_format == 'channels_last':
            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        elif self.data_format == 'channels_first':
            u = x.mean(1, keepdim=True)
            s = (x - u).pow(2).mean(1, keepdim=True)
            x = (x - u) / torch.sqrt(s + self.eps)
            x = self.weight[:, None, None] * x + self.bias[:, None, None]
            return x


model_urls = {
    'convnext_tiny_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth',
    'convnext_small_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth',
    'convnext_base_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth',
    'convnext_large_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth',
    'convnext_tiny_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth',
    'convnext_small_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth',
    'convnext_base_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth',
    'convnext_large_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth',
    'convnext_xlarge_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth',
}

@register_model
def convnext_tiny(pretrained=False,in_22k=False, **kwargs):
    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)
    if pretrained:
        url = model_urls['convnext_tiny_22k'] if in_22k else model_urls['convnext_tiny_1k']
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu', check_hash=True)
        model.load_state_dict(checkpoint['model'])
    return model

@register_model
def convnext_small(pretrained=False,in_22k=False, **kwargs):
    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], **kwargs)
    if pretrained:
        url = model_urls['convnext_small_22k'] if in_22k else model_urls['convnext_small_1k']
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')
        model.load_state_dict(checkpoint['model'])
    return model

@register_model
def convnext_base(pretrained=False, in_22k=False, **kwargs):
    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], **kwargs)
    if pretrained:
        url = model_urls['convnext_base_22k'] if in_22k else model_urls['convnext_base_1k']
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')
        model.load_state_dict(checkpoint['model'])
    return model

@register_model
def convnext_large(pretrained=False, in_22k=False, **kwargs):
    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], **kwargs)
    if pretrained:
        url = model_urls['convnext_large_22k'] if in_22k else model_urls['convnext_large_1k']
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')
        model.load_state_dict(checkpoint['model'])
    return model

@register_model
def convnext_xlarge(pretrained=False, in_22k=False, **kwargs):
    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048], **kwargs)
    if pretrained:
        assert in_22k, 'only ImageNet-22K pre-trained ConvNeXt-XL is available; please set in_22k=True'
        url = model_urls['convnext_xlarge_22k']
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')
        model.load_state_dict(checkpoint['model'])
    return model
```

</p>
</details> 


### 代码

训练代码大部分都是模板, 不过, 我发现我还没有一个属于自己的 trainer, 趁着这次训练模型的时候补充一个

使用别人的 trainer 难免会遇到 debug, 而代码不熟的情况, 自己写的 trainer 可以掌握各种细节

在整理了一些以前代码后, 总结出了覆盖许多训练模型情况的流程, 趁着这个时候测试一下现在ai编码的能力, 将流程发给 claude-sonnet 后, 输出了一版代码, 在我的一些小修小补(补充日志)后, 就可以跑起来了

<details><summary>trainer.py</summary>
<p>

```python
import gc
import json
import logging
import os
import shutil

import torch
from torch import optim
from torch.amp import GradScaler
from torch.nn.utils import clip_grad_norm_
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

example_config = {
    'model': 'model_name',
    'checkpoint_dir': './checkpoints',
    'tensorboard_dir': './tensorboard',
    'device': 'cuda',
    'enable_cudnn_benchmark': True,
    'enable_amp': False,
    'learning_rate': 1e-4,
    'betas': [0.9, 0.999],
    'eps': 1e-8,
    'enable_compile': False,
    'weight_decay': 0.05,
    'max_steps': 100000,
    'max_grad_norm': 1.0,
    'save_every': 10000,
    'gradient_accumulation_steps': 4
}


class Trainer:
    def __init__(self, config):
        self.config = config
        self.setup_logging()
        self.setup_device()
        self.setup_model()
        self.setup_training()
        
    def setup_logging(self):
        '''设置日志'''
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s %(levelname)s %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        self.writer = SummaryWriter(self.config['tensorboard_dir'])
        
    def setup_device(self):
        '''设置设备'''
        self.device = torch.device(self.config['device'])
        torch.backends.cudnn.benchmark = self.config.get('enable_cudnn_benchmark', True)
        if self.device.type == 'cuda':
            self.logger.info(f'Using device: {self.device} ({torch.cuda.get_device_name()})')
        else:
            self.logger.info(f'Using device: {self.device}')
            
    def setup_model(self):
        '''设置模型、损失函数等'''
        self.model = self.build_model().to(self.device)
        if self.config.get('enable_compile', False):
            self.model.compile()
        self.criterion = self.build_criterion()
        
        # 打印模型信息
        n_parameters = sum(p.numel() for p in self.model.parameters())
        self.logger.info(f'Number of parameters: {n_parameters:,}')
        
    def setup_training(self):
        '''设置训练相关组件'''
        # 优化器
        self.optimizer = self.build_optimizer()
        
        # 学习率调度器
        self.scheduler = self.build_scheduler()
        
        # 梯度缩放器(用于混合精度训练)
        self.scaler = GradScaler(
            enabled=self.config.get('enable_amp', False)
        )
        self.gradient_accumulation_steps = self.config.get('gradient_accumulation_steps', 1)
        
        # 加载检查点
        self.steps = 0
        self.best_metric = {}
        self.load_checkpoint()
        
    def build_model(self):
        '''构建模型(需要子类实现)'''
        raise NotImplementedError
        
    def build_criterion(self):
        '''构建损失函数(需要子类实现)'''
        raise NotImplementedError
        
    def build_optimizer(self):
        '''构建优化器'''
        # 区分需要和不需要weight decay的参数
        decay_params = []
        no_decay_params = []
        for name, param in self.model.named_parameters():
            if 'bias' in name or 'norm' in name:
                no_decay_params.append(param)
            else:
                decay_params.append(param)
                
        opt_params = [
            {'params': decay_params, 'weight_decay': self.config['weight_decay']},
            {'params': no_decay_params, 'weight_decay': 0.0}
        ]
        
        return optim.AdamW(
            opt_params,
            lr=self.config['learning_rate'],
            betas=self.config.get('betas', (0.9, 0.999)),
            eps=self.config.get('eps', 1e-8)
        )
        
    def build_scheduler(self):
        '''构建学习率调度器(需要子类实现)'''
        return NotImplementedError
        
    def build_dataloader(self):
        '''构建数据加载器(需要子类实现)'''
        raise NotImplementedError
        
    def train_step(self, batch):
        '''单步训练(需要子类实现)'''
        raise NotImplementedError
        
    def validate(self):
        '''验证(需要子类实现)'''
        raise NotImplementedError
        
    def save_checkpoint(self, is_best=False):
        '''保存检查点'''
        state = {
            'model': self.model.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'scheduler': self.scheduler.state_dict(),
            'scaler': self.scaler.state_dict(),
            'steps': self.steps,
            'best_metric': self.best_metric,
            'config': self.config
        }
        
        # 保存最新检查点
        torch.save(
            state,
            os.path.join(self.config['checkpoint_dir'], 'latest.pt')
        )
        
        # 保存最佳检查点
        if is_best:
            shutil.copy(
                os.path.join(self.config['checkpoint_dir'], 'latest.pt'),
                os.path.join(self.config['checkpoint_dir'], 'best.pt')
            )
            
    def load_checkpoint(self):
        '''加载检查点'''
        checkpoint_path = os.path.join(
            self.config['checkpoint_dir'],
            'latest.pt'
        )
        
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(
                checkpoint_path,
                map_location=self.device
            )
            
            self.model.load_state_dict(checkpoint['model'])
            self.optimizer.load_state_dict(checkpoint['optimizer'])
            self.scheduler.load_state_dict(checkpoint['scheduler'])
            self.scaler.load_state_dict(checkpoint['scaler'])
            self.steps = checkpoint['steps']
            self.best_metric = checkpoint['best_metric']
            
            self.logger.info(f'Loaded checkpoint from {checkpoint_path}')
            self.logger.info(f'Training will resume from step {self.steps}')
    
    @staticmethod
    def is_better_performance(baseline_dict, compare_dict):
        '''
        判断compare_dict中的指标是否全面超过baseline_dict
        
        Args:
            baseline_dict: 基准字典,格式为 {指标名: 值}
            compare_dict: 比较字典,格式为 {指标名: 值} 
        
        Returns:
            bool: 如果compare_dict中所有指标都严格大于baseline_dict则返回True,否则返回False
        '''
        if not baseline_dict:
            return True
        
        # 检查两个字典的键是否一致
        if set(baseline_dict.keys()) != set(compare_dict.keys()):
            return False
            
        # 检查每个指标是否都有提升
        for metric in baseline_dict:
            if compare_dict[metric] <= baseline_dict[metric]:
                return False
                
        return True
            
    def train(self):
        '''训练流程'''
        train_loader = self.build_dataloader()
        self.model.train()
        
        self.logger.info('Start training...')
        pbar = tqdm(total=self.config['max_steps'], initial=self.steps)
        
        while self.steps < self.config['max_steps']:
            for batch in train_loader:
                # 训练一步
                with torch.autocast(device_type=self.config['device'], enabled=self.config.get('enable_amp', False)):
                    loss = self.train_step(batch)
                self.scaler.scale(loss / self.gradient_accumulation_steps).backward()
                
                if (self.steps + 1) % self.gradient_accumulation_steps == 0:
                    # 梯度裁剪
                    if self.config.get('max_grad_norm', 0) > 0:
                        self.scaler.unscale_(self.optimizer)
                        clip_grad_norm_(
                            self.model.parameters(),
                            self.config['max_grad_norm']
                        )

                    # 优化器步进
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    self.optimizer.zero_grad(set_to_none=True)
                self.scheduler.step()
                
                # 记录
                self.writer.add_scalar('train/loss', loss, self.steps)
                self.writer.add_scalar(
                    'train/lr',
                    self.scheduler.get_last_lr()[0],
                    self.steps
                )
                
                self.steps += 1
                pbar.update(1)
                
                # 验证和保存
                if self.steps % self.config['save_every'] == 0:
                    metric = self.validate()
                    for i in metric:
                        self.logger.info(f'Validation {i}: {metric[i]}')
                        self.writer.add_scalar(f'val/{i}', metric[i], self.steps)
                    
                    is_best = self.is_better_performance(self.best_metric, metric)
                    if is_best:
                        self.best_metric = metric

                    self.model.train()
                    self.save_checkpoint(is_best)
                    
                if self.steps >= self.config['max_steps']:
                    break
                
            gc.collect()
            torch.cuda.empty_cache()
                    
        pbar.close()
        self.logger.info('Training finished!')


def main():
    '''主函数'''
    # 加载配置
    with open('config.json') as f:
        config = json.load(f)
        
    # 创建输出目录
    os.makedirs(config['checkpoint_dir'], exist_ok=True)
    os.makedirs(config['tensorboard_dir'], exist_ok=True)
    
    # 训练
    trainer = Trainer(config)
    trainer.train()
    
if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        pass
``` 

</p>
</details> 

`trainer.py` 简单整合了几个常用的训练手段, 比如混合精度训练, 梯度裁剪, 梯度累计, weight_decay(写死了), tensorboard的记录, 断点续训等操作, 需要注意的是, `trainer.py` 没有使用 epoch 作为训练进度, 而是用了更精细的 step (每次迭代参数即为一个step), 使用的时候需要自行实现模型构建, 损失函数构建 学习率调度器 数据集加载器, 单步训练, 验证的流程的子类实现

然后将一些配置放到config中便于读取, 其中有一些配置是必须的, 其他则是子类实现的时候需要的

听起来可能有点抽象, 下面是一个简单的trainer使用案例

<details><summary>trainer使用案例</summary>
<p>

import torchvision
import torch
from trainer import Trainer
from torchvision.models import resnet18
from torch.optim.lr_scheduler import LambdaLR



transform = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

class ConstantLambdaLR(LambdaLR):
    def __init__(self, optimizer, **kwargs):
        kwargs['optimizer'] = optimizer
        kwargs['lr_lambda'] = self._step_inner
        super().__init__(**kwargs)

    def _step_inner(self, steps):
        return 1


class Cifer10Trainer(Trainer):
    def __init__(self, config):
        super().__init__(config)

    def build_model(self):
        model = resnet18()
        model.fc = torch.nn.Linear(model.fc.in_features, 10)
        return model
    
    def build_criterion(self):
        return torch.nn.CrossEntropyLoss()
    
    def build_scheduler(self):
        return ConstantLambdaLR(self.optimizer)
    
    def build_dataloader(self):
        train_dataset = torchvision.datasets.CIFAR10(root='./temp', train=True, download=True, transform=transform)
        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.config['batch_size'], shuffle=True, num_workers=1)
        return train_loader
    
    def train_step(self, batch):
        inputs, labels = batch
        inputs, labels = inputs.to(self.device), labels.to(self.device)
        outputs = self.model(inputs)
        loss = self.criterion(outputs, labels)
        return loss
    
    def validate(self):
        self.model.eval()
        test_dataset = torchvision.datasets.CIFAR10(root='./temp', train=False, download=True, transform=transform)
        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=self.config['batch_size'], shuffle=False, num_workers=1)
        acc = []
        with torch.inference_mode():
            for batch in test_loader:
                inputs, labels = batch
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                y_hat = self.model(inputs)
                acc.append((y_hat.argmax(dim=1) == labels).sum().item() / labels.size(0))
                
        return {'acc': sum(acc) / len(acc)}
                

def main():
    config = {
        'model': 'resnet18',
        'checkpoint_dir': './checkpoints',
        'tensorboard_dir': './tensorboard',
        'device': 'cuda',
        'enable_cudnn_benchmark': True,
        'enable_amp': False,
        'learning_rate': 1e-3,
        'betas': [0.9, 0.999],
        'eps': 1e-8,
        'enable_compile': False,
        'weight_decay': 0.05,
        'max_steps': 500,
        'max_grad_norm': 1.0,
        'save_every': 100,
        'gradient_accumulation_steps': 1,
        'batch_size': 32
    }
    trainer = Cifer10Trainer(config)
    trainer.train()
    
if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        pass

</p>
</details> 

> 代码使用cifer10数据集, resnet18作为模型训练的简单的流程

有了流程接下来编写我们的训练代码

<details><summary>train.py(代码未整理完毕, 非初版代码, 仅供参考)</summary>
<p>

```python
import os
import random

import cv2
import numpy as np
from sklearn.metrics import f1_score
import torch
from PIL import Image
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.data import DataLoader, Dataset
from datasets import load_dataset
from torchvision import transforms

# from torchvision.models import resnet18
from model import convnext_base
from trainer import Trainer

image_size = 224
batch_size = 32
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')


# def get_color_from_image(image_path):
#     '''
#     从纯色图片中获取RGB颜色值
#     返回: (R, G, B)元组
#     '''
#     # 读取图片
#     image = Image.open(image_path).convert('RGB')
#     # 转换为numpy数组
#     img_array = np.array(image)
    
#     # 获取图片中心点的颜色值
#     h, w = img_array.shape[:2]
#     center_color = img_array[h//2, w//2]
    
#     # 或者计算整个图片的平均颜色
#     average_color = img_array.mean(axis=(0,1)).astype(int)
    
#     return tuple(average_color)  # 或者 tuple(average_color)


# class AugmentationUtils:
#     @staticmethod
#     def add_color_mask(image, is_positive):
#         '''给图片添加颜色遮罩'''
#         # 转换为numpy数组并确保类型为uint8
#         image = np.array(image, dtype=np.uint8)
        
#         # 创建与图像相同大小的遮罩
#         mask = np.ones_like(image, dtype=np.uint8)
        
#         # 随机生成颜色
#         if is_positive:
#             color = [random.randint(0, 255) for _ in range(3)]
#         else:
#             color = get_color_from_image('22.png')
        
#         # 为遮罩赋予颜色    
#         for i in range(3):
#             mask[:, :, i] = color[i]
        
#         # 确保mask也是uint8类型
#         mask = mask.astype(np.uint8)
        
#         # 添加遮罩
#         alpha = 0.5  # 透明度
#         image = cv2.addWeighted(image, 1-alpha, mask, alpha, 0)
        
#         return Image.fromarray(image)

#     @staticmethod
#     def embed_positive_in_negative(positive_img, negative_img):
#         '''在负样本中嵌入正样本'''
#         # 转换为numpy数组
#         pos_img = np.array(positive_img)
#         neg_img = np.array(negative_img)
        
#         # 确保图像是3通道的
#         if len(pos_img.shape) == 2:
#             pos_img = cv2.cvtColor(pos_img, cv2.COLOR_GRAY2BGR)
#         if len(neg_img.shape) == 2:
#             neg_img = cv2.cvtColor(neg_img, cv2.COLOR_GRAY2BGR)
        
#         # 获取负样本尺寸
#         h, w = neg_img.shape[:2]
#         pos_h, pos_w = pos_img.shape[:2]
        
#         # 计算合适的缩放比例
#         scale = min(
#             random.uniform(0.5, 0.8),
#             (w * 0.8) / pos_w,
#             (h * 0.8) / pos_h
#         )
        
#         # 缩放正样本
#         new_size = (int(pos_w * scale), int(pos_h * scale))
#         pos_img_resized = cv2.resize(pos_img, new_size)
        
#         # 确保有效的随机位置范围
#         max_x = max(0, w - new_size[0])
#         max_y = max(0, h - new_size[1])
        
#         # 随机选择插入位置
#         x = random.randint(0, max_x) if max_x > 0 else 0
#         y = random.randint(0, max_y) if max_y > 0 else 0
        
#         # 获取ROI区域并确保与缩放后的正样本具有相同的形状
#         roi = neg_img[y:y+new_size[1], x:x+new_size[0]]
        
#         # 确保ROI和pos_img_resized具有相同的形状和通道数
#         if roi.shape == pos_img_resized.shape:
#             # 混合图像
#             blended = cv2.addWeighted(roi, 0.3, pos_img_resized, 0.7, 0)
#             neg_img[y:y+new_size[1], x:x+new_size[0]] = blended
        
#         return Image.fromarray(neg_img)
    
#     @staticmethod
#     def embed_same(positive_img, negative_img):
#         '''在负样本中嵌入正样本'''
#         # 转换为numpy数组
#         pos_img = np.array(positive_img)
#         neg_img = np.array(negative_img)
        
#         # 确保图像是3通道的
#         if len(pos_img.shape) == 2:
#             pos_img = cv2.cvtColor(pos_img, cv2.COLOR_GRAY2BGR)
#         if len(neg_img.shape) == 2:
#             neg_img = cv2.cvtColor(neg_img, cv2.COLOR_GRAY2BGR)
        
#         # 获取负样本尺寸
#         h, w = neg_img.shape[:2]
#         pos_h, pos_w = pos_img.shape[:2]
        
#         # 计算合适的缩放比例
#         scale = min(
#             random.uniform(0.5, 0.8),
#             (w * 0.8) / pos_w,
#             (h * 0.8) / pos_h
#         )
        
#         # 缩放正样本
#         new_size = (int(pos_w * scale), int(pos_h * scale))
#         pos_img_resized = cv2.resize(pos_img, new_size)
        
#         # 确保有效的随机位置范围
#         max_x = max(0, w - new_size[0])
#         max_y = max(0, h - new_size[1])
        
#         # 随机选择插入位置
#         x = random.randint(0, max_x) if max_x > 0 else 0
#         y = random.randint(0, max_y) if max_y > 0 else 0
        
#         # 获取ROI区域并确保与缩放后的正样本具有相同的形状
#         roi = neg_img[y:y+new_size[1], x:x+new_size[0]]
        
#         # 确保ROI和pos_img_resized具有相同的形状和通道数
#         if roi.shape == pos_img_resized.shape:
#             # 混合图像
#             blended = cv2.addWeighted(roi, 0.3, pos_img_resized, 0.7, 0)
#             neg_img[y:y+new_size[1], x:x+new_size[0]] = blended
        
#         return Image.fromarray(neg_img)

#     @staticmethod
#     def flip_image(image):
#         '''图片轴对称'''
#         return Image.fromarray(np.array(image)[:, ::-1])
    
#     @staticmethod
#     def mirror_half_image(image):
#         img_array = np.array(image)
    
#         # 获取图片尺寸
#         h, w = img_array.shape[:2]
        
#         # 取左半边
#         half_w = w // 2
#         left_half = img_array[:, :half_w]
        
#         # 水平翻转左半边得到右半边
#         right_half = left_half[:, ::-1]
        
#         # 拼接两个半边
#         mirrored = np.concatenate([left_half, right_half], axis=1)
        
#         return Image.fromarray(mirrored)
    

# def augment_dataset(positive_images, negative_images):
#     aug_utils = AugmentationUtils()
#     augmented_data = []
    
#     # 增强正样本
#     for pos_img in positive_images:
#         img = Image.open(pos_img).convert('RGB')
#         # 原图
#         augmented_data.append((img, 1))
#         # 颜色遮罩
#         augmented_data.append((aug_utils.add_color_mask(img, True), 1))
#         # 轴对称
#         augmented_data.append((aug_utils.flip_image(img), 1))
#         # 镜像一半
#         augmented_data.append((aug_utils.mirror_half_image(img), 1))
#         # 嵌入相同
#         img_id = random.randint(0, len(positive_images)-1)
#         aaa = Image.open(positive_images[img_id]).convert('RGB')
#         augmented_data.append((aug_utils.embed_same(aaa, img), 1))
        
    
#     # 增强负样本
#     for i, neg_img in enumerate(negative_images):
#         img = Image.open(neg_img).convert('RGB')
#         # 原图
#         augmented_data.append((img, 0))
#         # 颜色遮罩
#         augmented_data.append((aug_utils.add_color_mask(img, False), 0))
#         # 镜像一半
#         augmented_data.append((aug_utils.mirror_half_image(img), 0))
#         # 嵌入正样本
#         pos_img = Image.open(positive_images[random.randint(0, len(positive_images)-1)]).convert('RGB')
#         augmented_data.append((aug_utils.embed_positive_in_negative(pos_img, img), 1))
#         # 嵌入相同
#         img_id = random.randint(0, len(negative_images)-1)
#         aaa = Image.open(negative_images[img_id]).convert('RGB')
#         augmented_data.append((aug_utils.embed_same(aaa, img), 0))
        

        
#     # # 显示并保存
#     # for i, (img, label) in enumerate(augmented_data):
#     #     # img.show()
#     #     os.makedirs('aug_images', exist_ok=True)
#     #     img.save(f'aug_images/aug_{i}.jpg')
    
#     # 统计
#     print(f'Positive: {len([x for x, y in augmented_data if y == 1])}, Negative: {len([x for x, y in augmented_data if y == 0])}')
#     return augmented_data


class LinearWarmUpCosineAnnealingLR(LambdaLR):
    def __init__(self, optimizer, *, warmup_iters, max_learning_rate, min_lr, lr_decay_iters, **kwargs):
        self.warmup_iters = warmup_iters
        self.max_learning_rate = max_learning_rate
        self.lr_decay_iters = lr_decay_iters
        self.min_lr = min_lr
        kwargs['optimizer'] = optimizer
        kwargs['lr_lambda'] = self._step_inner
        super().__init__(**kwargs)

    def _step_inner(self, steps):
        if steps < self.warmup_iters:
            return self.max_learning_rate * steps / self.warmup_iters
        elif steps < self.lr_decay_iters:
            return self.min_lr + 0.5 * (1.0 + np.cos((steps - self.warmup_iters) / (self.lr_decay_iters - self.warmup_iters)*np.pi)) * (self.max_learning_rate - self.min_lr)
        else:
            return self.min_lr


def transform_img(img):
    # 处理图片
    img_np = np.array(img)
    img_tensor = torch.from_numpy(img_np).permute(2, 0, 1)  # C, H, W
    img_tensor = torch.nn.functional.interpolate(img_tensor.unsqueeze(0), size=(image_size, image_size), mode='bilinear', align_corners=False).squeeze(0)
    # normalize
    normalized_img = img_tensor.float() / 255.0
    return normalized_img


transform = transforms.Compose([
    transforms.Resize((image_size, image_size)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


def transform_img_torchvision(data):
    data['x'] = [transform(img.convert('RGB')) for img in data['image']]
    return data


label_mapping = {
    'nailong': 0,
    'emoji': 1,
    'anime': 2,
    'others': 3,
    'long': 4
}

def extract_datasets():
    ds = load_dataset('refoundd/NailongClassification', cache_dir='data', split='train')
    ds = ds.map(lambda x: {'label': label_mapping[x['label']]})
    ds = ds.map(transform_img_torchvision, remove_columns=['image'], batched=True)
    dataset = ds.train_test_split(test_size=0.2)
    return dataset

dataset = extract_datasets()


class NaiLongDataset(Dataset):
    def __init__(self, mode='train'):
        assert mode in ['train', 'test']
        self.dataset = dataset[mode]

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]['x']
        label = self.dataset[idx]['label']
        return torch.tensor(item), torch.tensor(label)



class NaiLongTrainer(Trainer):
    def __init__(self, config):
        super().__init__(config)

    def build_model(self):
        # model = resnet18()
        # model.fc = torch.nn.Linear(model.fc.in_features, 2)
        # return model
        return convnext_base(pretrained=False, num_classes=5)
    
    def build_criterion(self):
        return torch.nn.CrossEntropyLoss()
    
    def build_scheduler(self):
        return LinearWarmUpCosineAnnealingLR(self.optimizer, warmup_iters=self.config['warmup_iters'], max_learning_rate=self.config['max_learning_rate'], min_lr=self.config['min_lr'], lr_decay_iters=self.config['lr_decay_iters'])
    
    def build_dataloader(self, mode='train'):
        dataset = NaiLongDataset(mode='train')
        return DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    def train_step(self, batch):
        x, y = batch
        x, y = x.to(device), y.to(device)
        return self.criterion(self.model(x), y)
    
    def validate(self):
        self.logger.info('Validating...')
        self.model.eval()
        dataloader = self.build_dataloader(mode='test')
        acc = []
        f1 = [[], []]
        with torch.no_grad(): 
            for i, (x, y) in enumerate(dataloader):
                x, y = x.to(device), y.to(device)
                # print(f'Validation: {i}, {y}')
                y_hat = self.model(x)
                acc.append(torch.sum(torch.argmax(y_hat, dim=1) == y).item() / len(y))
                f1[0].extend(y.cpu().tolist())
                f1[1].extend(torch.argmax(y_hat, dim=1).cpu().tolist())
            f1_scores = f1_score(f1[0], f1[1], average='macro')
        return {'acc': sum(acc) / len(acc), 'f1': f1_scores}


def main():
    config = {  # test
        'model': 'convnext_tiny',
        'checkpoint_dir': './checkpoints',
        'tensorboard_dir': './tensorboard',
        'device': 'cuda',
        'enable_cudnn_benchmark': True,
        'enable_amp': False,
        'learning_rate': 1,  # 启动lr_scheduler 这里必须是1
        'betas': [0.9, 0.999],
        'eps': 1e-8,
        'enable_compile': False,
        'weight_decay': 0.0,
        'max_steps': 5000,
        'max_grad_norm': 1.0,
        'save_every': 500,
        'gradient_accumulation_steps': 1,
        'warmup_iters': 500,
        'max_learning_rate': 1e-3,
        'min_lr': 1e-4,
        'lr_decay_iters': 1000
    }
    os.makedirs(config['checkpoint_dir'], exist_ok=True)
    os.makedirs(config['tensorboard_dir'], exist_ok=True)
    trainer = NaiLongTrainer(config)
    trainer.train()

if __name__ == '__main__':
    # 删除tensorboard下的文件, 但不删除文件夹
    for i in os.listdir('./tensorboard'):
        os.remove(os.path.join('./tensorboard', i))
    # 删除checkpoints下的文件
    for i in os.listdir('./checkpoints'):
        os.remove(os.path.join('./checkpoints', i))
    try:
        main()
    except KeyboardInterrupt:
        print('KeyboardInterrupt')
        
```

</p>
</details> 

### 数据增广

原始数据集只有两百多张图片, 这个时候无法避免的要做数据增广, 扩展 nailong 标签的数据, 这里因为是初版方案, 也没有非常精细的增广方案, 这里使用了以下几种方式(代码在如上train.py中):

- 给图片添加颜色遮罩
    让模型不要将遇到黄色的就判定为奶龙
- 在负样本中嵌入正样本
    很经典的增广数据的手法
- 图片轴对称
- 取图像的一半镜像翻转

### 训练

#### 参数搜索

虽然是个人小项目, 简单的参数搜索不能少, 继续上面写的 `trainer.py`, 我也写了一个简单的 `hyperparameter_seacher.py` 来搜索超参

<details><summary>hyperparameter_seacher.py</summary>
<p>

```python
from trainer import Trainer
import optuna


example_config = {
    'model': 'convnext_tiny',
    'checkpoint_dir': './checkpoints',
    'tensorboard_dir': './tensorboard',
    'device': 'cuda',
    'enable_cudnn_benchmark': True,
    'enable_amp': False,
    'learning_rate': 1e-4,
    'betas': [0.9, 0.999],
    'eps': 1e-8,
    'enable_compile': False,
    'weight_decay': 0.05,
    'max_steps': 100,
    'max_grad_norm': 1.0,
    'save_every': 1000000,  # 不保存
    'gradient_accumulation_steps': 4
}

example_search_config = {
    'params': {
        'learning_rate': {
            'type': 'float',
            'range': [1e-5, 1e-2],
            'log': True
        },
        'gradient_accumulation_steps': {
            'type': 'int',
            'range': [1, 8],
            'log': False
        }
    },
    'if_save_info': False,
    'n_trials': 10
}

class HyperparameterSearcher:
    def __init__(self, config, trainer):
        assert isinstance(trainer, Trainer), 'trainer must be an instance of Trainer'
        self.config = config
        self.trainer = trainer
        
    def objective(self, trial):
        search_params = self.config['params']
        
        for param_name, param_config in search_params.items():
            if param_config['type'] == 'float':
                self.trainer.config[param_name] = trial.suggest_float(
                    param_name,
                    param_config['range'][0],
                    param_config['range'][1],
                    log=param_config.get('log', False)
                )
            elif param_config['type'] == 'int':
                self.trainer.config[param_name] = trial.suggest_int(
                    param_name,
                    param_config['range'][0],
                    param_config['range'][1]
                )
            elif param_config['type'] == 'list':
                self.trainer.config[param_name] = trial.suggest_categorical(
                    param_name,
                    param_config['range']
                )
            else:
                raise ValueError(f'Unsupported parameter type: {param_config['type']}, only support float and int')
        
        self.trainer.setup_training()
        self.trainer.train()
        metric = self.trainer.validate()
        if 'acc' not in metric:
            raise ValueError('metric must contain 'acc'')
        return -metric['acc']  # only support maximizing acc
    
    def search(self):
        study = optuna.create_study(direction='maximize')
        study.optimize(self.objective, n_trials=self.config['n_trials'])
        print('Best params:', study.best_params)
        print('Best value:', -study.best_value)
        if self.config['if_save_info']:
            study.trials_dataframe().to_csv('./output/optuna_results.csv')
        return study.best_params
    
def main():
    
    pass

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        pass
```

</p>
</details> 

超参搜索需要 trainer 的配合, 使用了与模型无关的 optuna 来跑给定范围的超参值, 这个时候可以给trainer一个比较容易训练的超参设置(短的epoch等), 同时关闭保存模式

我也写了个简单的超参搜索的例子

<details><summary>hyperparameter_seacher使用案例</summary>
<p>

from example_trainer import Cifer10Trainer
from hyperparameter_seacher import HyperparameterSearcher

class Cifer10HyperparameterSearcher(HyperparameterSearcher):
    def __init__(self, config, trainer):
        super().__init__(config, trainer)


def main():
    search_config = {
        'params': {
            'learning_rate': {
                'type': 'float',
                'range': [1e-5, 1e-2],
                'log': True
            },
            'gradient_accumulation_steps': {
                'type': 'int',
                'range': [1, 8],
                'log': False
            }
        },
        'if_save_info': True,
        'n_trials': 10
    }

    trainer_config = {
        'model': 'resnet18',
        'checkpoint_dir': './checkpoints',
        'tensorboard_dir': './tensorboard',
        'device': 'cuda',
        'enable_cudnn_benchmark': True,
        'enable_amp': False,
        'learning_rate': 1e-3,
        'betas': [0.9, 0.999],
        'eps': 1e-8,
        'enable_compile': False,
        'weight_decay': 0.05,
        'max_steps': 500,
        'max_grad_norm': 1.0,
        'save_every': 10000,  # large than max_steps, no save
        'gradient_accumulation_steps': 4,
        'batch_size': 32
    }
    trainer = Cifer10Trainer(trainer_config)
    searcher = Cifer10HyperparameterSearcher(search_config, trainer)
    best_params = searcher.search()
    print(best_params)

if __name__ == '__main__':
    main()

</p>
</details> 

> 搜索上面那个例子中的合适的超参数

在准备好这些后, 编写我们项目的超参搜索器

<details><summary>之后补充</summary>
<p>



</p>
</details> 

> 搜索出来的最佳超参是一个很长的小数, 四舍五入合适的位数即可

#### 第一次训练

在准备好后, 开始第一次训练

在较新的GPU下, 训练以前较小的模型可谓降维打击, 不到一个小时训练完毕

然而, 第一个问题出来了

acc很高, f1很低

编写测试代码:

<details><summary>test.py(非初版代码, 仅供参考)</summary>
<p>

```python
from sklearn.metrics import f1_score
import torch

from model import convnext_base
from PIL import Image
import numpy as np
from glob import glob
from torchvision import transforms

device = 'cuda'
image_size = 224

model = convnext_base(pretrained=False, num_classes=5).to(device)
# model = resnet18()
# model.fc = torch.nn.Linear(model.fc.in_features, 2)
checkpoint = torch.load('./checkpoints/best.pt', map_location=device)
model.load_state_dict(checkpoint['model'])

transform = transforms.Compose([
    transforms.Resize((image_size, image_size)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

def transform_img(img):
    # 处理图片
    img_np = np.array(img)
    img_tensor = torch.from_numpy(img_np).permute(2, 0, 1)  # C, H, W
    img_tensor = torch.nn.functional.interpolate(img_tensor.unsqueeze(0), size=(image_size, image_size), mode='bilinear', align_corners=False).squeeze(0)
    # normalize
    normalized_img = img_tensor.float() / 255.0
    return normalized_img


def get_input_images(image_path):
    img = Image.open(image_path).convert('RGB')
    img = transform(img)
    return torch.tensor(img).to(device).unsqueeze(0)

model.eval()

# 导出onnx
input_names = ['input']
output_names = ['output']
dynamic_axes = {
    'input': {0: 'batch_size'},  # 输入的第一个维度是动态的
    'output': {0: 'batch_size'}  # 输出的第一个维度是动态的
}
torch.onnx.export(model, torch.randn(1, 3, 224, 224).to(device), 'model.onnx', input_names=input_names, output_names=output_names, dynamic_axes=dynamic_axes, opset_version=11)

with torch.no_grad():
    # image = torch.randn(1, 3, 256, 256)
    print(torch.softmax(model(get_input_images('1.jpg')), dim=1))
    input()
    print(torch.softmax(model(get_input_images('3.jpg')), dim=1))
    input()
    acc = []
    f1 = [[], []]
    
    
    for file in glob('./datasets/nailong/*'):
        y_hat, y = model(get_input_images(file)), torch.tensor([0]).to(device)
        acc.append(torch.sum(torch.argmax(y_hat, dim=1) == y).item() / len(y))
        f1[0].append(y.cpu().tolist()[0])
        f1[1].append(torch.argmax(y_hat, dim=1).cpu().tolist()[0])
        

    # for file in glob('./datasets/cifer10/*'):
    #     y_hat, y = model(get_input_images(file)), torch.tensor([3]).to(device)
    #     acc.append(torch.sum(torch.argmax(y_hat, dim=1) == y).item() / len(y))
    #     f1[0].append(y.cpu().tolist()[0])
    #     f1[1].append(torch.argmax(y_hat, dim=1).cpu().tolist()[0])

print(sum(acc) / len(acc))
f1_scores = f1_score(f1[0], f1[1], average='macro')
print(f1_scores)
```

</p>
</details> 


发现 acc 与 f1 的值非常低(百分之10到百分之20附近)

这个时候查看模型的输出, 发现模型的输出接近初始化的输出(softmax后), 模型没有怎么被训练

这个时候怀疑是训练代码出了问题, 然而 cifer10 的 训练并没有什么问题
检查数据增广代码, 查看增广后的图片, 发现增广做的不是很好, 正样本内嵌负样本没嵌好

在经过修改后, 重启训练
然而, 问题变成了, 模型的输出接近(0.5, 0.5)(二分类任务)

跟人讨论后, 认为是数据集难度太大, 检查表情包数据集, 都是一些分布与 nailong 数据差异很大的图片. 

> 非严格推理, 纯脑测
模型发现, 给一张新的图片预测 nailong 类, 还是其他类, 都会导致loss上升, 于是干脆摆烂乱猜,  最终的概率分布会输出数据分布, 经过数据增广后的数据恰好是两类 1:1, 模型退化成统计数据集了

导致这个的最直接原因是输入特征不够, 到图像分类就是模型找不到决定图片分类的模式

于是, 第一阶段的训练结束了

#### 第二次训练

在数据集作者不断的努力下, nailong 数据集有了一些完善, 主要的完善点在于: 
1. 新添更多 nailong
2. 不是二分类了, 新增了表情包分类, 动画分类等五分类, 不过不同类别的数据数量差异很大(两个数量级)
3. 加入了一些 corner case, 比如 藤田琴音等其他颜色为黄色的图像

因为第一次训练代码已经写好了, 改起来也不是很麻烦, 只需要换个数据集定义与读取. 作者的数据集放在 huggingface 上, 于是我们使用 datasets 进行读取.

> 我也不知道是不是我写的问题, datasets读起来很慢, dataloader 后, 会把 label 自动变成torch.tensor格式, 但是 n, c, h, w 格式的图片只会把 w 维度变成 torch.tensor 格式, 其他维度还是 List, 需要在 dataset 类定义的时候使用 __getitem__() 将数据提前变为 torch.tensor
> 然后不支持多线程读取(会卡住), 单线程读取读起来很慢, gpu 的 cuda 呈现尖刺状
> 然后, dataset 的读取**要先**读取 id 再读取 x 跟 label
> 没怎么用过 dataset, 这次属实是学到了

修改好后数据加载的代码后并注释掉先前的数据增广代码后(后续研究), 第二次训练开始了

这次结果好过头了
模型的 loss 收敛到了 $1e^{-5}$, acc跟f1更是到达了 $100%$

使用测试代码简单测试, 发现在数据集的数据都能完美分类, 不在数据集的分类只要分不出是奶龙即可. 检查模型输出权重, 也没啥问题, 看起来是完美了?

然而 这张图还是给了模型一拳

<details><summary>图</summary>
<p>

![22](https://github.com/user-attachments/assets/3cb2b111-e01f-44dd-8e71-0509ab2bb6c0)

</p>
</details> 


他会识别成 nailong, 不过我觉得问题不大(确实有人把他抽象的认成 nailong)

上面的 `test.py` 中 写了onnx导出的代码, 支持任意 batch 的输入(解锁了 n, c, h, w 的 n 维度)

简单编写onnx推理代码

<details><summary>onnx_inference.py</summary>
<p>

```python
# from torchvision import transforms
import onnxruntime as ort
from PIL import Image
import numpy as np

img_size = 224

# transform = transforms.Compose([
#     transforms.Resize((img_size, img_size)),
#     transforms.ToTensor(),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
# ])

def transform_img(img: Image, image_size=224, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):
    img = img.convert('RGB').resize((image_size, image_size), Image.Resampling.LANCZOS)
    img = np.array(img)
    img = (img / 255 - mean) / std
    img = img.transpose((2, 0, 1))
    img = np.expand_dims(img, axis=0)
    return img.astype(np.float32)


label_mapping = {
    'nailong': 0,
    'emoji': 1,
    'anime': 2,
    'others': 3,
    'long': 4
}

reverse_label_mapping = {v: k for k, v in label_mapping.items()}

model_path = 'model.onnx'
session = ort.InferenceSession(model_path)

image_path = '3.jpg'
image = Image.open(image_path).convert('RGB')
# image = transform(image).unsqueeze(0).numpy()
image = transform_img(image)

# 运行推理
input_name = session.get_inputs()[0].name
output_name = session.get_outputs()[0].name
outputs = session.run([output_name], {input_name: image})

# 获取分类结果
output = outputs[0]
predicted_class = np.argmax(output, axis=1)
predicted_label = reverse_label_mapping[predicted_class[0]]

print(f'Predicted class: {predicted_label}')

```

</p>
</details> 

> 训练的时候引入了 torchvision 的 transforms, 这里为了减少依赖, 选择手动实现, 有需要也可以自行取消注释并修改

### 部署

TODO
。">
<meta property="og:title" content="nailong数据集, 检测nailong的模型, 训练与推理 (一)">
<meta property="og:description" content="> 回归 老本行

偶然得到 nailong 数据集, 分为两块, 一种是给[分类模型使用的数据集](https://huggingface.co/datasets/refoundd/NailongClassification), 另一种是给[目标检测模型使用的数据集](https://huggingface.co/datasets/refoundd/NailongDetection)

> 后者的数据量不是非常多(6张), 等到有足够多的数据或者我一时兴起手动标注在进行研究

## 初版方案

### 数据集选择

在我刚接触这个数据集的时候, 数据集是只有奶龙的(无其他标签的数据), 这个时候第一个想法就是引入其他分类, 这里采用cifer10数据集的数据, 对数据集进行增广

然而, cifer10 的数据分布毕竟与常见群聊内发送的图片不同, 我觉得会影响最终能力, 应该有选择性而不是随意添加其他类型的图片, 在一番搜索之后, 选中了 [表情包数据集](https://github.com/LLM-Red-Team/emo-visual-data)

虽然这个数据集的原计划是用来检测 VLLM 的能力, 但我认为在我们这个任务中也可以使用

### 模型

在敲定数据集之后, 就开始挑选模型了, 因为是个人小项目, 这里采用我个人喜好的模型选择, 使用了 [convnext 系模型](https://github.com/facebookresearch/ConvNeXt)

这个模型的论文是一篇非常经典的实验文, 里面大量探索了一些技巧对模型能力的影响 (各类消融实验), 虽然他是 2020 年推出, 但他对现在的卷积网络的训练技巧的指引很大

具体细节可以搜索相关的模型解析, 这里不再赘述

<details><summary>model.py</summary>
<p>

```python
# copy from facebook/ConvNeXt
import torch
import torch.nn as nn
import torch.nn.functional as F
from timm.models.layers import trunc_normal_, DropPath
from timm.models.registry import register_model

class Block(nn.Module):
    r''' ConvNeXt Block. There are two equivalent implementations:
    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)
    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back
    We use (2) as we find it slightly faster in PyTorch
    
    Args:
        dim (int): Number of input channels.
        drop_path (float): Stochastic depth rate. Default: 0.0
        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.
    '''
    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):
        super().__init__()
        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv
        self.norm = LayerNorm(dim, eps=1e-6)
        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers
        self.act = nn.GELU()
        self.pwconv2 = nn.Linear(4 * dim, dim)
        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), 
                                    requires_grad=True) if layer_scale_init_value > 0 else None
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()

    def forward(self, x):
        input = x
        x = self.dwconv(x)
        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)
        x = self.norm(x)
        x = self.pwconv1(x)
        x = self.act(x)
        x = self.pwconv2(x)
        if self.gamma is not None:
            x = self.gamma * x
        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)

        x = input + self.drop_path(x)
        return x

class ConvNeXt(nn.Module):
    r''' ConvNeXt
        A PyTorch impl of : `A ConvNet for the 2020s`  -
          https://arxiv.org/pdf/2201.03545.pdf

    Args:
        in_chans (int): Number of input image channels. Default: 3
        num_classes (int): Number of classes for classification head. Default: 1000
        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]
        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]
        drop_path_rate (float): Stochastic depth rate. Default: 0.
        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.
        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.
    '''
    def __init__(self, in_chans=3, num_classes=1000, 
                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0., 
                 layer_scale_init_value=1e-6, head_init_scale=1.,
                 ):
        super().__init__()

        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers
        stem = nn.Sequential(
            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),
            LayerNorm(dims[0], eps=1e-6, data_format='channels_first')
        )
        self.downsample_layers.append(stem)
        for i in range(3):
            downsample_layer = nn.Sequential(
                    LayerNorm(dims[i], eps=1e-6, data_format='channels_first'),
                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),
            )
            self.downsample_layers.append(downsample_layer)

        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks
        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] 
        cur = 0
        for i in range(4):
            stage = nn.Sequential(
                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], 
                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]
            )
            self.stages.append(stage)
            cur += depths[i]

        self.norm = nn.LayerNorm(dims[-1], eps=1e-6) # final norm layer
        self.head = nn.Linear(dims[-1], num_classes)

        self.apply(self._init_weights)
        self.head.weight.data.mul_(head_init_scale)
        self.head.bias.data.mul_(head_init_scale)

    def _init_weights(self, m):
        if isinstance(m, (nn.Conv2d, nn.Linear)):
            trunc_normal_(m.weight, std=.02)
            nn.init.constant_(m.bias, 0)

    def forward_features(self, x):
        for i in range(4):
            x = self.downsample_layers[i](x)
            x = self.stages[i](x)
        return self.norm(x.mean([-2, -1])) # global average pooling, (N, C, H, W) -> (N, C)

    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x)
        return x

class LayerNorm(nn.Module):
    r''' LayerNorm that supports two data formats: channels_last (default) or channels_first. 
    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with 
    shape (batch_size, height, width, channels) while channels_first corresponds to inputs 
    with shape (batch_size, channels, height, width).
    '''
    def __init__(self, normalized_shape, eps=1e-6, data_format='channels_last'):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.eps = eps
        self.data_format = data_format
        if self.data_format not in ['channels_last', 'channels_first']:
            raise NotImplementedError 
        self.normalized_shape = (normalized_shape, )
    
    def forward(self, x):
        if self.data_format == 'channels_last':
            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        elif self.data_format == 'channels_first':
            u = x.mean(1, keepdim=True)
            s = (x - u).pow(2).mean(1, keepdim=True)
            x = (x - u) / torch.sqrt(s + self.eps)
            x = self.weight[:, None, None] * x + self.bias[:, None, None]
            return x


model_urls = {
    'convnext_tiny_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth',
    'convnext_small_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth',
    'convnext_base_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth',
    'convnext_large_1k': 'https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth',
    'convnext_tiny_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth',
    'convnext_small_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth',
    'convnext_base_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth',
    'convnext_large_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth',
    'convnext_xlarge_22k': 'https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth',
}

@register_model
def convnext_tiny(pretrained=False,in_22k=False, **kwargs):
    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)
    if pretrained:
        url = model_urls['convnext_tiny_22k'] if in_22k else model_urls['convnext_tiny_1k']
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu', check_hash=True)
        model.load_state_dict(checkpoint['model'])
    return model

@register_model
def convnext_small(pretrained=False,in_22k=False, **kwargs):
    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], **kwargs)
    if pretrained:
        url = model_urls['convnext_small_22k'] if in_22k else model_urls['convnext_small_1k']
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')
        model.load_state_dict(checkpoint['model'])
    return model

@register_model
def convnext_base(pretrained=False, in_22k=False, **kwargs):
    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], **kwargs)
    if pretrained:
        url = model_urls['convnext_base_22k'] if in_22k else model_urls['convnext_base_1k']
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')
        model.load_state_dict(checkpoint['model'])
    return model

@register_model
def convnext_large(pretrained=False, in_22k=False, **kwargs):
    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], **kwargs)
    if pretrained:
        url = model_urls['convnext_large_22k'] if in_22k else model_urls['convnext_large_1k']
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')
        model.load_state_dict(checkpoint['model'])
    return model

@register_model
def convnext_xlarge(pretrained=False, in_22k=False, **kwargs):
    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048], **kwargs)
    if pretrained:
        assert in_22k, 'only ImageNet-22K pre-trained ConvNeXt-XL is available; please set in_22k=True'
        url = model_urls['convnext_xlarge_22k']
        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location='cpu')
        model.load_state_dict(checkpoint['model'])
    return model
```

</p>
</details> 


### 代码

训练代码大部分都是模板, 不过, 我发现我还没有一个属于自己的 trainer, 趁着这次训练模型的时候补充一个

使用别人的 trainer 难免会遇到 debug, 而代码不熟的情况, 自己写的 trainer 可以掌握各种细节

在整理了一些以前代码后, 总结出了覆盖许多训练模型情况的流程, 趁着这个时候测试一下现在ai编码的能力, 将流程发给 claude-sonnet 后, 输出了一版代码, 在我的一些小修小补(补充日志)后, 就可以跑起来了

<details><summary>trainer.py</summary>
<p>

```python
import gc
import json
import logging
import os
import shutil

import torch
from torch import optim
from torch.amp import GradScaler
from torch.nn.utils import clip_grad_norm_
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

example_config = {
    'model': 'model_name',
    'checkpoint_dir': './checkpoints',
    'tensorboard_dir': './tensorboard',
    'device': 'cuda',
    'enable_cudnn_benchmark': True,
    'enable_amp': False,
    'learning_rate': 1e-4,
    'betas': [0.9, 0.999],
    'eps': 1e-8,
    'enable_compile': False,
    'weight_decay': 0.05,
    'max_steps': 100000,
    'max_grad_norm': 1.0,
    'save_every': 10000,
    'gradient_accumulation_steps': 4
}


class Trainer:
    def __init__(self, config):
        self.config = config
        self.setup_logging()
        self.setup_device()
        self.setup_model()
        self.setup_training()
        
    def setup_logging(self):
        '''设置日志'''
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s %(levelname)s %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        self.writer = SummaryWriter(self.config['tensorboard_dir'])
        
    def setup_device(self):
        '''设置设备'''
        self.device = torch.device(self.config['device'])
        torch.backends.cudnn.benchmark = self.config.get('enable_cudnn_benchmark', True)
        if self.device.type == 'cuda':
            self.logger.info(f'Using device: {self.device} ({torch.cuda.get_device_name()})')
        else:
            self.logger.info(f'Using device: {self.device}')
            
    def setup_model(self):
        '''设置模型、损失函数等'''
        self.model = self.build_model().to(self.device)
        if self.config.get('enable_compile', False):
            self.model.compile()
        self.criterion = self.build_criterion()
        
        # 打印模型信息
        n_parameters = sum(p.numel() for p in self.model.parameters())
        self.logger.info(f'Number of parameters: {n_parameters:,}')
        
    def setup_training(self):
        '''设置训练相关组件'''
        # 优化器
        self.optimizer = self.build_optimizer()
        
        # 学习率调度器
        self.scheduler = self.build_scheduler()
        
        # 梯度缩放器(用于混合精度训练)
        self.scaler = GradScaler(
            enabled=self.config.get('enable_amp', False)
        )
        self.gradient_accumulation_steps = self.config.get('gradient_accumulation_steps', 1)
        
        # 加载检查点
        self.steps = 0
        self.best_metric = {}
        self.load_checkpoint()
        
    def build_model(self):
        '''构建模型(需要子类实现)'''
        raise NotImplementedError
        
    def build_criterion(self):
        '''构建损失函数(需要子类实现)'''
        raise NotImplementedError
        
    def build_optimizer(self):
        '''构建优化器'''
        # 区分需要和不需要weight decay的参数
        decay_params = []
        no_decay_params = []
        for name, param in self.model.named_parameters():
            if 'bias' in name or 'norm' in name:
                no_decay_params.append(param)
            else:
                decay_params.append(param)
                
        opt_params = [
            {'params': decay_params, 'weight_decay': self.config['weight_decay']},
            {'params': no_decay_params, 'weight_decay': 0.0}
        ]
        
        return optim.AdamW(
            opt_params,
            lr=self.config['learning_rate'],
            betas=self.config.get('betas', (0.9, 0.999)),
            eps=self.config.get('eps', 1e-8)
        )
        
    def build_scheduler(self):
        '''构建学习率调度器(需要子类实现)'''
        return NotImplementedError
        
    def build_dataloader(self):
        '''构建数据加载器(需要子类实现)'''
        raise NotImplementedError
        
    def train_step(self, batch):
        '''单步训练(需要子类实现)'''
        raise NotImplementedError
        
    def validate(self):
        '''验证(需要子类实现)'''
        raise NotImplementedError
        
    def save_checkpoint(self, is_best=False):
        '''保存检查点'''
        state = {
            'model': self.model.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'scheduler': self.scheduler.state_dict(),
            'scaler': self.scaler.state_dict(),
            'steps': self.steps,
            'best_metric': self.best_metric,
            'config': self.config
        }
        
        # 保存最新检查点
        torch.save(
            state,
            os.path.join(self.config['checkpoint_dir'], 'latest.pt')
        )
        
        # 保存最佳检查点
        if is_best:
            shutil.copy(
                os.path.join(self.config['checkpoint_dir'], 'latest.pt'),
                os.path.join(self.config['checkpoint_dir'], 'best.pt')
            )
            
    def load_checkpoint(self):
        '''加载检查点'''
        checkpoint_path = os.path.join(
            self.config['checkpoint_dir'],
            'latest.pt'
        )
        
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(
                checkpoint_path,
                map_location=self.device
            )
            
            self.model.load_state_dict(checkpoint['model'])
            self.optimizer.load_state_dict(checkpoint['optimizer'])
            self.scheduler.load_state_dict(checkpoint['scheduler'])
            self.scaler.load_state_dict(checkpoint['scaler'])
            self.steps = checkpoint['steps']
            self.best_metric = checkpoint['best_metric']
            
            self.logger.info(f'Loaded checkpoint from {checkpoint_path}')
            self.logger.info(f'Training will resume from step {self.steps}')
    
    @staticmethod
    def is_better_performance(baseline_dict, compare_dict):
        '''
        判断compare_dict中的指标是否全面超过baseline_dict
        
        Args:
            baseline_dict: 基准字典,格式为 {指标名: 值}
            compare_dict: 比较字典,格式为 {指标名: 值} 
        
        Returns:
            bool: 如果compare_dict中所有指标都严格大于baseline_dict则返回True,否则返回False
        '''
        if not baseline_dict:
            return True
        
        # 检查两个字典的键是否一致
        if set(baseline_dict.keys()) != set(compare_dict.keys()):
            return False
            
        # 检查每个指标是否都有提升
        for metric in baseline_dict:
            if compare_dict[metric] <= baseline_dict[metric]:
                return False
                
        return True
            
    def train(self):
        '''训练流程'''
        train_loader = self.build_dataloader()
        self.model.train()
        
        self.logger.info('Start training...')
        pbar = tqdm(total=self.config['max_steps'], initial=self.steps)
        
        while self.steps < self.config['max_steps']:
            for batch in train_loader:
                # 训练一步
                with torch.autocast(device_type=self.config['device'], enabled=self.config.get('enable_amp', False)):
                    loss = self.train_step(batch)
                self.scaler.scale(loss / self.gradient_accumulation_steps).backward()
                
                if (self.steps + 1) % self.gradient_accumulation_steps == 0:
                    # 梯度裁剪
                    if self.config.get('max_grad_norm', 0) > 0:
                        self.scaler.unscale_(self.optimizer)
                        clip_grad_norm_(
                            self.model.parameters(),
                            self.config['max_grad_norm']
                        )

                    # 优化器步进
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    self.optimizer.zero_grad(set_to_none=True)
                self.scheduler.step()
                
                # 记录
                self.writer.add_scalar('train/loss', loss, self.steps)
                self.writer.add_scalar(
                    'train/lr',
                    self.scheduler.get_last_lr()[0],
                    self.steps
                )
                
                self.steps += 1
                pbar.update(1)
                
                # 验证和保存
                if self.steps % self.config['save_every'] == 0:
                    metric = self.validate()
                    for i in metric:
                        self.logger.info(f'Validation {i}: {metric[i]}')
                        self.writer.add_scalar(f'val/{i}', metric[i], self.steps)
                    
                    is_best = self.is_better_performance(self.best_metric, metric)
                    if is_best:
                        self.best_metric = metric

                    self.model.train()
                    self.save_checkpoint(is_best)
                    
                if self.steps >= self.config['max_steps']:
                    break
                
            gc.collect()
            torch.cuda.empty_cache()
                    
        pbar.close()
        self.logger.info('Training finished!')


def main():
    '''主函数'''
    # 加载配置
    with open('config.json') as f:
        config = json.load(f)
        
    # 创建输出目录
    os.makedirs(config['checkpoint_dir'], exist_ok=True)
    os.makedirs(config['tensorboard_dir'], exist_ok=True)
    
    # 训练
    trainer = Trainer(config)
    trainer.train()
    
if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        pass
``` 

</p>
</details> 

`trainer.py` 简单整合了几个常用的训练手段, 比如混合精度训练, 梯度裁剪, 梯度累计, weight_decay(写死了), tensorboard的记录, 断点续训等操作, 需要注意的是, `trainer.py` 没有使用 epoch 作为训练进度, 而是用了更精细的 step (每次迭代参数即为一个step), 使用的时候需要自行实现模型构建, 损失函数构建 学习率调度器 数据集加载器, 单步训练, 验证的流程的子类实现

然后将一些配置放到config中便于读取, 其中有一些配置是必须的, 其他则是子类实现的时候需要的

听起来可能有点抽象, 下面是一个简单的trainer使用案例

<details><summary>trainer使用案例</summary>
<p>

import torchvision
import torch
from trainer import Trainer
from torchvision.models import resnet18
from torch.optim.lr_scheduler import LambdaLR



transform = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

class ConstantLambdaLR(LambdaLR):
    def __init__(self, optimizer, **kwargs):
        kwargs['optimizer'] = optimizer
        kwargs['lr_lambda'] = self._step_inner
        super().__init__(**kwargs)

    def _step_inner(self, steps):
        return 1


class Cifer10Trainer(Trainer):
    def __init__(self, config):
        super().__init__(config)

    def build_model(self):
        model = resnet18()
        model.fc = torch.nn.Linear(model.fc.in_features, 10)
        return model
    
    def build_criterion(self):
        return torch.nn.CrossEntropyLoss()
    
    def build_scheduler(self):
        return ConstantLambdaLR(self.optimizer)
    
    def build_dataloader(self):
        train_dataset = torchvision.datasets.CIFAR10(root='./temp', train=True, download=True, transform=transform)
        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.config['batch_size'], shuffle=True, num_workers=1)
        return train_loader
    
    def train_step(self, batch):
        inputs, labels = batch
        inputs, labels = inputs.to(self.device), labels.to(self.device)
        outputs = self.model(inputs)
        loss = self.criterion(outputs, labels)
        return loss
    
    def validate(self):
        self.model.eval()
        test_dataset = torchvision.datasets.CIFAR10(root='./temp', train=False, download=True, transform=transform)
        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=self.config['batch_size'], shuffle=False, num_workers=1)
        acc = []
        with torch.inference_mode():
            for batch in test_loader:
                inputs, labels = batch
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                y_hat = self.model(inputs)
                acc.append((y_hat.argmax(dim=1) == labels).sum().item() / labels.size(0))
                
        return {'acc': sum(acc) / len(acc)}
                

def main():
    config = {
        'model': 'resnet18',
        'checkpoint_dir': './checkpoints',
        'tensorboard_dir': './tensorboard',
        'device': 'cuda',
        'enable_cudnn_benchmark': True,
        'enable_amp': False,
        'learning_rate': 1e-3,
        'betas': [0.9, 0.999],
        'eps': 1e-8,
        'enable_compile': False,
        'weight_decay': 0.05,
        'max_steps': 500,
        'max_grad_norm': 1.0,
        'save_every': 100,
        'gradient_accumulation_steps': 1,
        'batch_size': 32
    }
    trainer = Cifer10Trainer(config)
    trainer.train()
    
if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        pass

</p>
</details> 

> 代码使用cifer10数据集, resnet18作为模型训练的简单的流程

有了流程接下来编写我们的训练代码

<details><summary>train.py(代码未整理完毕, 非初版代码, 仅供参考)</summary>
<p>

```python
import os
import random

import cv2
import numpy as np
from sklearn.metrics import f1_score
import torch
from PIL import Image
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.data import DataLoader, Dataset
from datasets import load_dataset
from torchvision import transforms

# from torchvision.models import resnet18
from model import convnext_base
from trainer import Trainer

image_size = 224
batch_size = 32
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')


# def get_color_from_image(image_path):
#     '''
#     从纯色图片中获取RGB颜色值
#     返回: (R, G, B)元组
#     '''
#     # 读取图片
#     image = Image.open(image_path).convert('RGB')
#     # 转换为numpy数组
#     img_array = np.array(image)
    
#     # 获取图片中心点的颜色值
#     h, w = img_array.shape[:2]
#     center_color = img_array[h//2, w//2]
    
#     # 或者计算整个图片的平均颜色
#     average_color = img_array.mean(axis=(0,1)).astype(int)
    
#     return tuple(average_color)  # 或者 tuple(average_color)


# class AugmentationUtils:
#     @staticmethod
#     def add_color_mask(image, is_positive):
#         '''给图片添加颜色遮罩'''
#         # 转换为numpy数组并确保类型为uint8
#         image = np.array(image, dtype=np.uint8)
        
#         # 创建与图像相同大小的遮罩
#         mask = np.ones_like(image, dtype=np.uint8)
        
#         # 随机生成颜色
#         if is_positive:
#             color = [random.randint(0, 255) for _ in range(3)]
#         else:
#             color = get_color_from_image('22.png')
        
#         # 为遮罩赋予颜色    
#         for i in range(3):
#             mask[:, :, i] = color[i]
        
#         # 确保mask也是uint8类型
#         mask = mask.astype(np.uint8)
        
#         # 添加遮罩
#         alpha = 0.5  # 透明度
#         image = cv2.addWeighted(image, 1-alpha, mask, alpha, 0)
        
#         return Image.fromarray(image)

#     @staticmethod
#     def embed_positive_in_negative(positive_img, negative_img):
#         '''在负样本中嵌入正样本'''
#         # 转换为numpy数组
#         pos_img = np.array(positive_img)
#         neg_img = np.array(negative_img)
        
#         # 确保图像是3通道的
#         if len(pos_img.shape) == 2:
#             pos_img = cv2.cvtColor(pos_img, cv2.COLOR_GRAY2BGR)
#         if len(neg_img.shape) == 2:
#             neg_img = cv2.cvtColor(neg_img, cv2.COLOR_GRAY2BGR)
        
#         # 获取负样本尺寸
#         h, w = neg_img.shape[:2]
#         pos_h, pos_w = pos_img.shape[:2]
        
#         # 计算合适的缩放比例
#         scale = min(
#             random.uniform(0.5, 0.8),
#             (w * 0.8) / pos_w,
#             (h * 0.8) / pos_h
#         )
        
#         # 缩放正样本
#         new_size = (int(pos_w * scale), int(pos_h * scale))
#         pos_img_resized = cv2.resize(pos_img, new_size)
        
#         # 确保有效的随机位置范围
#         max_x = max(0, w - new_size[0])
#         max_y = max(0, h - new_size[1])
        
#         # 随机选择插入位置
#         x = random.randint(0, max_x) if max_x > 0 else 0
#         y = random.randint(0, max_y) if max_y > 0 else 0
        
#         # 获取ROI区域并确保与缩放后的正样本具有相同的形状
#         roi = neg_img[y:y+new_size[1], x:x+new_size[0]]
        
#         # 确保ROI和pos_img_resized具有相同的形状和通道数
#         if roi.shape == pos_img_resized.shape:
#             # 混合图像
#             blended = cv2.addWeighted(roi, 0.3, pos_img_resized, 0.7, 0)
#             neg_img[y:y+new_size[1], x:x+new_size[0]] = blended
        
#         return Image.fromarray(neg_img)
    
#     @staticmethod
#     def embed_same(positive_img, negative_img):
#         '''在负样本中嵌入正样本'''
#         # 转换为numpy数组
#         pos_img = np.array(positive_img)
#         neg_img = np.array(negative_img)
        
#         # 确保图像是3通道的
#         if len(pos_img.shape) == 2:
#             pos_img = cv2.cvtColor(pos_img, cv2.COLOR_GRAY2BGR)
#         if len(neg_img.shape) == 2:
#             neg_img = cv2.cvtColor(neg_img, cv2.COLOR_GRAY2BGR)
        
#         # 获取负样本尺寸
#         h, w = neg_img.shape[:2]
#         pos_h, pos_w = pos_img.shape[:2]
        
#         # 计算合适的缩放比例
#         scale = min(
#             random.uniform(0.5, 0.8),
#             (w * 0.8) / pos_w,
#             (h * 0.8) / pos_h
#         )
        
#         # 缩放正样本
#         new_size = (int(pos_w * scale), int(pos_h * scale))
#         pos_img_resized = cv2.resize(pos_img, new_size)
        
#         # 确保有效的随机位置范围
#         max_x = max(0, w - new_size[0])
#         max_y = max(0, h - new_size[1])
        
#         # 随机选择插入位置
#         x = random.randint(0, max_x) if max_x > 0 else 0
#         y = random.randint(0, max_y) if max_y > 0 else 0
        
#         # 获取ROI区域并确保与缩放后的正样本具有相同的形状
#         roi = neg_img[y:y+new_size[1], x:x+new_size[0]]
        
#         # 确保ROI和pos_img_resized具有相同的形状和通道数
#         if roi.shape == pos_img_resized.shape:
#             # 混合图像
#             blended = cv2.addWeighted(roi, 0.3, pos_img_resized, 0.7, 0)
#             neg_img[y:y+new_size[1], x:x+new_size[0]] = blended
        
#         return Image.fromarray(neg_img)

#     @staticmethod
#     def flip_image(image):
#         '''图片轴对称'''
#         return Image.fromarray(np.array(image)[:, ::-1])
    
#     @staticmethod
#     def mirror_half_image(image):
#         img_array = np.array(image)
    
#         # 获取图片尺寸
#         h, w = img_array.shape[:2]
        
#         # 取左半边
#         half_w = w // 2
#         left_half = img_array[:, :half_w]
        
#         # 水平翻转左半边得到右半边
#         right_half = left_half[:, ::-1]
        
#         # 拼接两个半边
#         mirrored = np.concatenate([left_half, right_half], axis=1)
        
#         return Image.fromarray(mirrored)
    

# def augment_dataset(positive_images, negative_images):
#     aug_utils = AugmentationUtils()
#     augmented_data = []
    
#     # 增强正样本
#     for pos_img in positive_images:
#         img = Image.open(pos_img).convert('RGB')
#         # 原图
#         augmented_data.append((img, 1))
#         # 颜色遮罩
#         augmented_data.append((aug_utils.add_color_mask(img, True), 1))
#         # 轴对称
#         augmented_data.append((aug_utils.flip_image(img), 1))
#         # 镜像一半
#         augmented_data.append((aug_utils.mirror_half_image(img), 1))
#         # 嵌入相同
#         img_id = random.randint(0, len(positive_images)-1)
#         aaa = Image.open(positive_images[img_id]).convert('RGB')
#         augmented_data.append((aug_utils.embed_same(aaa, img), 1))
        
    
#     # 增强负样本
#     for i, neg_img in enumerate(negative_images):
#         img = Image.open(neg_img).convert('RGB')
#         # 原图
#         augmented_data.append((img, 0))
#         # 颜色遮罩
#         augmented_data.append((aug_utils.add_color_mask(img, False), 0))
#         # 镜像一半
#         augmented_data.append((aug_utils.mirror_half_image(img), 0))
#         # 嵌入正样本
#         pos_img = Image.open(positive_images[random.randint(0, len(positive_images)-1)]).convert('RGB')
#         augmented_data.append((aug_utils.embed_positive_in_negative(pos_img, img), 1))
#         # 嵌入相同
#         img_id = random.randint(0, len(negative_images)-1)
#         aaa = Image.open(negative_images[img_id]).convert('RGB')
#         augmented_data.append((aug_utils.embed_same(aaa, img), 0))
        

        
#     # # 显示并保存
#     # for i, (img, label) in enumerate(augmented_data):
#     #     # img.show()
#     #     os.makedirs('aug_images', exist_ok=True)
#     #     img.save(f'aug_images/aug_{i}.jpg')
    
#     # 统计
#     print(f'Positive: {len([x for x, y in augmented_data if y == 1])}, Negative: {len([x for x, y in augmented_data if y == 0])}')
#     return augmented_data


class LinearWarmUpCosineAnnealingLR(LambdaLR):
    def __init__(self, optimizer, *, warmup_iters, max_learning_rate, min_lr, lr_decay_iters, **kwargs):
        self.warmup_iters = warmup_iters
        self.max_learning_rate = max_learning_rate
        self.lr_decay_iters = lr_decay_iters
        self.min_lr = min_lr
        kwargs['optimizer'] = optimizer
        kwargs['lr_lambda'] = self._step_inner
        super().__init__(**kwargs)

    def _step_inner(self, steps):
        if steps < self.warmup_iters:
            return self.max_learning_rate * steps / self.warmup_iters
        elif steps < self.lr_decay_iters:
            return self.min_lr + 0.5 * (1.0 + np.cos((steps - self.warmup_iters) / (self.lr_decay_iters - self.warmup_iters)*np.pi)) * (self.max_learning_rate - self.min_lr)
        else:
            return self.min_lr


def transform_img(img):
    # 处理图片
    img_np = np.array(img)
    img_tensor = torch.from_numpy(img_np).permute(2, 0, 1)  # C, H, W
    img_tensor = torch.nn.functional.interpolate(img_tensor.unsqueeze(0), size=(image_size, image_size), mode='bilinear', align_corners=False).squeeze(0)
    # normalize
    normalized_img = img_tensor.float() / 255.0
    return normalized_img


transform = transforms.Compose([
    transforms.Resize((image_size, image_size)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


def transform_img_torchvision(data):
    data['x'] = [transform(img.convert('RGB')) for img in data['image']]
    return data


label_mapping = {
    'nailong': 0,
    'emoji': 1,
    'anime': 2,
    'others': 3,
    'long': 4
}

def extract_datasets():
    ds = load_dataset('refoundd/NailongClassification', cache_dir='data', split='train')
    ds = ds.map(lambda x: {'label': label_mapping[x['label']]})
    ds = ds.map(transform_img_torchvision, remove_columns=['image'], batched=True)
    dataset = ds.train_test_split(test_size=0.2)
    return dataset

dataset = extract_datasets()


class NaiLongDataset(Dataset):
    def __init__(self, mode='train'):
        assert mode in ['train', 'test']
        self.dataset = dataset[mode]

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]['x']
        label = self.dataset[idx]['label']
        return torch.tensor(item), torch.tensor(label)



class NaiLongTrainer(Trainer):
    def __init__(self, config):
        super().__init__(config)

    def build_model(self):
        # model = resnet18()
        # model.fc = torch.nn.Linear(model.fc.in_features, 2)
        # return model
        return convnext_base(pretrained=False, num_classes=5)
    
    def build_criterion(self):
        return torch.nn.CrossEntropyLoss()
    
    def build_scheduler(self):
        return LinearWarmUpCosineAnnealingLR(self.optimizer, warmup_iters=self.config['warmup_iters'], max_learning_rate=self.config['max_learning_rate'], min_lr=self.config['min_lr'], lr_decay_iters=self.config['lr_decay_iters'])
    
    def build_dataloader(self, mode='train'):
        dataset = NaiLongDataset(mode='train')
        return DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    def train_step(self, batch):
        x, y = batch
        x, y = x.to(device), y.to(device)
        return self.criterion(self.model(x), y)
    
    def validate(self):
        self.logger.info('Validating...')
        self.model.eval()
        dataloader = self.build_dataloader(mode='test')
        acc = []
        f1 = [[], []]
        with torch.no_grad(): 
            for i, (x, y) in enumerate(dataloader):
                x, y = x.to(device), y.to(device)
                # print(f'Validation: {i}, {y}')
                y_hat = self.model(x)
                acc.append(torch.sum(torch.argmax(y_hat, dim=1) == y).item() / len(y))
                f1[0].extend(y.cpu().tolist())
                f1[1].extend(torch.argmax(y_hat, dim=1).cpu().tolist())
            f1_scores = f1_score(f1[0], f1[1], average='macro')
        return {'acc': sum(acc) / len(acc), 'f1': f1_scores}


def main():
    config = {  # test
        'model': 'convnext_tiny',
        'checkpoint_dir': './checkpoints',
        'tensorboard_dir': './tensorboard',
        'device': 'cuda',
        'enable_cudnn_benchmark': True,
        'enable_amp': False,
        'learning_rate': 1,  # 启动lr_scheduler 这里必须是1
        'betas': [0.9, 0.999],
        'eps': 1e-8,
        'enable_compile': False,
        'weight_decay': 0.0,
        'max_steps': 5000,
        'max_grad_norm': 1.0,
        'save_every': 500,
        'gradient_accumulation_steps': 1,
        'warmup_iters': 500,
        'max_learning_rate': 1e-3,
        'min_lr': 1e-4,
        'lr_decay_iters': 1000
    }
    os.makedirs(config['checkpoint_dir'], exist_ok=True)
    os.makedirs(config['tensorboard_dir'], exist_ok=True)
    trainer = NaiLongTrainer(config)
    trainer.train()

if __name__ == '__main__':
    # 删除tensorboard下的文件, 但不删除文件夹
    for i in os.listdir('./tensorboard'):
        os.remove(os.path.join('./tensorboard', i))
    # 删除checkpoints下的文件
    for i in os.listdir('./checkpoints'):
        os.remove(os.path.join('./checkpoints', i))
    try:
        main()
    except KeyboardInterrupt:
        print('KeyboardInterrupt')
        
```

</p>
</details> 

### 数据增广

原始数据集只有两百多张图片, 这个时候无法避免的要做数据增广, 扩展 nailong 标签的数据, 这里因为是初版方案, 也没有非常精细的增广方案, 这里使用了以下几种方式(代码在如上train.py中):

- 给图片添加颜色遮罩
    让模型不要将遇到黄色的就判定为奶龙
- 在负样本中嵌入正样本
    很经典的增广数据的手法
- 图片轴对称
- 取图像的一半镜像翻转

### 训练

#### 参数搜索

虽然是个人小项目, 简单的参数搜索不能少, 继续上面写的 `trainer.py`, 我也写了一个简单的 `hyperparameter_seacher.py` 来搜索超参

<details><summary>hyperparameter_seacher.py</summary>
<p>

```python
from trainer import Trainer
import optuna


example_config = {
    'model': 'convnext_tiny',
    'checkpoint_dir': './checkpoints',
    'tensorboard_dir': './tensorboard',
    'device': 'cuda',
    'enable_cudnn_benchmark': True,
    'enable_amp': False,
    'learning_rate': 1e-4,
    'betas': [0.9, 0.999],
    'eps': 1e-8,
    'enable_compile': False,
    'weight_decay': 0.05,
    'max_steps': 100,
    'max_grad_norm': 1.0,
    'save_every': 1000000,  # 不保存
    'gradient_accumulation_steps': 4
}

example_search_config = {
    'params': {
        'learning_rate': {
            'type': 'float',
            'range': [1e-5, 1e-2],
            'log': True
        },
        'gradient_accumulation_steps': {
            'type': 'int',
            'range': [1, 8],
            'log': False
        }
    },
    'if_save_info': False,
    'n_trials': 10
}

class HyperparameterSearcher:
    def __init__(self, config, trainer):
        assert isinstance(trainer, Trainer), 'trainer must be an instance of Trainer'
        self.config = config
        self.trainer = trainer
        
    def objective(self, trial):
        search_params = self.config['params']
        
        for param_name, param_config in search_params.items():
            if param_config['type'] == 'float':
                self.trainer.config[param_name] = trial.suggest_float(
                    param_name,
                    param_config['range'][0],
                    param_config['range'][1],
                    log=param_config.get('log', False)
                )
            elif param_config['type'] == 'int':
                self.trainer.config[param_name] = trial.suggest_int(
                    param_name,
                    param_config['range'][0],
                    param_config['range'][1]
                )
            elif param_config['type'] == 'list':
                self.trainer.config[param_name] = trial.suggest_categorical(
                    param_name,
                    param_config['range']
                )
            else:
                raise ValueError(f'Unsupported parameter type: {param_config['type']}, only support float and int')
        
        self.trainer.setup_training()
        self.trainer.train()
        metric = self.trainer.validate()
        if 'acc' not in metric:
            raise ValueError('metric must contain 'acc'')
        return -metric['acc']  # only support maximizing acc
    
    def search(self):
        study = optuna.create_study(direction='maximize')
        study.optimize(self.objective, n_trials=self.config['n_trials'])
        print('Best params:', study.best_params)
        print('Best value:', -study.best_value)
        if self.config['if_save_info']:
            study.trials_dataframe().to_csv('./output/optuna_results.csv')
        return study.best_params
    
def main():
    
    pass

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        pass
```

</p>
</details> 

超参搜索需要 trainer 的配合, 使用了与模型无关的 optuna 来跑给定范围的超参值, 这个时候可以给trainer一个比较容易训练的超参设置(短的epoch等), 同时关闭保存模式

我也写了个简单的超参搜索的例子

<details><summary>hyperparameter_seacher使用案例</summary>
<p>

from example_trainer import Cifer10Trainer
from hyperparameter_seacher import HyperparameterSearcher

class Cifer10HyperparameterSearcher(HyperparameterSearcher):
    def __init__(self, config, trainer):
        super().__init__(config, trainer)


def main():
    search_config = {
        'params': {
            'learning_rate': {
                'type': 'float',
                'range': [1e-5, 1e-2],
                'log': True
            },
            'gradient_accumulation_steps': {
                'type': 'int',
                'range': [1, 8],
                'log': False
            }
        },
        'if_save_info': True,
        'n_trials': 10
    }

    trainer_config = {
        'model': 'resnet18',
        'checkpoint_dir': './checkpoints',
        'tensorboard_dir': './tensorboard',
        'device': 'cuda',
        'enable_cudnn_benchmark': True,
        'enable_amp': False,
        'learning_rate': 1e-3,
        'betas': [0.9, 0.999],
        'eps': 1e-8,
        'enable_compile': False,
        'weight_decay': 0.05,
        'max_steps': 500,
        'max_grad_norm': 1.0,
        'save_every': 10000,  # large than max_steps, no save
        'gradient_accumulation_steps': 4,
        'batch_size': 32
    }
    trainer = Cifer10Trainer(trainer_config)
    searcher = Cifer10HyperparameterSearcher(search_config, trainer)
    best_params = searcher.search()
    print(best_params)

if __name__ == '__main__':
    main()

</p>
</details> 

> 搜索上面那个例子中的合适的超参数

在准备好这些后, 编写我们项目的超参搜索器

<details><summary>之后补充</summary>
<p>



</p>
</details> 

> 搜索出来的最佳超参是一个很长的小数, 四舍五入合适的位数即可

#### 第一次训练

在准备好后, 开始第一次训练

在较新的GPU下, 训练以前较小的模型可谓降维打击, 不到一个小时训练完毕

然而, 第一个问题出来了

acc很高, f1很低

编写测试代码:

<details><summary>test.py(非初版代码, 仅供参考)</summary>
<p>

```python
from sklearn.metrics import f1_score
import torch

from model import convnext_base
from PIL import Image
import numpy as np
from glob import glob
from torchvision import transforms

device = 'cuda'
image_size = 224

model = convnext_base(pretrained=False, num_classes=5).to(device)
# model = resnet18()
# model.fc = torch.nn.Linear(model.fc.in_features, 2)
checkpoint = torch.load('./checkpoints/best.pt', map_location=device)
model.load_state_dict(checkpoint['model'])

transform = transforms.Compose([
    transforms.Resize((image_size, image_size)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

def transform_img(img):
    # 处理图片
    img_np = np.array(img)
    img_tensor = torch.from_numpy(img_np).permute(2, 0, 1)  # C, H, W
    img_tensor = torch.nn.functional.interpolate(img_tensor.unsqueeze(0), size=(image_size, image_size), mode='bilinear', align_corners=False).squeeze(0)
    # normalize
    normalized_img = img_tensor.float() / 255.0
    return normalized_img


def get_input_images(image_path):
    img = Image.open(image_path).convert('RGB')
    img = transform(img)
    return torch.tensor(img).to(device).unsqueeze(0)

model.eval()

# 导出onnx
input_names = ['input']
output_names = ['output']
dynamic_axes = {
    'input': {0: 'batch_size'},  # 输入的第一个维度是动态的
    'output': {0: 'batch_size'}  # 输出的第一个维度是动态的
}
torch.onnx.export(model, torch.randn(1, 3, 224, 224).to(device), 'model.onnx', input_names=input_names, output_names=output_names, dynamic_axes=dynamic_axes, opset_version=11)

with torch.no_grad():
    # image = torch.randn(1, 3, 256, 256)
    print(torch.softmax(model(get_input_images('1.jpg')), dim=1))
    input()
    print(torch.softmax(model(get_input_images('3.jpg')), dim=1))
    input()
    acc = []
    f1 = [[], []]
    
    
    for file in glob('./datasets/nailong/*'):
        y_hat, y = model(get_input_images(file)), torch.tensor([0]).to(device)
        acc.append(torch.sum(torch.argmax(y_hat, dim=1) == y).item() / len(y))
        f1[0].append(y.cpu().tolist()[0])
        f1[1].append(torch.argmax(y_hat, dim=1).cpu().tolist()[0])
        

    # for file in glob('./datasets/cifer10/*'):
    #     y_hat, y = model(get_input_images(file)), torch.tensor([3]).to(device)
    #     acc.append(torch.sum(torch.argmax(y_hat, dim=1) == y).item() / len(y))
    #     f1[0].append(y.cpu().tolist()[0])
    #     f1[1].append(torch.argmax(y_hat, dim=1).cpu().tolist()[0])

print(sum(acc) / len(acc))
f1_scores = f1_score(f1[0], f1[1], average='macro')
print(f1_scores)
```

</p>
</details> 


发现 acc 与 f1 的值非常低(百分之10到百分之20附近)

这个时候查看模型的输出, 发现模型的输出接近初始化的输出(softmax后), 模型没有怎么被训练

这个时候怀疑是训练代码出了问题, 然而 cifer10 的 训练并没有什么问题
检查数据增广代码, 查看增广后的图片, 发现增广做的不是很好, 正样本内嵌负样本没嵌好

在经过修改后, 重启训练
然而, 问题变成了, 模型的输出接近(0.5, 0.5)(二分类任务)

跟人讨论后, 认为是数据集难度太大, 检查表情包数据集, 都是一些分布与 nailong 数据差异很大的图片. 

> 非严格推理, 纯脑测
模型发现, 给一张新的图片预测 nailong 类, 还是其他类, 都会导致loss上升, 于是干脆摆烂乱猜,  最终的概率分布会输出数据分布, 经过数据增广后的数据恰好是两类 1:1, 模型退化成统计数据集了

导致这个的最直接原因是输入特征不够, 到图像分类就是模型找不到决定图片分类的模式

于是, 第一阶段的训练结束了

#### 第二次训练

在数据集作者不断的努力下, nailong 数据集有了一些完善, 主要的完善点在于: 
1. 新添更多 nailong
2. 不是二分类了, 新增了表情包分类, 动画分类等五分类, 不过不同类别的数据数量差异很大(两个数量级)
3. 加入了一些 corner case, 比如 藤田琴音等其他颜色为黄色的图像

因为第一次训练代码已经写好了, 改起来也不是很麻烦, 只需要换个数据集定义与读取. 作者的数据集放在 huggingface 上, 于是我们使用 datasets 进行读取.

> 我也不知道是不是我写的问题, datasets读起来很慢, dataloader 后, 会把 label 自动变成torch.tensor格式, 但是 n, c, h, w 格式的图片只会把 w 维度变成 torch.tensor 格式, 其他维度还是 List, 需要在 dataset 类定义的时候使用 __getitem__() 将数据提前变为 torch.tensor
> 然后不支持多线程读取(会卡住), 单线程读取读起来很慢, gpu 的 cuda 呈现尖刺状
> 然后, dataset 的读取**要先**读取 id 再读取 x 跟 label
> 没怎么用过 dataset, 这次属实是学到了

修改好后数据加载的代码后并注释掉先前的数据增广代码后(后续研究), 第二次训练开始了

这次结果好过头了
模型的 loss 收敛到了 $1e^{-5}$, acc跟f1更是到达了 $100%$

使用测试代码简单测试, 发现在数据集的数据都能完美分类, 不在数据集的分类只要分不出是奶龙即可. 检查模型输出权重, 也没啥问题, 看起来是完美了?

然而 这张图还是给了模型一拳

<details><summary>图</summary>
<p>

![22](https://github.com/user-attachments/assets/3cb2b111-e01f-44dd-8e71-0509ab2bb6c0)

</p>
</details> 


他会识别成 nailong, 不过我觉得问题不大(确实有人把他抽象的认成 nailong)

上面的 `test.py` 中 写了onnx导出的代码, 支持任意 batch 的输入(解锁了 n, c, h, w 的 n 维度)

简单编写onnx推理代码

<details><summary>onnx_inference.py</summary>
<p>

```python
# from torchvision import transforms
import onnxruntime as ort
from PIL import Image
import numpy as np

img_size = 224

# transform = transforms.Compose([
#     transforms.Resize((img_size, img_size)),
#     transforms.ToTensor(),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
# ])

def transform_img(img: Image, image_size=224, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):
    img = img.convert('RGB').resize((image_size, image_size), Image.Resampling.LANCZOS)
    img = np.array(img)
    img = (img / 255 - mean) / std
    img = img.transpose((2, 0, 1))
    img = np.expand_dims(img, axis=0)
    return img.astype(np.float32)


label_mapping = {
    'nailong': 0,
    'emoji': 1,
    'anime': 2,
    'others': 3,
    'long': 4
}

reverse_label_mapping = {v: k for k, v in label_mapping.items()}

model_path = 'model.onnx'
session = ort.InferenceSession(model_path)

image_path = '3.jpg'
image = Image.open(image_path).convert('RGB')
# image = transform(image).unsqueeze(0).numpy()
image = transform_img(image)

# 运行推理
input_name = session.get_inputs()[0].name
output_name = session.get_outputs()[0].name
outputs = session.run([output_name], {input_name: image})

# 获取分类结果
output = outputs[0]
predicted_class = np.argmax(output, axis=1)
predicted_label = reverse_label_mapping[predicted_class[0]]

print(f'Predicted class: {predicted_label}')

```

</p>
</details> 

> 训练的时候引入了 torchvision 的 transforms, 这里为了减少依赖, 选择手动实现, 有需要也可以自行取消注释并修改

### 部署

TODO
。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://FairyOwO.github.io/post/nailong-shu-ju-ji-%2C%20-jian-ce-nailong-de-mo-xing-%2C%20-xun-lian-yu-tui-li-%20%28-yi-%29.html">
<meta property="og:image" content="https://github.githubassets.com/favicons/favicon.svg">
<title>nailong数据集, 检测nailong的模型, 训练与推理 (一)</title>
<link href="//unpkg.com/@wooorm/starry-night@2.1.1/style/both.css" rel="stylesheet" />


</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}
.copy-feedback {
    display: none;
    position: absolute;
    top: 10px;
    right: 50px;
    color: var(--color-fg-on-emphasis);
    background-color: var(--color-fg-muted);
    border-radius: 3px;
    padding: 5px 8px;
    font-size: 12px;
}
</style>




<body>
    <div id="header">
<h1 class="postTitle">nailong数据集, 检测nailong的模型, 训练与推理 (一)</h1>
<div class="title-right">
    <a href="https://FairyOwO.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/FairyOwO/FairyOwO.github.io/issues/4" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><blockquote>
<p>回归 老本行</p>
</blockquote>
<p>偶然得到 nailong 数据集, 分为两块, 一种是给<a href="https://huggingface.co/datasets/refoundd/NailongClassification" rel="nofollow">分类模型使用的数据集</a>, 另一种是给<a href="https://huggingface.co/datasets/refoundd/NailongDetection" rel="nofollow">目标检测模型使用的数据集</a></p>
<blockquote>
<p>后者的数据量不是非常多(6张), 等到有足够多的数据或者我一时兴起手动标注在进行研究</p>
</blockquote>
<h2>初版方案</h2>
<h3>数据集选择</h3>
<p>在我刚接触这个数据集的时候, 数据集是只有奶龙的(无其他标签的数据), 这个时候第一个想法就是引入其他分类, 这里采用cifer10数据集的数据, 对数据集进行增广</p>
<p>然而, cifer10 的数据分布毕竟与常见群聊内发送的图片不同, 我觉得会影响最终能力, 应该有选择性而不是随意添加其他类型的图片, 在一番搜索之后, 选中了 <a href="https://github.com/LLM-Red-Team/emo-visual-data">表情包数据集</a></p>
<p>虽然这个数据集的原计划是用来检测 VLLM 的能力, 但我认为在我们这个任务中也可以使用</p>
<h3>模型</h3>
<p>在敲定数据集之后, 就开始挑选模型了, 因为是个人小项目, 这里采用我个人喜好的模型选择, 使用了 <a href="https://github.com/facebookresearch/ConvNeXt">convnext 系模型</a></p>
<p>这个模型的论文是一篇非常经典的实验文, 里面大量探索了一些技巧对模型能力的影响 (各类消融实验), 虽然他是 2020 年推出, 但他对现在的卷积网络的训练技巧的指引很大</p>
<p>具体细节可以搜索相关的模型解析, 这里不再赘述</p>
<details><summary>model.py</summary>
<p>
</p><div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-c"># copy from facebook/ConvNeXt</span>
<span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">import</span> <span class="pl-s1">torch</span>.<span class="pl-s1">nn</span> <span class="pl-k">as</span> <span class="pl-s1">nn</span>
<span class="pl-k">import</span> <span class="pl-s1">torch</span>.<span class="pl-s1">nn</span>.<span class="pl-s1">functional</span> <span class="pl-k">as</span> <span class="pl-v">F</span>
<span class="pl-k">from</span> <span class="pl-s1">timm</span>.<span class="pl-s1">models</span>.<span class="pl-s1">layers</span> <span class="pl-k">import</span> <span class="pl-s1">trunc_normal_</span>, <span class="pl-v">DropPath</span>
<span class="pl-k">from</span> <span class="pl-s1">timm</span>.<span class="pl-s1">models</span>.<span class="pl-s1">registry</span> <span class="pl-k">import</span> <span class="pl-s1">register_model</span>

<span class="pl-k">class</span> <span class="pl-v">Block</span>(<span class="pl-s1">nn</span>.<span class="pl-v">Module</span>):
    <span class="pl-s">r""" ConvNeXt Block. There are two equivalent implementations:</span>
<span class="pl-s">    (1) DwConv -&gt; LayerNorm (channels_first) -&gt; 1x1 Conv -&gt; GELU -&gt; 1x1 Conv; all in (N, C, H, W)</span>
<span class="pl-s">    (2) DwConv -&gt; Permute to (N, H, W, C); LayerNorm (channels_last) -&gt; Linear -&gt; GELU -&gt; Linear; Permute back</span>
<span class="pl-s">    We use (2) as we find it slightly faster in PyTorch</span>
<span class="pl-s">    </span>
<span class="pl-s">    Args:</span>
<span class="pl-s">        dim (int): Number of input channels.</span>
<span class="pl-s">        drop_path (float): Stochastic depth rate. Default: 0.0</span>
<span class="pl-s">        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.</span>
<span class="pl-s">    """</span>
    <span class="pl-k">def</span> <span class="pl-en">__init__</span>(<span class="pl-s1">self</span>, <span class="pl-s1">dim</span>, <span class="pl-s1">drop_path</span><span class="pl-c1">=</span><span class="pl-c1">0.</span>, <span class="pl-s1">layer_scale_init_value</span><span class="pl-c1">=</span><span class="pl-c1">1e-6</span>):
        <span class="pl-en">super</span>().<span class="pl-en">__init__</span>()
        <span class="pl-s1">self</span>.<span class="pl-s1">dwconv</span> <span class="pl-c1">=</span> <span class="pl-s1">nn</span>.<span class="pl-v">Conv2d</span>(<span class="pl-s1">dim</span>, <span class="pl-s1">dim</span>, <span class="pl-s1">kernel_size</span><span class="pl-c1">=</span><span class="pl-c1">7</span>, <span class="pl-s1">padding</span><span class="pl-c1">=</span><span class="pl-c1">3</span>, <span class="pl-s1">groups</span><span class="pl-c1">=</span><span class="pl-s1">dim</span>) <span class="pl-c"># depthwise conv</span>
        <span class="pl-s1">self</span>.<span class="pl-s1">norm</span> <span class="pl-c1">=</span> <span class="pl-v">LayerNorm</span>(<span class="pl-s1">dim</span>, <span class="pl-s1">eps</span><span class="pl-c1">=</span><span class="pl-c1">1e-6</span>)
        <span class="pl-s1">self</span>.<span class="pl-s1">pwconv1</span> <span class="pl-c1">=</span> <span class="pl-s1">nn</span>.<span class="pl-v">Linear</span>(<span class="pl-s1">dim</span>, <span class="pl-c1">4</span> <span class="pl-c1">*</span> <span class="pl-s1">dim</span>) <span class="pl-c"># pointwise/1x1 convs, implemented with linear layers</span>
        <span class="pl-s1">self</span>.<span class="pl-s1">act</span> <span class="pl-c1">=</span> <span class="pl-s1">nn</span>.<span class="pl-v">GELU</span>()
        <span class="pl-s1">self</span>.<span class="pl-s1">pwconv2</span> <span class="pl-c1">=</span> <span class="pl-s1">nn</span>.<span class="pl-v">Linear</span>(<span class="pl-c1">4</span> <span class="pl-c1">*</span> <span class="pl-s1">dim</span>, <span class="pl-s1">dim</span>)
        <span class="pl-s1">self</span>.<span class="pl-s1">gamma</span> <span class="pl-c1">=</span> <span class="pl-s1">nn</span>.<span class="pl-v">Parameter</span>(<span class="pl-s1">layer_scale_init_value</span> <span class="pl-c1">*</span> <span class="pl-s1">torch</span>.<span class="pl-en">ones</span>((<span class="pl-s1">dim</span>)), 
                                    <span class="pl-s1">requires_grad</span><span class="pl-c1">=</span><span class="pl-c1">True</span>) <span class="pl-k">if</span> <span class="pl-s1">layer_scale_init_value</span> <span class="pl-c1">&gt;</span> <span class="pl-c1">0</span> <span class="pl-k">else</span> <span class="pl-c1">None</span>
        <span class="pl-s1">self</span>.<span class="pl-s1">drop_path</span> <span class="pl-c1">=</span> <span class="pl-v">DropPath</span>(<span class="pl-s1">drop_path</span>) <span class="pl-k">if</span> <span class="pl-s1">drop_path</span> <span class="pl-c1">&gt;</span> <span class="pl-c1">0.</span> <span class="pl-k">else</span> <span class="pl-s1">nn</span>.<span class="pl-v">Identity</span>()

    <span class="pl-k">def</span> <span class="pl-en">forward</span>(<span class="pl-s1">self</span>, <span class="pl-s1">x</span>):
        <span class="pl-s1">input</span> <span class="pl-c1">=</span> <span class="pl-s1">x</span>
        <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-en">dwconv</span>(<span class="pl-s1">x</span>)
        <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">x</span>.<span class="pl-en">permute</span>(<span class="pl-c1">0</span>, <span class="pl-c1">2</span>, <span class="pl-c1">3</span>, <span class="pl-c1">1</span>) <span class="pl-c"># (N, C, H, W) -&gt; (N, H, W, C)</span>
        <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-en">norm</span>(<span class="pl-s1">x</span>)
        <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-en">pwconv1</span>(<span class="pl-s1">x</span>)
        <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-en">act</span>(<span class="pl-s1">x</span>)
        <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-en">pwconv2</span>(<span class="pl-s1">x</span>)
        <span class="pl-k">if</span> <span class="pl-s1">self</span>.<span class="pl-s1">gamma</span> <span class="pl-c1">is</span> <span class="pl-c1">not</span> <span class="pl-c1">None</span>:
            <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-s1">gamma</span> <span class="pl-c1">*</span> <span class="pl-s1">x</span>
        <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">x</span>.<span class="pl-en">permute</span>(<span class="pl-c1">0</span>, <span class="pl-c1">3</span>, <span class="pl-c1">1</span>, <span class="pl-c1">2</span>) <span class="pl-c"># (N, H, W, C) -&gt; (N, C, H, W)</span>

        <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">input</span> <span class="pl-c1">+</span> <span class="pl-s1">self</span>.<span class="pl-en">drop_path</span>(<span class="pl-s1">x</span>)
        <span class="pl-k">return</span> <span class="pl-s1">x</span>

<span class="pl-k">class</span> <span class="pl-v">ConvNeXt</span>(<span class="pl-s1">nn</span>.<span class="pl-v">Module</span>):
    <span class="pl-s">r""" ConvNeXt</span>
<span class="pl-s">        A PyTorch impl of : `A ConvNet for the 2020s`  -</span>
<span class="pl-s">          https://arxiv.org/pdf/2201.03545.pdf</span>
<span class="pl-s"></span>
<span class="pl-s">    Args:</span>
<span class="pl-s">        in_chans (int): Number of input image channels. Default: 3</span>
<span class="pl-s">        num_classes (int): Number of classes for classification head. Default: 1000</span>
<span class="pl-s">        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]</span>
<span class="pl-s">        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]</span>
<span class="pl-s">        drop_path_rate (float): Stochastic depth rate. Default: 0.</span>
<span class="pl-s">        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.</span>
<span class="pl-s">        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.</span>
<span class="pl-s">    """</span>
    <span class="pl-k">def</span> <span class="pl-en">__init__</span>(<span class="pl-s1">self</span>, <span class="pl-s1">in_chans</span><span class="pl-c1">=</span><span class="pl-c1">3</span>, <span class="pl-s1">num_classes</span><span class="pl-c1">=</span><span class="pl-c1">1000</span>, 
                 <span class="pl-s1">depths</span><span class="pl-c1">=</span>[<span class="pl-c1">3</span>, <span class="pl-c1">3</span>, <span class="pl-c1">9</span>, <span class="pl-c1">3</span>], <span class="pl-s1">dims</span><span class="pl-c1">=</span>[<span class="pl-c1">96</span>, <span class="pl-c1">192</span>, <span class="pl-c1">384</span>, <span class="pl-c1">768</span>], <span class="pl-s1">drop_path_rate</span><span class="pl-c1">=</span><span class="pl-c1">0.</span>, 
                 <span class="pl-s1">layer_scale_init_value</span><span class="pl-c1">=</span><span class="pl-c1">1e-6</span>, <span class="pl-s1">head_init_scale</span><span class="pl-c1">=</span><span class="pl-c1">1.</span>,
                 ):
        <span class="pl-en">super</span>().<span class="pl-en">__init__</span>()

        <span class="pl-s1">self</span>.<span class="pl-s1">downsample_layers</span> <span class="pl-c1">=</span> <span class="pl-s1">nn</span>.<span class="pl-v">ModuleList</span>() <span class="pl-c"># stem and 3 intermediate downsampling conv layers</span>
        <span class="pl-s1">stem</span> <span class="pl-c1">=</span> <span class="pl-s1">nn</span>.<span class="pl-v">Sequential</span>(
            <span class="pl-s1">nn</span>.<span class="pl-v">Conv2d</span>(<span class="pl-s1">in_chans</span>, <span class="pl-s1">dims</span>[<span class="pl-c1">0</span>], <span class="pl-s1">kernel_size</span><span class="pl-c1">=</span><span class="pl-c1">4</span>, <span class="pl-s1">stride</span><span class="pl-c1">=</span><span class="pl-c1">4</span>),
            <span class="pl-v">LayerNorm</span>(<span class="pl-s1">dims</span>[<span class="pl-c1">0</span>], <span class="pl-s1">eps</span><span class="pl-c1">=</span><span class="pl-c1">1e-6</span>, <span class="pl-s1">data_format</span><span class="pl-c1">=</span><span class="pl-s">"channels_first"</span>)
        )
        <span class="pl-s1">self</span>.<span class="pl-s1">downsample_layers</span>.<span class="pl-en">append</span>(<span class="pl-s1">stem</span>)
        <span class="pl-k">for</span> <span class="pl-s1">i</span> <span class="pl-c1">in</span> <span class="pl-en">range</span>(<span class="pl-c1">3</span>):
            <span class="pl-s1">downsample_layer</span> <span class="pl-c1">=</span> <span class="pl-s1">nn</span>.<span class="pl-v">Sequential</span>(
                    <span class="pl-v">LayerNorm</span>(<span class="pl-s1">dims</span>[<span class="pl-s1">i</span>], <span class="pl-s1">eps</span><span class="pl-c1">=</span><span class="pl-c1">1e-6</span>, <span class="pl-s1">data_format</span><span class="pl-c1">=</span><span class="pl-s">"channels_first"</span>),
                    <span class="pl-s1">nn</span>.<span class="pl-v">Conv2d</span>(<span class="pl-s1">dims</span>[<span class="pl-s1">i</span>], <span class="pl-s1">dims</span>[<span class="pl-s1">i</span><span class="pl-c1">+</span><span class="pl-c1">1</span>], <span class="pl-s1">kernel_size</span><span class="pl-c1">=</span><span class="pl-c1">2</span>, <span class="pl-s1">stride</span><span class="pl-c1">=</span><span class="pl-c1">2</span>),
            )
            <span class="pl-s1">self</span>.<span class="pl-s1">downsample_layers</span>.<span class="pl-en">append</span>(<span class="pl-s1">downsample_layer</span>)

        <span class="pl-s1">self</span>.<span class="pl-s1">stages</span> <span class="pl-c1">=</span> <span class="pl-s1">nn</span>.<span class="pl-v">ModuleList</span>() <span class="pl-c"># 4 feature resolution stages, each consisting of multiple residual blocks</span>
        <span class="pl-s1">dp_rates</span><span class="pl-c1">=</span>[<span class="pl-s1">x</span>.<span class="pl-en">item</span>() <span class="pl-k">for</span> <span class="pl-s1">x</span> <span class="pl-c1">in</span> <span class="pl-s1">torch</span>.<span class="pl-en">linspace</span>(<span class="pl-c1">0</span>, <span class="pl-s1">drop_path_rate</span>, <span class="pl-en">sum</span>(<span class="pl-s1">depths</span>))] 
        <span class="pl-s1">cur</span> <span class="pl-c1">=</span> <span class="pl-c1">0</span>
        <span class="pl-k">for</span> <span class="pl-s1">i</span> <span class="pl-c1">in</span> <span class="pl-en">range</span>(<span class="pl-c1">4</span>):
            <span class="pl-s1">stage</span> <span class="pl-c1">=</span> <span class="pl-s1">nn</span>.<span class="pl-v">Sequential</span>(
                <span class="pl-c1">*</span>[<span class="pl-v">Block</span>(<span class="pl-s1">dim</span><span class="pl-c1">=</span><span class="pl-s1">dims</span>[<span class="pl-s1">i</span>], <span class="pl-s1">drop_path</span><span class="pl-c1">=</span><span class="pl-s1">dp_rates</span>[<span class="pl-s1">cur</span> <span class="pl-c1">+</span> <span class="pl-s1">j</span>], 
                <span class="pl-s1">layer_scale_init_value</span><span class="pl-c1">=</span><span class="pl-s1">layer_scale_init_value</span>) <span class="pl-k">for</span> <span class="pl-s1">j</span> <span class="pl-c1">in</span> <span class="pl-en">range</span>(<span class="pl-s1">depths</span>[<span class="pl-s1">i</span>])]
            )
            <span class="pl-s1">self</span>.<span class="pl-s1">stages</span>.<span class="pl-en">append</span>(<span class="pl-s1">stage</span>)
            <span class="pl-s1">cur</span> <span class="pl-c1">+=</span> <span class="pl-s1">depths</span>[<span class="pl-s1">i</span>]

        <span class="pl-s1">self</span>.<span class="pl-s1">norm</span> <span class="pl-c1">=</span> <span class="pl-s1">nn</span>.<span class="pl-v">LayerNorm</span>(<span class="pl-s1">dims</span>[<span class="pl-c1">-</span><span class="pl-c1">1</span>], <span class="pl-s1">eps</span><span class="pl-c1">=</span><span class="pl-c1">1e-6</span>) <span class="pl-c"># final norm layer</span>
        <span class="pl-s1">self</span>.<span class="pl-s1">head</span> <span class="pl-c1">=</span> <span class="pl-s1">nn</span>.<span class="pl-v">Linear</span>(<span class="pl-s1">dims</span>[<span class="pl-c1">-</span><span class="pl-c1">1</span>], <span class="pl-s1">num_classes</span>)

        <span class="pl-s1">self</span>.<span class="pl-en">apply</span>(<span class="pl-s1">self</span>.<span class="pl-s1">_init_weights</span>)
        <span class="pl-s1">self</span>.<span class="pl-s1">head</span>.<span class="pl-s1">weight</span>.<span class="pl-s1">data</span>.<span class="pl-en">mul_</span>(<span class="pl-s1">head_init_scale</span>)
        <span class="pl-s1">self</span>.<span class="pl-s1">head</span>.<span class="pl-s1">bias</span>.<span class="pl-s1">data</span>.<span class="pl-en">mul_</span>(<span class="pl-s1">head_init_scale</span>)

    <span class="pl-k">def</span> <span class="pl-en">_init_weights</span>(<span class="pl-s1">self</span>, <span class="pl-s1">m</span>):
        <span class="pl-k">if</span> <span class="pl-en">isinstance</span>(<span class="pl-s1">m</span>, (<span class="pl-s1">nn</span>.<span class="pl-v">Conv2d</span>, <span class="pl-s1">nn</span>.<span class="pl-v">Linear</span>)):
            <span class="pl-en">trunc_normal_</span>(<span class="pl-s1">m</span>.<span class="pl-s1">weight</span>, <span class="pl-s1">std</span><span class="pl-c1">=</span><span class="pl-c1">.02</span>)
            <span class="pl-s1">nn</span>.<span class="pl-s1">init</span>.<span class="pl-en">constant_</span>(<span class="pl-s1">m</span>.<span class="pl-s1">bias</span>, <span class="pl-c1">0</span>)

    <span class="pl-k">def</span> <span class="pl-en">forward_features</span>(<span class="pl-s1">self</span>, <span class="pl-s1">x</span>):
        <span class="pl-k">for</span> <span class="pl-s1">i</span> <span class="pl-c1">in</span> <span class="pl-en">range</span>(<span class="pl-c1">4</span>):
            <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-s1">downsample_layers</span>[<span class="pl-s1">i</span>](<span class="pl-s1">x</span>)
            <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-s1">stages</span>[<span class="pl-s1">i</span>](<span class="pl-s1">x</span>)
        <span class="pl-k">return</span> <span class="pl-s1">self</span>.<span class="pl-en">norm</span>(<span class="pl-s1">x</span>.<span class="pl-en">mean</span>([<span class="pl-c1">-</span><span class="pl-c1">2</span>, <span class="pl-c1">-</span><span class="pl-c1">1</span>])) <span class="pl-c"># global average pooling, (N, C, H, W) -&gt; (N, C)</span>

    <span class="pl-k">def</span> <span class="pl-en">forward</span>(<span class="pl-s1">self</span>, <span class="pl-s1">x</span>):
        <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-en">forward_features</span>(<span class="pl-s1">x</span>)
        <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-en">head</span>(<span class="pl-s1">x</span>)
        <span class="pl-k">return</span> <span class="pl-s1">x</span>

<span class="pl-k">class</span> <span class="pl-v">LayerNorm</span>(<span class="pl-s1">nn</span>.<span class="pl-v">Module</span>):
    <span class="pl-s">r""" LayerNorm that supports two data formats: channels_last (default) or channels_first. </span>
<span class="pl-s">    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with </span>
<span class="pl-s">    shape (batch_size, height, width, channels) while channels_first corresponds to inputs </span>
<span class="pl-s">    with shape (batch_size, channels, height, width).</span>
<span class="pl-s">    """</span>
    <span class="pl-k">def</span> <span class="pl-en">__init__</span>(<span class="pl-s1">self</span>, <span class="pl-s1">normalized_shape</span>, <span class="pl-s1">eps</span><span class="pl-c1">=</span><span class="pl-c1">1e-6</span>, <span class="pl-s1">data_format</span><span class="pl-c1">=</span><span class="pl-s">"channels_last"</span>):
        <span class="pl-en">super</span>().<span class="pl-en">__init__</span>()
        <span class="pl-s1">self</span>.<span class="pl-s1">weight</span> <span class="pl-c1">=</span> <span class="pl-s1">nn</span>.<span class="pl-v">Parameter</span>(<span class="pl-s1">torch</span>.<span class="pl-en">ones</span>(<span class="pl-s1">normalized_shape</span>))
        <span class="pl-s1">self</span>.<span class="pl-s1">bias</span> <span class="pl-c1">=</span> <span class="pl-s1">nn</span>.<span class="pl-v">Parameter</span>(<span class="pl-s1">torch</span>.<span class="pl-en">zeros</span>(<span class="pl-s1">normalized_shape</span>))
        <span class="pl-s1">self</span>.<span class="pl-s1">eps</span> <span class="pl-c1">=</span> <span class="pl-s1">eps</span>
        <span class="pl-s1">self</span>.<span class="pl-s1">data_format</span> <span class="pl-c1">=</span> <span class="pl-s1">data_format</span>
        <span class="pl-k">if</span> <span class="pl-s1">self</span>.<span class="pl-s1">data_format</span> <span class="pl-c1">not</span> <span class="pl-c1">in</span> [<span class="pl-s">"channels_last"</span>, <span class="pl-s">"channels_first"</span>]:
            <span class="pl-k">raise</span> <span class="pl-v">NotImplementedError</span> 
        <span class="pl-s1">self</span>.<span class="pl-s1">normalized_shape</span> <span class="pl-c1">=</span> (<span class="pl-s1">normalized_shape</span>, )
    
    <span class="pl-k">def</span> <span class="pl-en">forward</span>(<span class="pl-s1">self</span>, <span class="pl-s1">x</span>):
        <span class="pl-k">if</span> <span class="pl-s1">self</span>.<span class="pl-s1">data_format</span> <span class="pl-c1">==</span> <span class="pl-s">"channels_last"</span>:
            <span class="pl-k">return</span> <span class="pl-v">F</span>.<span class="pl-en">layer_norm</span>(<span class="pl-s1">x</span>, <span class="pl-s1">self</span>.<span class="pl-s1">normalized_shape</span>, <span class="pl-s1">self</span>.<span class="pl-s1">weight</span>, <span class="pl-s1">self</span>.<span class="pl-s1">bias</span>, <span class="pl-s1">self</span>.<span class="pl-s1">eps</span>)
        <span class="pl-k">elif</span> <span class="pl-s1">self</span>.<span class="pl-s1">data_format</span> <span class="pl-c1">==</span> <span class="pl-s">"channels_first"</span>:
            <span class="pl-s1">u</span> <span class="pl-c1">=</span> <span class="pl-s1">x</span>.<span class="pl-en">mean</span>(<span class="pl-c1">1</span>, <span class="pl-s1">keepdim</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
            <span class="pl-s1">s</span> <span class="pl-c1">=</span> (<span class="pl-s1">x</span> <span class="pl-c1">-</span> <span class="pl-s1">u</span>).<span class="pl-en">pow</span>(<span class="pl-c1">2</span>).<span class="pl-en">mean</span>(<span class="pl-c1">1</span>, <span class="pl-s1">keepdim</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
            <span class="pl-s1">x</span> <span class="pl-c1">=</span> (<span class="pl-s1">x</span> <span class="pl-c1">-</span> <span class="pl-s1">u</span>) <span class="pl-c1">/</span> <span class="pl-s1">torch</span>.<span class="pl-en">sqrt</span>(<span class="pl-s1">s</span> <span class="pl-c1">+</span> <span class="pl-s1">self</span>.<span class="pl-s1">eps</span>)
            <span class="pl-s1">x</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-s1">weight</span>[:, <span class="pl-c1">None</span>, <span class="pl-c1">None</span>] <span class="pl-c1">*</span> <span class="pl-s1">x</span> <span class="pl-c1">+</span> <span class="pl-s1">self</span>.<span class="pl-s1">bias</span>[:, <span class="pl-c1">None</span>, <span class="pl-c1">None</span>]
            <span class="pl-k">return</span> <span class="pl-s1">x</span>


<span class="pl-s1">model_urls</span> <span class="pl-c1">=</span> {
    <span class="pl-s">"convnext_tiny_1k"</span>: <span class="pl-s">"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth"</span>,
    <span class="pl-s">"convnext_small_1k"</span>: <span class="pl-s">"https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth"</span>,
    <span class="pl-s">"convnext_base_1k"</span>: <span class="pl-s">"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth"</span>,
    <span class="pl-s">"convnext_large_1k"</span>: <span class="pl-s">"https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth"</span>,
    <span class="pl-s">"convnext_tiny_22k"</span>: <span class="pl-s">"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth"</span>,
    <span class="pl-s">"convnext_small_22k"</span>: <span class="pl-s">"https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth"</span>,
    <span class="pl-s">"convnext_base_22k"</span>: <span class="pl-s">"https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth"</span>,
    <span class="pl-s">"convnext_large_22k"</span>: <span class="pl-s">"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth"</span>,
    <span class="pl-s">"convnext_xlarge_22k"</span>: <span class="pl-s">"https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth"</span>,
}

<span class="pl-en">@<span class="pl-s1">register_model</span></span>
<span class="pl-k">def</span> <span class="pl-en">convnext_tiny</span>(<span class="pl-s1">pretrained</span><span class="pl-c1">=</span><span class="pl-c1">False</span>,<span class="pl-s1">in_22k</span><span class="pl-c1">=</span><span class="pl-c1">False</span>, <span class="pl-c1">**</span><span class="pl-s1">kwargs</span>):
    <span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-v">ConvNeXt</span>(<span class="pl-s1">depths</span><span class="pl-c1">=</span>[<span class="pl-c1">3</span>, <span class="pl-c1">3</span>, <span class="pl-c1">9</span>, <span class="pl-c1">3</span>], <span class="pl-s1">dims</span><span class="pl-c1">=</span>[<span class="pl-c1">96</span>, <span class="pl-c1">192</span>, <span class="pl-c1">384</span>, <span class="pl-c1">768</span>], <span class="pl-c1">**</span><span class="pl-s1">kwargs</span>)
    <span class="pl-k">if</span> <span class="pl-s1">pretrained</span>:
        <span class="pl-s1">url</span> <span class="pl-c1">=</span> <span class="pl-s1">model_urls</span>[<span class="pl-s">'convnext_tiny_22k'</span>] <span class="pl-k">if</span> <span class="pl-s1">in_22k</span> <span class="pl-k">else</span> <span class="pl-s1">model_urls</span>[<span class="pl-s">'convnext_tiny_1k'</span>]
        <span class="pl-s1">checkpoint</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-s1">hub</span>.<span class="pl-en">load_state_dict_from_url</span>(<span class="pl-s1">url</span><span class="pl-c1">=</span><span class="pl-s1">url</span>, <span class="pl-s1">map_location</span><span class="pl-c1">=</span><span class="pl-s">"cpu"</span>, <span class="pl-s1">check_hash</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
        <span class="pl-s1">model</span>.<span class="pl-en">load_state_dict</span>(<span class="pl-s1">checkpoint</span>[<span class="pl-s">"model"</span>])
    <span class="pl-k">return</span> <span class="pl-s1">model</span>

<span class="pl-en">@<span class="pl-s1">register_model</span></span>
<span class="pl-k">def</span> <span class="pl-en">convnext_small</span>(<span class="pl-s1">pretrained</span><span class="pl-c1">=</span><span class="pl-c1">False</span>,<span class="pl-s1">in_22k</span><span class="pl-c1">=</span><span class="pl-c1">False</span>, <span class="pl-c1">**</span><span class="pl-s1">kwargs</span>):
    <span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-v">ConvNeXt</span>(<span class="pl-s1">depths</span><span class="pl-c1">=</span>[<span class="pl-c1">3</span>, <span class="pl-c1">3</span>, <span class="pl-c1">27</span>, <span class="pl-c1">3</span>], <span class="pl-s1">dims</span><span class="pl-c1">=</span>[<span class="pl-c1">96</span>, <span class="pl-c1">192</span>, <span class="pl-c1">384</span>, <span class="pl-c1">768</span>], <span class="pl-c1">**</span><span class="pl-s1">kwargs</span>)
    <span class="pl-k">if</span> <span class="pl-s1">pretrained</span>:
        <span class="pl-s1">url</span> <span class="pl-c1">=</span> <span class="pl-s1">model_urls</span>[<span class="pl-s">'convnext_small_22k'</span>] <span class="pl-k">if</span> <span class="pl-s1">in_22k</span> <span class="pl-k">else</span> <span class="pl-s1">model_urls</span>[<span class="pl-s">'convnext_small_1k'</span>]
        <span class="pl-s1">checkpoint</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-s1">hub</span>.<span class="pl-en">load_state_dict_from_url</span>(<span class="pl-s1">url</span><span class="pl-c1">=</span><span class="pl-s1">url</span>, <span class="pl-s1">map_location</span><span class="pl-c1">=</span><span class="pl-s">"cpu"</span>)
        <span class="pl-s1">model</span>.<span class="pl-en">load_state_dict</span>(<span class="pl-s1">checkpoint</span>[<span class="pl-s">"model"</span>])
    <span class="pl-k">return</span> <span class="pl-s1">model</span>

<span class="pl-en">@<span class="pl-s1">register_model</span></span>
<span class="pl-k">def</span> <span class="pl-en">convnext_base</span>(<span class="pl-s1">pretrained</span><span class="pl-c1">=</span><span class="pl-c1">False</span>, <span class="pl-s1">in_22k</span><span class="pl-c1">=</span><span class="pl-c1">False</span>, <span class="pl-c1">**</span><span class="pl-s1">kwargs</span>):
    <span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-v">ConvNeXt</span>(<span class="pl-s1">depths</span><span class="pl-c1">=</span>[<span class="pl-c1">3</span>, <span class="pl-c1">3</span>, <span class="pl-c1">27</span>, <span class="pl-c1">3</span>], <span class="pl-s1">dims</span><span class="pl-c1">=</span>[<span class="pl-c1">128</span>, <span class="pl-c1">256</span>, <span class="pl-c1">512</span>, <span class="pl-c1">1024</span>], <span class="pl-c1">**</span><span class="pl-s1">kwargs</span>)
    <span class="pl-k">if</span> <span class="pl-s1">pretrained</span>:
        <span class="pl-s1">url</span> <span class="pl-c1">=</span> <span class="pl-s1">model_urls</span>[<span class="pl-s">'convnext_base_22k'</span>] <span class="pl-k">if</span> <span class="pl-s1">in_22k</span> <span class="pl-k">else</span> <span class="pl-s1">model_urls</span>[<span class="pl-s">'convnext_base_1k'</span>]
        <span class="pl-s1">checkpoint</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-s1">hub</span>.<span class="pl-en">load_state_dict_from_url</span>(<span class="pl-s1">url</span><span class="pl-c1">=</span><span class="pl-s1">url</span>, <span class="pl-s1">map_location</span><span class="pl-c1">=</span><span class="pl-s">"cpu"</span>)
        <span class="pl-s1">model</span>.<span class="pl-en">load_state_dict</span>(<span class="pl-s1">checkpoint</span>[<span class="pl-s">"model"</span>])
    <span class="pl-k">return</span> <span class="pl-s1">model</span>

<span class="pl-en">@<span class="pl-s1">register_model</span></span>
<span class="pl-k">def</span> <span class="pl-en">convnext_large</span>(<span class="pl-s1">pretrained</span><span class="pl-c1">=</span><span class="pl-c1">False</span>, <span class="pl-s1">in_22k</span><span class="pl-c1">=</span><span class="pl-c1">False</span>, <span class="pl-c1">**</span><span class="pl-s1">kwargs</span>):
    <span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-v">ConvNeXt</span>(<span class="pl-s1">depths</span><span class="pl-c1">=</span>[<span class="pl-c1">3</span>, <span class="pl-c1">3</span>, <span class="pl-c1">27</span>, <span class="pl-c1">3</span>], <span class="pl-s1">dims</span><span class="pl-c1">=</span>[<span class="pl-c1">192</span>, <span class="pl-c1">384</span>, <span class="pl-c1">768</span>, <span class="pl-c1">1536</span>], <span class="pl-c1">**</span><span class="pl-s1">kwargs</span>)
    <span class="pl-k">if</span> <span class="pl-s1">pretrained</span>:
        <span class="pl-s1">url</span> <span class="pl-c1">=</span> <span class="pl-s1">model_urls</span>[<span class="pl-s">'convnext_large_22k'</span>] <span class="pl-k">if</span> <span class="pl-s1">in_22k</span> <span class="pl-k">else</span> <span class="pl-s1">model_urls</span>[<span class="pl-s">'convnext_large_1k'</span>]
        <span class="pl-s1">checkpoint</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-s1">hub</span>.<span class="pl-en">load_state_dict_from_url</span>(<span class="pl-s1">url</span><span class="pl-c1">=</span><span class="pl-s1">url</span>, <span class="pl-s1">map_location</span><span class="pl-c1">=</span><span class="pl-s">"cpu"</span>)
        <span class="pl-s1">model</span>.<span class="pl-en">load_state_dict</span>(<span class="pl-s1">checkpoint</span>[<span class="pl-s">"model"</span>])
    <span class="pl-k">return</span> <span class="pl-s1">model</span>

<span class="pl-en">@<span class="pl-s1">register_model</span></span>
<span class="pl-k">def</span> <span class="pl-en">convnext_xlarge</span>(<span class="pl-s1">pretrained</span><span class="pl-c1">=</span><span class="pl-c1">False</span>, <span class="pl-s1">in_22k</span><span class="pl-c1">=</span><span class="pl-c1">False</span>, <span class="pl-c1">**</span><span class="pl-s1">kwargs</span>):
    <span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-v">ConvNeXt</span>(<span class="pl-s1">depths</span><span class="pl-c1">=</span>[<span class="pl-c1">3</span>, <span class="pl-c1">3</span>, <span class="pl-c1">27</span>, <span class="pl-c1">3</span>], <span class="pl-s1">dims</span><span class="pl-c1">=</span>[<span class="pl-c1">256</span>, <span class="pl-c1">512</span>, <span class="pl-c1">1024</span>, <span class="pl-c1">2048</span>], <span class="pl-c1">**</span><span class="pl-s1">kwargs</span>)
    <span class="pl-k">if</span> <span class="pl-s1">pretrained</span>:
        <span class="pl-k">assert</span> <span class="pl-s1">in_22k</span>, <span class="pl-s">"only ImageNet-22K pre-trained ConvNeXt-XL is available; please set in_22k=True"</span>
        <span class="pl-s1">url</span> <span class="pl-c1">=</span> <span class="pl-s1">model_urls</span>[<span class="pl-s">'convnext_xlarge_22k'</span>]
        <span class="pl-s1">checkpoint</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-s1">hub</span>.<span class="pl-en">load_state_dict_from_url</span>(<span class="pl-s1">url</span><span class="pl-c1">=</span><span class="pl-s1">url</span>, <span class="pl-s1">map_location</span><span class="pl-c1">=</span><span class="pl-s">"cpu"</span>)
        <span class="pl-s1">model</span>.<span class="pl-en">load_state_dict</span>(<span class="pl-s1">checkpoint</span>[<span class="pl-s">"model"</span>])
    <span class="pl-k">return</span> <span class="pl-s1">model</span></pre></div>
<p></p>
</details> 
<h3>代码</h3>
<p>训练代码大部分都是模板, 不过, 我发现我还没有一个属于自己的 trainer, 趁着这次训练模型的时候补充一个</p>
<p>使用别人的 trainer 难免会遇到 debug, 而代码不熟的情况, 自己写的 trainer 可以掌握各种细节</p>
<p>在整理了一些以前代码后, 总结出了覆盖许多训练模型情况的流程, 趁着这个时候测试一下现在ai编码的能力, 将流程发给 claude-sonnet 后, 输出了一版代码, 在我的一些小修小补(补充日志)后, 就可以跑起来了</p>
<details><summary>trainer.py</summary>
<p>
</p><div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">import</span> <span class="pl-s1">gc</span>
<span class="pl-k">import</span> <span class="pl-s1">json</span>
<span class="pl-k">import</span> <span class="pl-s1">logging</span>
<span class="pl-k">import</span> <span class="pl-s1">os</span>
<span class="pl-k">import</span> <span class="pl-s1">shutil</span>

<span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">from</span> <span class="pl-s1">torch</span> <span class="pl-k">import</span> <span class="pl-s1">optim</span>
<span class="pl-k">from</span> <span class="pl-s1">torch</span>.<span class="pl-s1">amp</span> <span class="pl-k">import</span> <span class="pl-v">GradScaler</span>
<span class="pl-k">from</span> <span class="pl-s1">torch</span>.<span class="pl-s1">nn</span>.<span class="pl-s1">utils</span> <span class="pl-k">import</span> <span class="pl-s1">clip_grad_norm_</span>
<span class="pl-k">from</span> <span class="pl-s1">torch</span>.<span class="pl-s1">utils</span>.<span class="pl-s1">tensorboard</span> <span class="pl-k">import</span> <span class="pl-v">SummaryWriter</span>
<span class="pl-k">from</span> <span class="pl-s1">tqdm</span> <span class="pl-k">import</span> <span class="pl-s1">tqdm</span>

<span class="pl-s1">example_config</span> <span class="pl-c1">=</span> {
    <span class="pl-s">"model"</span>: <span class="pl-s">"model_name"</span>,
    <span class="pl-s">"checkpoint_dir"</span>: <span class="pl-s">"./checkpoints"</span>,
    <span class="pl-s">"tensorboard_dir"</span>: <span class="pl-s">"./tensorboard"</span>,
    <span class="pl-s">"device"</span>: <span class="pl-s">"cuda"</span>,
    <span class="pl-s">"enable_cudnn_benchmark"</span>: <span class="pl-c1">True</span>,
    <span class="pl-s">"enable_amp"</span>: <span class="pl-c1">False</span>,
    <span class="pl-s">"learning_rate"</span>: <span class="pl-c1">1e-4</span>,
    <span class="pl-s">"betas"</span>: [<span class="pl-c1">0.9</span>, <span class="pl-c1">0.999</span>],
    <span class="pl-s">"eps"</span>: <span class="pl-c1">1e-8</span>,
    <span class="pl-s">"enable_compile"</span>: <span class="pl-c1">False</span>,
    <span class="pl-s">"weight_decay"</span>: <span class="pl-c1">0.05</span>,
    <span class="pl-s">"max_steps"</span>: <span class="pl-c1">100000</span>,
    <span class="pl-s">"max_grad_norm"</span>: <span class="pl-c1">1.0</span>,
    <span class="pl-s">"save_every"</span>: <span class="pl-c1">10000</span>,
    <span class="pl-s">"gradient_accumulation_steps"</span>: <span class="pl-c1">4</span>
}


<span class="pl-k">class</span> <span class="pl-v">Trainer</span>:
    <span class="pl-k">def</span> <span class="pl-en">__init__</span>(<span class="pl-s1">self</span>, <span class="pl-s1">config</span>):
        <span class="pl-s1">self</span>.<span class="pl-s1">config</span> <span class="pl-c1">=</span> <span class="pl-s1">config</span>
        <span class="pl-s1">self</span>.<span class="pl-en">setup_logging</span>()
        <span class="pl-s1">self</span>.<span class="pl-en">setup_device</span>()
        <span class="pl-s1">self</span>.<span class="pl-en">setup_model</span>()
        <span class="pl-s1">self</span>.<span class="pl-en">setup_training</span>()
        
    <span class="pl-k">def</span> <span class="pl-en">setup_logging</span>(<span class="pl-s1">self</span>):
        <span class="pl-s">"""设置日志"""</span>
        <span class="pl-s1">logging</span>.<span class="pl-en">basicConfig</span>(
            <span class="pl-s1">level</span><span class="pl-c1">=</span><span class="pl-s1">logging</span>.<span class="pl-v">INFO</span>,
            <span class="pl-s1">format</span><span class="pl-c1">=</span><span class="pl-s">'%(asctime)s %(levelname)s %(message)s'</span>
        )
        <span class="pl-s1">self</span>.<span class="pl-s1">logger</span> <span class="pl-c1">=</span> <span class="pl-s1">logging</span>.<span class="pl-en">getLogger</span>(<span class="pl-s1">__name__</span>)
        <span class="pl-s1">self</span>.<span class="pl-s1">writer</span> <span class="pl-c1">=</span> <span class="pl-v">SummaryWriter</span>(<span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'tensorboard_dir'</span>])
        
    <span class="pl-k">def</span> <span class="pl-en">setup_device</span>(<span class="pl-s1">self</span>):
        <span class="pl-s">"""设置设备"""</span>
        <span class="pl-s1">self</span>.<span class="pl-s1">device</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-en">device</span>(<span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'device'</span>])
        <span class="pl-s1">torch</span>.<span class="pl-s1">backends</span>.<span class="pl-s1">cudnn</span>.<span class="pl-s1">benchmark</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-s1">config</span>.<span class="pl-en">get</span>(<span class="pl-s">'enable_cudnn_benchmark'</span>, <span class="pl-c1">True</span>)
        <span class="pl-k">if</span> <span class="pl-s1">self</span>.<span class="pl-s1">device</span>.<span class="pl-s1">type</span> <span class="pl-c1">==</span> <span class="pl-s">'cuda'</span>:
            <span class="pl-s1">self</span>.<span class="pl-s1">logger</span>.<span class="pl-en">info</span>(<span class="pl-s">f'Using device: <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">self</span>.<span class="pl-s1">device</span><span class="pl-kos">}</span></span> (<span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">torch</span>.<span class="pl-s1">cuda</span>.<span class="pl-en">get_device_name</span>()<span class="pl-kos">}</span></span>)'</span>)
        <span class="pl-k">else</span>:
            <span class="pl-s1">self</span>.<span class="pl-s1">logger</span>.<span class="pl-en">info</span>(<span class="pl-s">f'Using device: <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">self</span>.<span class="pl-s1">device</span><span class="pl-kos">}</span></span>'</span>)
            
    <span class="pl-k">def</span> <span class="pl-en">setup_model</span>(<span class="pl-s1">self</span>):
        <span class="pl-s">"""设置模型、损失函数等"""</span>
        <span class="pl-s1">self</span>.<span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-en">build_model</span>().<span class="pl-en">to</span>(<span class="pl-s1">self</span>.<span class="pl-s1">device</span>)
        <span class="pl-k">if</span> <span class="pl-s1">self</span>.<span class="pl-s1">config</span>.<span class="pl-en">get</span>(<span class="pl-s">'enable_compile'</span>, <span class="pl-c1">False</span>):
            <span class="pl-s1">self</span>.<span class="pl-s1">model</span>.<span class="pl-en">compile</span>()
        <span class="pl-s1">self</span>.<span class="pl-s1">criterion</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-en">build_criterion</span>()
        
        <span class="pl-c"># 打印模型信息</span>
        <span class="pl-s1">n_parameters</span> <span class="pl-c1">=</span> <span class="pl-en">sum</span>(<span class="pl-s1">p</span>.<span class="pl-en">numel</span>() <span class="pl-k">for</span> <span class="pl-s1">p</span> <span class="pl-c1">in</span> <span class="pl-s1">self</span>.<span class="pl-s1">model</span>.<span class="pl-en">parameters</span>())
        <span class="pl-s1">self</span>.<span class="pl-s1">logger</span>.<span class="pl-en">info</span>(<span class="pl-s">f'Number of parameters: <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">n_parameters</span>:,<span class="pl-kos">}</span></span>'</span>)
        
    <span class="pl-k">def</span> <span class="pl-en">setup_training</span>(<span class="pl-s1">self</span>):
        <span class="pl-s">"""设置训练相关组件"""</span>
        <span class="pl-c"># 优化器</span>
        <span class="pl-s1">self</span>.<span class="pl-s1">optimizer</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-en">build_optimizer</span>()
        
        <span class="pl-c"># 学习率调度器</span>
        <span class="pl-s1">self</span>.<span class="pl-s1">scheduler</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-en">build_scheduler</span>()
        
        <span class="pl-c"># 梯度缩放器(用于混合精度训练)</span>
        <span class="pl-s1">self</span>.<span class="pl-s1">scaler</span> <span class="pl-c1">=</span> <span class="pl-v">GradScaler</span>(
            <span class="pl-s1">enabled</span><span class="pl-c1">=</span><span class="pl-s1">self</span>.<span class="pl-s1">config</span>.<span class="pl-en">get</span>(<span class="pl-s">'enable_amp'</span>, <span class="pl-c1">False</span>)
        )
        <span class="pl-s1">self</span>.<span class="pl-s1">gradient_accumulation_steps</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-s1">config</span>.<span class="pl-en">get</span>(<span class="pl-s">'gradient_accumulation_steps'</span>, <span class="pl-c1">1</span>)
        
        <span class="pl-c"># 加载检查点</span>
        <span class="pl-s1">self</span>.<span class="pl-s1">steps</span> <span class="pl-c1">=</span> <span class="pl-c1">0</span>
        <span class="pl-s1">self</span>.<span class="pl-s1">best_metric</span> <span class="pl-c1">=</span> {}
        <span class="pl-s1">self</span>.<span class="pl-en">load_checkpoint</span>()
        
    <span class="pl-k">def</span> <span class="pl-en">build_model</span>(<span class="pl-s1">self</span>):
        <span class="pl-s">"""构建模型(需要子类实现)"""</span>
        <span class="pl-k">raise</span> <span class="pl-v">NotImplementedError</span>
        
    <span class="pl-k">def</span> <span class="pl-en">build_criterion</span>(<span class="pl-s1">self</span>):
        <span class="pl-s">"""构建损失函数(需要子类实现)"""</span>
        <span class="pl-k">raise</span> <span class="pl-v">NotImplementedError</span>
        
    <span class="pl-k">def</span> <span class="pl-en">build_optimizer</span>(<span class="pl-s1">self</span>):
        <span class="pl-s">"""构建优化器"""</span>
        <span class="pl-c"># 区分需要和不需要weight decay的参数</span>
        <span class="pl-s1">decay_params</span> <span class="pl-c1">=</span> []
        <span class="pl-s1">no_decay_params</span> <span class="pl-c1">=</span> []
        <span class="pl-k">for</span> <span class="pl-s1">name</span>, <span class="pl-s1">param</span> <span class="pl-c1">in</span> <span class="pl-s1">self</span>.<span class="pl-s1">model</span>.<span class="pl-en">named_parameters</span>():
            <span class="pl-k">if</span> <span class="pl-s">'bias'</span> <span class="pl-c1">in</span> <span class="pl-s1">name</span> <span class="pl-c1">or</span> <span class="pl-s">'norm'</span> <span class="pl-c1">in</span> <span class="pl-s1">name</span>:
                <span class="pl-s1">no_decay_params</span>.<span class="pl-en">append</span>(<span class="pl-s1">param</span>)
            <span class="pl-k">else</span>:
                <span class="pl-s1">decay_params</span>.<span class="pl-en">append</span>(<span class="pl-s1">param</span>)
                
        <span class="pl-s1">opt_params</span> <span class="pl-c1">=</span> [
            {<span class="pl-s">'params'</span>: <span class="pl-s1">decay_params</span>, <span class="pl-s">'weight_decay'</span>: <span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'weight_decay'</span>]},
            {<span class="pl-s">'params'</span>: <span class="pl-s1">no_decay_params</span>, <span class="pl-s">'weight_decay'</span>: <span class="pl-c1">0.0</span>}
        ]
        
        <span class="pl-k">return</span> <span class="pl-s1">optim</span>.<span class="pl-v">AdamW</span>(
            <span class="pl-s1">opt_params</span>,
            <span class="pl-s1">lr</span><span class="pl-c1">=</span><span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'learning_rate'</span>],
            <span class="pl-s1">betas</span><span class="pl-c1">=</span><span class="pl-s1">self</span>.<span class="pl-s1">config</span>.<span class="pl-en">get</span>(<span class="pl-s">'betas'</span>, (<span class="pl-c1">0.9</span>, <span class="pl-c1">0.999</span>)),
            <span class="pl-s1">eps</span><span class="pl-c1">=</span><span class="pl-s1">self</span>.<span class="pl-s1">config</span>.<span class="pl-en">get</span>(<span class="pl-s">'eps'</span>, <span class="pl-c1">1e-8</span>)
        )
        
    <span class="pl-k">def</span> <span class="pl-en">build_scheduler</span>(<span class="pl-s1">self</span>):
        <span class="pl-s">"""构建学习率调度器(需要子类实现)"""</span>
        <span class="pl-k">return</span> <span class="pl-v">NotImplementedError</span>
        
    <span class="pl-k">def</span> <span class="pl-en">build_dataloader</span>(<span class="pl-s1">self</span>):
        <span class="pl-s">"""构建数据加载器(需要子类实现)"""</span>
        <span class="pl-k">raise</span> <span class="pl-v">NotImplementedError</span>
        
    <span class="pl-k">def</span> <span class="pl-en">train_step</span>(<span class="pl-s1">self</span>, <span class="pl-s1">batch</span>):
        <span class="pl-s">"""单步训练(需要子类实现)"""</span>
        <span class="pl-k">raise</span> <span class="pl-v">NotImplementedError</span>
        
    <span class="pl-k">def</span> <span class="pl-en">validate</span>(<span class="pl-s1">self</span>):
        <span class="pl-s">"""验证(需要子类实现)"""</span>
        <span class="pl-k">raise</span> <span class="pl-v">NotImplementedError</span>
        
    <span class="pl-k">def</span> <span class="pl-en">save_checkpoint</span>(<span class="pl-s1">self</span>, <span class="pl-s1">is_best</span><span class="pl-c1">=</span><span class="pl-c1">False</span>):
        <span class="pl-s">"""保存检查点"""</span>
        <span class="pl-s1">state</span> <span class="pl-c1">=</span> {
            <span class="pl-s">'model'</span>: <span class="pl-s1">self</span>.<span class="pl-s1">model</span>.<span class="pl-en">state_dict</span>(),
            <span class="pl-s">'optimizer'</span>: <span class="pl-s1">self</span>.<span class="pl-s1">optimizer</span>.<span class="pl-en">state_dict</span>(),
            <span class="pl-s">'scheduler'</span>: <span class="pl-s1">self</span>.<span class="pl-s1">scheduler</span>.<span class="pl-en">state_dict</span>(),
            <span class="pl-s">'scaler'</span>: <span class="pl-s1">self</span>.<span class="pl-s1">scaler</span>.<span class="pl-en">state_dict</span>(),
            <span class="pl-s">'steps'</span>: <span class="pl-s1">self</span>.<span class="pl-s1">steps</span>,
            <span class="pl-s">'best_metric'</span>: <span class="pl-s1">self</span>.<span class="pl-s1">best_metric</span>,
            <span class="pl-s">'config'</span>: <span class="pl-s1">self</span>.<span class="pl-s1">config</span>
        }
        
        <span class="pl-c"># 保存最新检查点</span>
        <span class="pl-s1">torch</span>.<span class="pl-en">save</span>(
            <span class="pl-s1">state</span>,
            <span class="pl-s1">os</span>.<span class="pl-s1">path</span>.<span class="pl-en">join</span>(<span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'checkpoint_dir'</span>], <span class="pl-s">'latest.pt'</span>)
        )
        
        <span class="pl-c"># 保存最佳检查点</span>
        <span class="pl-k">if</span> <span class="pl-s1">is_best</span>:
            <span class="pl-s1">shutil</span>.<span class="pl-en">copy</span>(
                <span class="pl-s1">os</span>.<span class="pl-s1">path</span>.<span class="pl-en">join</span>(<span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'checkpoint_dir'</span>], <span class="pl-s">'latest.pt'</span>),
                <span class="pl-s1">os</span>.<span class="pl-s1">path</span>.<span class="pl-en">join</span>(<span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'checkpoint_dir'</span>], <span class="pl-s">'best.pt'</span>)
            )
            
    <span class="pl-k">def</span> <span class="pl-en">load_checkpoint</span>(<span class="pl-s1">self</span>):
        <span class="pl-s">"""加载检查点"""</span>
        <span class="pl-s1">checkpoint_path</span> <span class="pl-c1">=</span> <span class="pl-s1">os</span>.<span class="pl-s1">path</span>.<span class="pl-en">join</span>(
            <span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'checkpoint_dir'</span>],
            <span class="pl-s">'latest.pt'</span>
        )
        
        <span class="pl-k">if</span> <span class="pl-s1">os</span>.<span class="pl-s1">path</span>.<span class="pl-en">exists</span>(<span class="pl-s1">checkpoint_path</span>):
            <span class="pl-s1">checkpoint</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-en">load</span>(
                <span class="pl-s1">checkpoint_path</span>,
                <span class="pl-s1">map_location</span><span class="pl-c1">=</span><span class="pl-s1">self</span>.<span class="pl-s1">device</span>
            )
            
            <span class="pl-s1">self</span>.<span class="pl-s1">model</span>.<span class="pl-en">load_state_dict</span>(<span class="pl-s1">checkpoint</span>[<span class="pl-s">'model'</span>])
            <span class="pl-s1">self</span>.<span class="pl-s1">optimizer</span>.<span class="pl-en">load_state_dict</span>(<span class="pl-s1">checkpoint</span>[<span class="pl-s">'optimizer'</span>])
            <span class="pl-s1">self</span>.<span class="pl-s1">scheduler</span>.<span class="pl-en">load_state_dict</span>(<span class="pl-s1">checkpoint</span>[<span class="pl-s">'scheduler'</span>])
            <span class="pl-s1">self</span>.<span class="pl-s1">scaler</span>.<span class="pl-en">load_state_dict</span>(<span class="pl-s1">checkpoint</span>[<span class="pl-s">'scaler'</span>])
            <span class="pl-s1">self</span>.<span class="pl-s1">steps</span> <span class="pl-c1">=</span> <span class="pl-s1">checkpoint</span>[<span class="pl-s">'steps'</span>]
            <span class="pl-s1">self</span>.<span class="pl-s1">best_metric</span> <span class="pl-c1">=</span> <span class="pl-s1">checkpoint</span>[<span class="pl-s">'best_metric'</span>]
            
            <span class="pl-s1">self</span>.<span class="pl-s1">logger</span>.<span class="pl-en">info</span>(<span class="pl-s">f'Loaded checkpoint from <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">checkpoint_path</span><span class="pl-kos">}</span></span>'</span>)
            <span class="pl-s1">self</span>.<span class="pl-s1">logger</span>.<span class="pl-en">info</span>(<span class="pl-s">f'Training will resume from step <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">self</span>.<span class="pl-s1">steps</span><span class="pl-kos">}</span></span>'</span>)
    
    <span class="pl-en">@<span class="pl-s1">staticmethod</span></span>
    <span class="pl-k">def</span> <span class="pl-en">is_better_performance</span>(<span class="pl-s1">baseline_dict</span>, <span class="pl-s1">compare_dict</span>):
        <span class="pl-s">"""</span>
<span class="pl-s">        判断compare_dict中的指标是否全面超过baseline_dict</span>
<span class="pl-s">        </span>
<span class="pl-s">        Args:</span>
<span class="pl-s">            baseline_dict: 基准字典,格式为 {指标名: 值}</span>
<span class="pl-s">            compare_dict: 比较字典,格式为 {指标名: 值} </span>
<span class="pl-s">        </span>
<span class="pl-s">        Returns:</span>
<span class="pl-s">            bool: 如果compare_dict中所有指标都严格大于baseline_dict则返回True,否则返回False</span>
<span class="pl-s">        """</span>
        <span class="pl-k">if</span> <span class="pl-c1">not</span> <span class="pl-s1">baseline_dict</span>:
            <span class="pl-k">return</span> <span class="pl-c1">True</span>
        
        <span class="pl-c"># 检查两个字典的键是否一致</span>
        <span class="pl-k">if</span> <span class="pl-en">set</span>(<span class="pl-s1">baseline_dict</span>.<span class="pl-en">keys</span>()) <span class="pl-c1">!=</span> <span class="pl-en">set</span>(<span class="pl-s1">compare_dict</span>.<span class="pl-en">keys</span>()):
            <span class="pl-k">return</span> <span class="pl-c1">False</span>
            
        <span class="pl-c"># 检查每个指标是否都有提升</span>
        <span class="pl-k">for</span> <span class="pl-s1">metric</span> <span class="pl-c1">in</span> <span class="pl-s1">baseline_dict</span>:
            <span class="pl-k">if</span> <span class="pl-s1">compare_dict</span>[<span class="pl-s1">metric</span>] <span class="pl-c1">&lt;=</span> <span class="pl-s1">baseline_dict</span>[<span class="pl-s1">metric</span>]:
                <span class="pl-k">return</span> <span class="pl-c1">False</span>
                
        <span class="pl-k">return</span> <span class="pl-c1">True</span>
            
    <span class="pl-k">def</span> <span class="pl-en">train</span>(<span class="pl-s1">self</span>):
        <span class="pl-s">"""训练流程"""</span>
        <span class="pl-s1">train_loader</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-en">build_dataloader</span>()
        <span class="pl-s1">self</span>.<span class="pl-s1">model</span>.<span class="pl-en">train</span>()
        
        <span class="pl-s1">self</span>.<span class="pl-s1">logger</span>.<span class="pl-en">info</span>(<span class="pl-s">'Start training...'</span>)
        <span class="pl-s1">pbar</span> <span class="pl-c1">=</span> <span class="pl-en">tqdm</span>(<span class="pl-s1">total</span><span class="pl-c1">=</span><span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'max_steps'</span>], <span class="pl-s1">initial</span><span class="pl-c1">=</span><span class="pl-s1">self</span>.<span class="pl-s1">steps</span>)
        
        <span class="pl-k">while</span> <span class="pl-s1">self</span>.<span class="pl-s1">steps</span> <span class="pl-c1">&lt;</span> <span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'max_steps'</span>]:
            <span class="pl-k">for</span> <span class="pl-s1">batch</span> <span class="pl-c1">in</span> <span class="pl-s1">train_loader</span>:
                <span class="pl-c"># 训练一步</span>
                <span class="pl-k">with</span> <span class="pl-s1">torch</span>.<span class="pl-en">autocast</span>(<span class="pl-s1">device_type</span><span class="pl-c1">=</span><span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'device'</span>], <span class="pl-s1">enabled</span><span class="pl-c1">=</span><span class="pl-s1">self</span>.<span class="pl-s1">config</span>.<span class="pl-en">get</span>(<span class="pl-s">'enable_amp'</span>, <span class="pl-c1">False</span>)):
                    <span class="pl-s1">loss</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-en">train_step</span>(<span class="pl-s1">batch</span>)
                <span class="pl-s1">self</span>.<span class="pl-s1">scaler</span>.<span class="pl-en">scale</span>(<span class="pl-s1">loss</span> <span class="pl-c1">/</span> <span class="pl-s1">self</span>.<span class="pl-s1">gradient_accumulation_steps</span>).<span class="pl-en">backward</span>()
                
                <span class="pl-k">if</span> (<span class="pl-s1">self</span>.<span class="pl-s1">steps</span> <span class="pl-c1">+</span> <span class="pl-c1">1</span>) <span class="pl-c1">%</span> <span class="pl-s1">self</span>.<span class="pl-s1">gradient_accumulation_steps</span> <span class="pl-c1">==</span> <span class="pl-c1">0</span>:
                    <span class="pl-c"># 梯度裁剪</span>
                    <span class="pl-k">if</span> <span class="pl-s1">self</span>.<span class="pl-s1">config</span>.<span class="pl-en">get</span>(<span class="pl-s">'max_grad_norm'</span>, <span class="pl-c1">0</span>) <span class="pl-c1">&gt;</span> <span class="pl-c1">0</span>:
                        <span class="pl-s1">self</span>.<span class="pl-s1">scaler</span>.<span class="pl-en">unscale_</span>(<span class="pl-s1">self</span>.<span class="pl-s1">optimizer</span>)
                        <span class="pl-en">clip_grad_norm_</span>(
                            <span class="pl-s1">self</span>.<span class="pl-s1">model</span>.<span class="pl-en">parameters</span>(),
                            <span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'max_grad_norm'</span>]
                        )

                    <span class="pl-c"># 优化器步进</span>
                    <span class="pl-s1">self</span>.<span class="pl-s1">scaler</span>.<span class="pl-en">step</span>(<span class="pl-s1">self</span>.<span class="pl-s1">optimizer</span>)
                    <span class="pl-s1">self</span>.<span class="pl-s1">scaler</span>.<span class="pl-en">update</span>()
                    <span class="pl-s1">self</span>.<span class="pl-s1">optimizer</span>.<span class="pl-en">zero_grad</span>(<span class="pl-s1">set_to_none</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
                <span class="pl-s1">self</span>.<span class="pl-s1">scheduler</span>.<span class="pl-en">step</span>()
                
                <span class="pl-c"># 记录</span>
                <span class="pl-s1">self</span>.<span class="pl-s1">writer</span>.<span class="pl-en">add_scalar</span>(<span class="pl-s">'train/loss'</span>, <span class="pl-s1">loss</span>, <span class="pl-s1">self</span>.<span class="pl-s1">steps</span>)
                <span class="pl-s1">self</span>.<span class="pl-s1">writer</span>.<span class="pl-en">add_scalar</span>(
                    <span class="pl-s">'train/lr'</span>,
                    <span class="pl-s1">self</span>.<span class="pl-s1">scheduler</span>.<span class="pl-en">get_last_lr</span>()[<span class="pl-c1">0</span>],
                    <span class="pl-s1">self</span>.<span class="pl-s1">steps</span>
                )
                
                <span class="pl-s1">self</span>.<span class="pl-s1">steps</span> <span class="pl-c1">+=</span> <span class="pl-c1">1</span>
                <span class="pl-s1">pbar</span>.<span class="pl-en">update</span>(<span class="pl-c1">1</span>)
                
                <span class="pl-c"># 验证和保存</span>
                <span class="pl-k">if</span> <span class="pl-s1">self</span>.<span class="pl-s1">steps</span> <span class="pl-c1">%</span> <span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'save_every'</span>] <span class="pl-c1">==</span> <span class="pl-c1">0</span>:
                    <span class="pl-s1">metric</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-en">validate</span>()
                    <span class="pl-k">for</span> <span class="pl-s1">i</span> <span class="pl-c1">in</span> <span class="pl-s1">metric</span>:
                        <span class="pl-s1">self</span>.<span class="pl-s1">logger</span>.<span class="pl-en">info</span>(<span class="pl-s">f'Validation <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">i</span><span class="pl-kos">}</span></span>: <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">metric</span>[<span class="pl-s1">i</span>]<span class="pl-kos">}</span></span>'</span>)
                        <span class="pl-s1">self</span>.<span class="pl-s1">writer</span>.<span class="pl-en">add_scalar</span>(<span class="pl-s">f'val/<span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">i</span><span class="pl-kos">}</span></span>'</span>, <span class="pl-s1">metric</span>[<span class="pl-s1">i</span>], <span class="pl-s1">self</span>.<span class="pl-s1">steps</span>)
                    
                    <span class="pl-s1">is_best</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-en">is_better_performance</span>(<span class="pl-s1">self</span>.<span class="pl-s1">best_metric</span>, <span class="pl-s1">metric</span>)
                    <span class="pl-k">if</span> <span class="pl-s1">is_best</span>:
                        <span class="pl-s1">self</span>.<span class="pl-s1">best_metric</span> <span class="pl-c1">=</span> <span class="pl-s1">metric</span>

                    <span class="pl-s1">self</span>.<span class="pl-s1">model</span>.<span class="pl-en">train</span>()
                    <span class="pl-s1">self</span>.<span class="pl-en">save_checkpoint</span>(<span class="pl-s1">is_best</span>)
                    
                <span class="pl-k">if</span> <span class="pl-s1">self</span>.<span class="pl-s1">steps</span> <span class="pl-c1">&gt;=</span> <span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'max_steps'</span>]:
                    <span class="pl-k">break</span>
                
            <span class="pl-s1">gc</span>.<span class="pl-en">collect</span>()
            <span class="pl-s1">torch</span>.<span class="pl-s1">cuda</span>.<span class="pl-en">empty_cache</span>()
                    
        <span class="pl-s1">pbar</span>.<span class="pl-en">close</span>()
        <span class="pl-s1">self</span>.<span class="pl-s1">logger</span>.<span class="pl-en">info</span>(<span class="pl-s">'Training finished!'</span>)


<span class="pl-k">def</span> <span class="pl-en">main</span>():
    <span class="pl-s">"""主函数"""</span>
    <span class="pl-c"># 加载配置</span>
    <span class="pl-k">with</span> <span class="pl-en">open</span>(<span class="pl-s">'config.json'</span>) <span class="pl-k">as</span> <span class="pl-s1">f</span>:
        <span class="pl-s1">config</span> <span class="pl-c1">=</span> <span class="pl-s1">json</span>.<span class="pl-en">load</span>(<span class="pl-s1">f</span>)
        
    <span class="pl-c"># 创建输出目录</span>
    <span class="pl-s1">os</span>.<span class="pl-en">makedirs</span>(<span class="pl-s1">config</span>[<span class="pl-s">'checkpoint_dir'</span>], <span class="pl-s1">exist_ok</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
    <span class="pl-s1">os</span>.<span class="pl-en">makedirs</span>(<span class="pl-s1">config</span>[<span class="pl-s">'tensorboard_dir'</span>], <span class="pl-s1">exist_ok</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
    
    <span class="pl-c"># 训练</span>
    <span class="pl-s1">trainer</span> <span class="pl-c1">=</span> <span class="pl-v">Trainer</span>(<span class="pl-s1">config</span>)
    <span class="pl-s1">trainer</span>.<span class="pl-en">train</span>()
    
<span class="pl-k">if</span> <span class="pl-s1">__name__</span> <span class="pl-c1">==</span> <span class="pl-s">'__main__'</span>:
    <span class="pl-k">try</span>:
        <span class="pl-en">main</span>()
    <span class="pl-k">except</span> <span class="pl-v">KeyboardInterrupt</span>:
        <span class="pl-k">pass</span></pre></div>
<p></p>
</details> 
<p><code class="notranslate">trainer.py</code> 简单整合了几个常用的训练手段, 比如混合精度训练, 梯度裁剪, 梯度累计, weight_decay(写死了), tensorboard的记录, 断点续训等操作, 需要注意的是, <code class="notranslate">trainer.py</code> 没有使用 epoch 作为训练进度, 而是用了更精细的 step (每次迭代参数即为一个step), 使用的时候需要自行实现模型构建, 损失函数构建 学习率调度器 数据集加载器, 单步训练, 验证的流程的子类实现</p>
<p>然后将一些配置放到config中便于读取, 其中有一些配置是必须的, 其他则是子类实现的时候需要的</p>
<p>听起来可能有点抽象, 下面是一个简单的trainer使用案例</p>
<details><summary>trainer使用案例</summary>
<p>
</p><p>import torchvision<br>
import torch<br>
from trainer import Trainer<br>
from torchvision.models import resnet18<br>
from torch.optim.lr_scheduler import LambdaLR</p>
<p>transform = torchvision.transforms.Compose([<br>
torchvision.transforms.ToTensor(),<br>
torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))<br>
])</p>
<p>class ConstantLambdaLR(LambdaLR):<br>
def <strong>init</strong>(self, optimizer, **kwargs):<br>
kwargs['optimizer'] = optimizer<br>
kwargs['lr_lambda'] = self._step_inner<br>
super().<strong>init</strong>(**kwargs)</p>
<pre class="notranslate"><code class="notranslate">def _step_inner(self, steps):
    return 1
</code></pre>
<p>class Cifer10Trainer(Trainer):<br>
def <strong>init</strong>(self, config):<br>
super().<strong>init</strong>(config)</p>
<pre class="notranslate"><code class="notranslate">def build_model(self):
    model = resnet18()
    model.fc = torch.nn.Linear(model.fc.in_features, 10)
    return model

def build_criterion(self):
    return torch.nn.CrossEntropyLoss()

def build_scheduler(self):
    return ConstantLambdaLR(self.optimizer)

def build_dataloader(self):
    train_dataset = torchvision.datasets.CIFAR10(root='./temp', train=True, download=True, transform=transform)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.config['batch_size'], shuffle=True, num_workers=1)
    return train_loader

def train_step(self, batch):
    inputs, labels = batch
    inputs, labels = inputs.to(self.device), labels.to(self.device)
    outputs = self.model(inputs)
    loss = self.criterion(outputs, labels)
    return loss

def validate(self):
    self.model.eval()
    test_dataset = torchvision.datasets.CIFAR10(root='./temp', train=False, download=True, transform=transform)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=self.config['batch_size'], shuffle=False, num_workers=1)
    acc = []
    with torch.inference_mode():
        for batch in test_loader:
            inputs, labels = batch
            inputs, labels = inputs.to(self.device), labels.to(self.device)
            y_hat = self.model(inputs)
            acc.append((y_hat.argmax(dim=1) == labels).sum().item() / labels.size(0))
            
    return {'acc': sum(acc) / len(acc)}
</code></pre>
<p>def main():<br>
config = {<br>
"model": "resnet18",<br>
"checkpoint_dir": "./checkpoints",<br>
"tensorboard_dir": "./tensorboard",<br>
"device": "cuda",<br>
"enable_cudnn_benchmark": True,<br>
"enable_amp": False,<br>
"learning_rate": 1e-3,<br>
"betas": [0.9, 0.999],<br>
"eps": 1e-8,<br>
"enable_compile": False,<br>
"weight_decay": 0.05,<br>
"max_steps": 500,<br>
"max_grad_norm": 1.0,<br>
"save_every": 100,<br>
"gradient_accumulation_steps": 1,<br>
'batch_size': 32<br>
}<br>
trainer = Cifer10Trainer(config)<br>
trainer.train()</p>
<p>if <strong>name</strong> == '<strong>main</strong>':<br>
try:<br>
main()<br>
except KeyboardInterrupt:<br>
pass</p>
<p></p>
</details> 
<blockquote>
<p>代码使用cifer10数据集, resnet18作为模型训练的简单的流程</p>
</blockquote>
<p>有了流程接下来编写我们的训练代码</p>
<details><summary>train.py(代码未整理完毕, 非初版代码, 仅供参考)</summary>
<p>
</p><div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">import</span> <span class="pl-s1">os</span>
<span class="pl-k">import</span> <span class="pl-s1">random</span>

<span class="pl-k">import</span> <span class="pl-s1">cv2</span>
<span class="pl-k">import</span> <span class="pl-s1">numpy</span> <span class="pl-k">as</span> <span class="pl-s1">np</span>
<span class="pl-k">from</span> <span class="pl-s1">sklearn</span>.<span class="pl-s1">metrics</span> <span class="pl-k">import</span> <span class="pl-s1">f1_score</span>
<span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">from</span> <span class="pl-v">PIL</span> <span class="pl-k">import</span> <span class="pl-v">Image</span>
<span class="pl-k">from</span> <span class="pl-s1">torch</span>.<span class="pl-s1">optim</span>.<span class="pl-s1">lr_scheduler</span> <span class="pl-k">import</span> <span class="pl-v">LambdaLR</span>
<span class="pl-k">from</span> <span class="pl-s1">torch</span>.<span class="pl-s1">utils</span>.<span class="pl-s1">data</span> <span class="pl-k">import</span> <span class="pl-v">DataLoader</span>, <span class="pl-v">Dataset</span>
<span class="pl-k">from</span> <span class="pl-s1">datasets</span> <span class="pl-k">import</span> <span class="pl-s1">load_dataset</span>
<span class="pl-k">from</span> <span class="pl-s1">torchvision</span> <span class="pl-k">import</span> <span class="pl-s1">transforms</span>

<span class="pl-c"># from torchvision.models import resnet18</span>
<span class="pl-k">from</span> <span class="pl-s1">model</span> <span class="pl-k">import</span> <span class="pl-s1">convnext_base</span>
<span class="pl-k">from</span> <span class="pl-s1">trainer</span> <span class="pl-k">import</span> <span class="pl-v">Trainer</span>

<span class="pl-s1">image_size</span> <span class="pl-c1">=</span> <span class="pl-c1">224</span>
<span class="pl-s1">batch_size</span> <span class="pl-c1">=</span> <span class="pl-c1">32</span>
<span class="pl-s1">device</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-en">device</span>(<span class="pl-s">"cuda"</span>) <span class="pl-k">if</span> <span class="pl-s1">torch</span>.<span class="pl-s1">cuda</span>.<span class="pl-en">is_available</span>() <span class="pl-k">else</span> <span class="pl-s1">torch</span>.<span class="pl-en">device</span>(<span class="pl-s">"cpu"</span>)


<span class="pl-c"># def get_color_from_image(image_path):</span>
<span class="pl-c">#     """</span>
<span class="pl-c">#     从纯色图片中获取RGB颜色值</span>
<span class="pl-c">#     返回: (R, G, B)元组</span>
<span class="pl-c">#     """</span>
<span class="pl-c">#     # 读取图片</span>
<span class="pl-c">#     image = Image.open(image_path).convert('RGB')</span>
<span class="pl-c">#     # 转换为numpy数组</span>
<span class="pl-c">#     img_array = np.array(image)</span>
    
<span class="pl-c">#     # 获取图片中心点的颜色值</span>
<span class="pl-c">#     h, w = img_array.shape[:2]</span>
<span class="pl-c">#     center_color = img_array[h//2, w//2]</span>
    
<span class="pl-c">#     # 或者计算整个图片的平均颜色</span>
<span class="pl-c">#     average_color = img_array.mean(axis=(0,1)).astype(int)</span>
    
<span class="pl-c">#     return tuple(average_color)  # 或者 tuple(average_color)</span>


<span class="pl-c"># class AugmentationUtils:</span>
<span class="pl-c">#     @staticmethod</span>
<span class="pl-c">#     def add_color_mask(image, is_positive):</span>
<span class="pl-c">#         """给图片添加颜色遮罩"""</span>
<span class="pl-c">#         # 转换为numpy数组并确保类型为uint8</span>
<span class="pl-c">#         image = np.array(image, dtype=np.uint8)</span>
        
<span class="pl-c">#         # 创建与图像相同大小的遮罩</span>
<span class="pl-c">#         mask = np.ones_like(image, dtype=np.uint8)</span>
        
<span class="pl-c">#         # 随机生成颜色</span>
<span class="pl-c">#         if is_positive:</span>
<span class="pl-c">#             color = [random.randint(0, 255) for _ in range(3)]</span>
<span class="pl-c">#         else:</span>
<span class="pl-c">#             color = get_color_from_image('22.png')</span>
        
<span class="pl-c">#         # 为遮罩赋予颜色    </span>
<span class="pl-c">#         for i in range(3):</span>
<span class="pl-c">#             mask[:, :, i] = color[i]</span>
        
<span class="pl-c">#         # 确保mask也是uint8类型</span>
<span class="pl-c">#         mask = mask.astype(np.uint8)</span>
        
<span class="pl-c">#         # 添加遮罩</span>
<span class="pl-c">#         alpha = 0.5  # 透明度</span>
<span class="pl-c">#         image = cv2.addWeighted(image, 1-alpha, mask, alpha, 0)</span>
        
<span class="pl-c">#         return Image.fromarray(image)</span>

<span class="pl-c">#     @staticmethod</span>
<span class="pl-c">#     def embed_positive_in_negative(positive_img, negative_img):</span>
<span class="pl-c">#         """在负样本中嵌入正样本"""</span>
<span class="pl-c">#         # 转换为numpy数组</span>
<span class="pl-c">#         pos_img = np.array(positive_img)</span>
<span class="pl-c">#         neg_img = np.array(negative_img)</span>
        
<span class="pl-c">#         # 确保图像是3通道的</span>
<span class="pl-c">#         if len(pos_img.shape) == 2:</span>
<span class="pl-c">#             pos_img = cv2.cvtColor(pos_img, cv2.COLOR_GRAY2BGR)</span>
<span class="pl-c">#         if len(neg_img.shape) == 2:</span>
<span class="pl-c">#             neg_img = cv2.cvtColor(neg_img, cv2.COLOR_GRAY2BGR)</span>
        
<span class="pl-c">#         # 获取负样本尺寸</span>
<span class="pl-c">#         h, w = neg_img.shape[:2]</span>
<span class="pl-c">#         pos_h, pos_w = pos_img.shape[:2]</span>
        
<span class="pl-c">#         # 计算合适的缩放比例</span>
<span class="pl-c">#         scale = min(</span>
<span class="pl-c">#             random.uniform(0.5, 0.8),</span>
<span class="pl-c">#             (w * 0.8) / pos_w,</span>
<span class="pl-c">#             (h * 0.8) / pos_h</span>
<span class="pl-c">#         )</span>
        
<span class="pl-c">#         # 缩放正样本</span>
<span class="pl-c">#         new_size = (int(pos_w * scale), int(pos_h * scale))</span>
<span class="pl-c">#         pos_img_resized = cv2.resize(pos_img, new_size)</span>
        
<span class="pl-c">#         # 确保有效的随机位置范围</span>
<span class="pl-c">#         max_x = max(0, w - new_size[0])</span>
<span class="pl-c">#         max_y = max(0, h - new_size[1])</span>
        
<span class="pl-c">#         # 随机选择插入位置</span>
<span class="pl-c">#         x = random.randint(0, max_x) if max_x &gt; 0 else 0</span>
<span class="pl-c">#         y = random.randint(0, max_y) if max_y &gt; 0 else 0</span>
        
<span class="pl-c">#         # 获取ROI区域并确保与缩放后的正样本具有相同的形状</span>
<span class="pl-c">#         roi = neg_img[y:y+new_size[1], x:x+new_size[0]]</span>
        
<span class="pl-c">#         # 确保ROI和pos_img_resized具有相同的形状和通道数</span>
<span class="pl-c">#         if roi.shape == pos_img_resized.shape:</span>
<span class="pl-c">#             # 混合图像</span>
<span class="pl-c">#             blended = cv2.addWeighted(roi, 0.3, pos_img_resized, 0.7, 0)</span>
<span class="pl-c">#             neg_img[y:y+new_size[1], x:x+new_size[0]] = blended</span>
        
<span class="pl-c">#         return Image.fromarray(neg_img)</span>
    
<span class="pl-c">#     @staticmethod</span>
<span class="pl-c">#     def embed_same(positive_img, negative_img):</span>
<span class="pl-c">#         """在负样本中嵌入正样本"""</span>
<span class="pl-c">#         # 转换为numpy数组</span>
<span class="pl-c">#         pos_img = np.array(positive_img)</span>
<span class="pl-c">#         neg_img = np.array(negative_img)</span>
        
<span class="pl-c">#         # 确保图像是3通道的</span>
<span class="pl-c">#         if len(pos_img.shape) == 2:</span>
<span class="pl-c">#             pos_img = cv2.cvtColor(pos_img, cv2.COLOR_GRAY2BGR)</span>
<span class="pl-c">#         if len(neg_img.shape) == 2:</span>
<span class="pl-c">#             neg_img = cv2.cvtColor(neg_img, cv2.COLOR_GRAY2BGR)</span>
        
<span class="pl-c">#         # 获取负样本尺寸</span>
<span class="pl-c">#         h, w = neg_img.shape[:2]</span>
<span class="pl-c">#         pos_h, pos_w = pos_img.shape[:2]</span>
        
<span class="pl-c">#         # 计算合适的缩放比例</span>
<span class="pl-c">#         scale = min(</span>
<span class="pl-c">#             random.uniform(0.5, 0.8),</span>
<span class="pl-c">#             (w * 0.8) / pos_w,</span>
<span class="pl-c">#             (h * 0.8) / pos_h</span>
<span class="pl-c">#         )</span>
        
<span class="pl-c">#         # 缩放正样本</span>
<span class="pl-c">#         new_size = (int(pos_w * scale), int(pos_h * scale))</span>
<span class="pl-c">#         pos_img_resized = cv2.resize(pos_img, new_size)</span>
        
<span class="pl-c">#         # 确保有效的随机位置范围</span>
<span class="pl-c">#         max_x = max(0, w - new_size[0])</span>
<span class="pl-c">#         max_y = max(0, h - new_size[1])</span>
        
<span class="pl-c">#         # 随机选择插入位置</span>
<span class="pl-c">#         x = random.randint(0, max_x) if max_x &gt; 0 else 0</span>
<span class="pl-c">#         y = random.randint(0, max_y) if max_y &gt; 0 else 0</span>
        
<span class="pl-c">#         # 获取ROI区域并确保与缩放后的正样本具有相同的形状</span>
<span class="pl-c">#         roi = neg_img[y:y+new_size[1], x:x+new_size[0]]</span>
        
<span class="pl-c">#         # 确保ROI和pos_img_resized具有相同的形状和通道数</span>
<span class="pl-c">#         if roi.shape == pos_img_resized.shape:</span>
<span class="pl-c">#             # 混合图像</span>
<span class="pl-c">#             blended = cv2.addWeighted(roi, 0.3, pos_img_resized, 0.7, 0)</span>
<span class="pl-c">#             neg_img[y:y+new_size[1], x:x+new_size[0]] = blended</span>
        
<span class="pl-c">#         return Image.fromarray(neg_img)</span>

<span class="pl-c">#     @staticmethod</span>
<span class="pl-c">#     def flip_image(image):</span>
<span class="pl-c">#         """图片轴对称"""</span>
<span class="pl-c">#         return Image.fromarray(np.array(image)[:, ::-1])</span>
    
<span class="pl-c">#     @staticmethod</span>
<span class="pl-c">#     def mirror_half_image(image):</span>
<span class="pl-c">#         img_array = np.array(image)</span>
    
<span class="pl-c">#         # 获取图片尺寸</span>
<span class="pl-c">#         h, w = img_array.shape[:2]</span>
        
<span class="pl-c">#         # 取左半边</span>
<span class="pl-c">#         half_w = w // 2</span>
<span class="pl-c">#         left_half = img_array[:, :half_w]</span>
        
<span class="pl-c">#         # 水平翻转左半边得到右半边</span>
<span class="pl-c">#         right_half = left_half[:, ::-1]</span>
        
<span class="pl-c">#         # 拼接两个半边</span>
<span class="pl-c">#         mirrored = np.concatenate([left_half, right_half], axis=1)</span>
        
<span class="pl-c">#         return Image.fromarray(mirrored)</span>
    

<span class="pl-c"># def augment_dataset(positive_images, negative_images):</span>
<span class="pl-c">#     aug_utils = AugmentationUtils()</span>
<span class="pl-c">#     augmented_data = []</span>
    
<span class="pl-c">#     # 增强正样本</span>
<span class="pl-c">#     for pos_img in positive_images:</span>
<span class="pl-c">#         img = Image.open(pos_img).convert('RGB')</span>
<span class="pl-c">#         # 原图</span>
<span class="pl-c">#         augmented_data.append((img, 1))</span>
<span class="pl-c">#         # 颜色遮罩</span>
<span class="pl-c">#         augmented_data.append((aug_utils.add_color_mask(img, True), 1))</span>
<span class="pl-c">#         # 轴对称</span>
<span class="pl-c">#         augmented_data.append((aug_utils.flip_image(img), 1))</span>
<span class="pl-c">#         # 镜像一半</span>
<span class="pl-c">#         augmented_data.append((aug_utils.mirror_half_image(img), 1))</span>
<span class="pl-c">#         # 嵌入相同</span>
<span class="pl-c">#         img_id = random.randint(0, len(positive_images)-1)</span>
<span class="pl-c">#         aaa = Image.open(positive_images[img_id]).convert('RGB')</span>
<span class="pl-c">#         augmented_data.append((aug_utils.embed_same(aaa, img), 1))</span>
        
    
<span class="pl-c">#     # 增强负样本</span>
<span class="pl-c">#     for i, neg_img in enumerate(negative_images):</span>
<span class="pl-c">#         img = Image.open(neg_img).convert('RGB')</span>
<span class="pl-c">#         # 原图</span>
<span class="pl-c">#         augmented_data.append((img, 0))</span>
<span class="pl-c">#         # 颜色遮罩</span>
<span class="pl-c">#         augmented_data.append((aug_utils.add_color_mask(img, False), 0))</span>
<span class="pl-c">#         # 镜像一半</span>
<span class="pl-c">#         augmented_data.append((aug_utils.mirror_half_image(img), 0))</span>
<span class="pl-c">#         # 嵌入正样本</span>
<span class="pl-c">#         pos_img = Image.open(positive_images[random.randint(0, len(positive_images)-1)]).convert('RGB')</span>
<span class="pl-c">#         augmented_data.append((aug_utils.embed_positive_in_negative(pos_img, img), 1))</span>
<span class="pl-c">#         # 嵌入相同</span>
<span class="pl-c">#         img_id = random.randint(0, len(negative_images)-1)</span>
<span class="pl-c">#         aaa = Image.open(negative_images[img_id]).convert('RGB')</span>
<span class="pl-c">#         augmented_data.append((aug_utils.embed_same(aaa, img), 0))</span>
        

        
<span class="pl-c">#     # # 显示并保存</span>
<span class="pl-c">#     # for i, (img, label) in enumerate(augmented_data):</span>
<span class="pl-c">#     #     # img.show()</span>
<span class="pl-c">#     #     os.makedirs('aug_images', exist_ok=True)</span>
<span class="pl-c">#     #     img.save(f'aug_images/aug_{i}.jpg')</span>
    
<span class="pl-c">#     # 统计</span>
<span class="pl-c">#     print(f"Positive: {len([x for x, y in augmented_data if y == 1])}, Negative: {len([x for x, y in augmented_data if y == 0])}")</span>
<span class="pl-c">#     return augmented_data</span>


<span class="pl-k">class</span> <span class="pl-v">LinearWarmUpCosineAnnealingLR</span>(<span class="pl-v">LambdaLR</span>):
    <span class="pl-k">def</span> <span class="pl-en">__init__</span>(<span class="pl-s1">self</span>, <span class="pl-s1">optimizer</span>, <span class="pl-c1">*</span>, <span class="pl-s1">warmup_iters</span>, <span class="pl-s1">max_learning_rate</span>, <span class="pl-s1">min_lr</span>, <span class="pl-s1">lr_decay_iters</span>, <span class="pl-c1">**</span><span class="pl-s1">kwargs</span>):
        <span class="pl-s1">self</span>.<span class="pl-s1">warmup_iters</span> <span class="pl-c1">=</span> <span class="pl-s1">warmup_iters</span>
        <span class="pl-s1">self</span>.<span class="pl-s1">max_learning_rate</span> <span class="pl-c1">=</span> <span class="pl-s1">max_learning_rate</span>
        <span class="pl-s1">self</span>.<span class="pl-s1">lr_decay_iters</span> <span class="pl-c1">=</span> <span class="pl-s1">lr_decay_iters</span>
        <span class="pl-s1">self</span>.<span class="pl-s1">min_lr</span> <span class="pl-c1">=</span> <span class="pl-s1">min_lr</span>
        <span class="pl-s1">kwargs</span>[<span class="pl-s">'optimizer'</span>] <span class="pl-c1">=</span> <span class="pl-s1">optimizer</span>
        <span class="pl-s1">kwargs</span>[<span class="pl-s">'lr_lambda'</span>] <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-s1">_step_inner</span>
        <span class="pl-en">super</span>().<span class="pl-en">__init__</span>(<span class="pl-c1">**</span><span class="pl-s1">kwargs</span>)

    <span class="pl-k">def</span> <span class="pl-en">_step_inner</span>(<span class="pl-s1">self</span>, <span class="pl-s1">steps</span>):
        <span class="pl-k">if</span> <span class="pl-s1">steps</span> <span class="pl-c1">&lt;</span> <span class="pl-s1">self</span>.<span class="pl-s1">warmup_iters</span>:
            <span class="pl-k">return</span> <span class="pl-s1">self</span>.<span class="pl-s1">max_learning_rate</span> <span class="pl-c1">*</span> <span class="pl-s1">steps</span> <span class="pl-c1">/</span> <span class="pl-s1">self</span>.<span class="pl-s1">warmup_iters</span>
        <span class="pl-k">elif</span> <span class="pl-s1">steps</span> <span class="pl-c1">&lt;</span> <span class="pl-s1">self</span>.<span class="pl-s1">lr_decay_iters</span>:
            <span class="pl-k">return</span> <span class="pl-s1">self</span>.<span class="pl-s1">min_lr</span> <span class="pl-c1">+</span> <span class="pl-c1">0.5</span> <span class="pl-c1">*</span> (<span class="pl-c1">1.0</span> <span class="pl-c1">+</span> <span class="pl-s1">np</span>.<span class="pl-en">cos</span>((<span class="pl-s1">steps</span> <span class="pl-c1">-</span> <span class="pl-s1">self</span>.<span class="pl-s1">warmup_iters</span>) <span class="pl-c1">/</span> (<span class="pl-s1">self</span>.<span class="pl-s1">lr_decay_iters</span> <span class="pl-c1">-</span> <span class="pl-s1">self</span>.<span class="pl-s1">warmup_iters</span>)<span class="pl-c1">*</span><span class="pl-s1">np</span>.<span class="pl-s1">pi</span>)) <span class="pl-c1">*</span> (<span class="pl-s1">self</span>.<span class="pl-s1">max_learning_rate</span> <span class="pl-c1">-</span> <span class="pl-s1">self</span>.<span class="pl-s1">min_lr</span>)
        <span class="pl-k">else</span>:
            <span class="pl-k">return</span> <span class="pl-s1">self</span>.<span class="pl-s1">min_lr</span>


<span class="pl-k">def</span> <span class="pl-en">transform_img</span>(<span class="pl-s1">img</span>):
    <span class="pl-c"># 处理图片</span>
    <span class="pl-s1">img_np</span> <span class="pl-c1">=</span> <span class="pl-s1">np</span>.<span class="pl-en">array</span>(<span class="pl-s1">img</span>)
    <span class="pl-s1">img_tensor</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-en">from_numpy</span>(<span class="pl-s1">img_np</span>).<span class="pl-en">permute</span>(<span class="pl-c1">2</span>, <span class="pl-c1">0</span>, <span class="pl-c1">1</span>)  <span class="pl-c"># C, H, W</span>
    <span class="pl-s1">img_tensor</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-s1">nn</span>.<span class="pl-s1">functional</span>.<span class="pl-en">interpolate</span>(<span class="pl-s1">img_tensor</span>.<span class="pl-en">unsqueeze</span>(<span class="pl-c1">0</span>), <span class="pl-s1">size</span><span class="pl-c1">=</span>(<span class="pl-s1">image_size</span>, <span class="pl-s1">image_size</span>), <span class="pl-s1">mode</span><span class="pl-c1">=</span><span class="pl-s">'bilinear'</span>, <span class="pl-s1">align_corners</span><span class="pl-c1">=</span><span class="pl-c1">False</span>).<span class="pl-en">squeeze</span>(<span class="pl-c1">0</span>)
    <span class="pl-c"># normalize</span>
    <span class="pl-s1">normalized_img</span> <span class="pl-c1">=</span> <span class="pl-s1">img_tensor</span>.<span class="pl-en">float</span>() <span class="pl-c1">/</span> <span class="pl-c1">255.0</span>
    <span class="pl-k">return</span> <span class="pl-s1">normalized_img</span>


<span class="pl-s1">transform</span> <span class="pl-c1">=</span> <span class="pl-s1">transforms</span>.<span class="pl-v">Compose</span>([
    <span class="pl-s1">transforms</span>.<span class="pl-v">Resize</span>((<span class="pl-s1">image_size</span>, <span class="pl-s1">image_size</span>)),
    <span class="pl-s1">transforms</span>.<span class="pl-v">ToTensor</span>(),
    <span class="pl-s1">transforms</span>.<span class="pl-v">Normalize</span>(<span class="pl-s1">mean</span><span class="pl-c1">=</span>[<span class="pl-c1">0.485</span>, <span class="pl-c1">0.456</span>, <span class="pl-c1">0.406</span>], <span class="pl-s1">std</span><span class="pl-c1">=</span>[<span class="pl-c1">0.229</span>, <span class="pl-c1">0.224</span>, <span class="pl-c1">0.225</span>]),
])


<span class="pl-k">def</span> <span class="pl-en">transform_img_torchvision</span>(<span class="pl-s1">data</span>):
    <span class="pl-s1">data</span>[<span class="pl-s">'x'</span>] <span class="pl-c1">=</span> [<span class="pl-en">transform</span>(<span class="pl-s1">img</span>.<span class="pl-en">convert</span>(<span class="pl-s">'RGB'</span>)) <span class="pl-k">for</span> <span class="pl-s1">img</span> <span class="pl-c1">in</span> <span class="pl-s1">data</span>[<span class="pl-s">'image'</span>]]
    <span class="pl-k">return</span> <span class="pl-s1">data</span>


<span class="pl-s1">label_mapping</span> <span class="pl-c1">=</span> {
    <span class="pl-s">"nailong"</span>: <span class="pl-c1">0</span>,
    <span class="pl-s">"emoji"</span>: <span class="pl-c1">1</span>,
    <span class="pl-s">"anime"</span>: <span class="pl-c1">2</span>,
    <span class="pl-s">"others"</span>: <span class="pl-c1">3</span>,
    <span class="pl-s">"long"</span>: <span class="pl-c1">4</span>
}

<span class="pl-k">def</span> <span class="pl-en">extract_datasets</span>():
    <span class="pl-s1">ds</span> <span class="pl-c1">=</span> <span class="pl-en">load_dataset</span>(<span class="pl-s">"refoundd/NailongClassification"</span>, <span class="pl-s1">cache_dir</span><span class="pl-c1">=</span><span class="pl-s">"data"</span>, <span class="pl-s1">split</span><span class="pl-c1">=</span><span class="pl-s">"train"</span>)
    <span class="pl-s1">ds</span> <span class="pl-c1">=</span> <span class="pl-s1">ds</span>.<span class="pl-en">map</span>(<span class="pl-k">lambda</span> <span class="pl-s1">x</span>: {<span class="pl-s">'label'</span>: <span class="pl-s1">label_mapping</span>[<span class="pl-s1">x</span>[<span class="pl-s">'label'</span>]]})
    <span class="pl-s1">ds</span> <span class="pl-c1">=</span> <span class="pl-s1">ds</span>.<span class="pl-en">map</span>(<span class="pl-s1">transform_img_torchvision</span>, <span class="pl-s1">remove_columns</span><span class="pl-c1">=</span>[<span class="pl-s">'image'</span>], <span class="pl-s1">batched</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
    <span class="pl-s1">dataset</span> <span class="pl-c1">=</span> <span class="pl-s1">ds</span>.<span class="pl-en">train_test_split</span>(<span class="pl-s1">test_size</span><span class="pl-c1">=</span><span class="pl-c1">0.2</span>)
    <span class="pl-k">return</span> <span class="pl-s1">dataset</span>

<span class="pl-s1">dataset</span> <span class="pl-c1">=</span> <span class="pl-en">extract_datasets</span>()


<span class="pl-k">class</span> <span class="pl-v">NaiLongDataset</span>(<span class="pl-v">Dataset</span>):
    <span class="pl-k">def</span> <span class="pl-en">__init__</span>(<span class="pl-s1">self</span>, <span class="pl-s1">mode</span><span class="pl-c1">=</span><span class="pl-s">'train'</span>):
        <span class="pl-k">assert</span> <span class="pl-s1">mode</span> <span class="pl-c1">in</span> [<span class="pl-s">'train'</span>, <span class="pl-s">'test'</span>]
        <span class="pl-s1">self</span>.<span class="pl-s1">dataset</span> <span class="pl-c1">=</span> <span class="pl-s1">dataset</span>[<span class="pl-s1">mode</span>]

    <span class="pl-k">def</span> <span class="pl-en">__len__</span>(<span class="pl-s1">self</span>):
        <span class="pl-k">return</span> <span class="pl-en">len</span>(<span class="pl-s1">self</span>.<span class="pl-s1">dataset</span>)

    <span class="pl-k">def</span> <span class="pl-en">__getitem__</span>(<span class="pl-s1">self</span>, <span class="pl-s1">idx</span>):
        <span class="pl-s1">item</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-s1">dataset</span>[<span class="pl-s1">idx</span>][<span class="pl-s">'x'</span>]
        <span class="pl-s1">label</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-s1">dataset</span>[<span class="pl-s1">idx</span>][<span class="pl-s">'label'</span>]
        <span class="pl-k">return</span> <span class="pl-s1">torch</span>.<span class="pl-en">tensor</span>(<span class="pl-s1">item</span>), <span class="pl-s1">torch</span>.<span class="pl-en">tensor</span>(<span class="pl-s1">label</span>)



<span class="pl-k">class</span> <span class="pl-v">NaiLongTrainer</span>(<span class="pl-v">Trainer</span>):
    <span class="pl-k">def</span> <span class="pl-en">__init__</span>(<span class="pl-s1">self</span>, <span class="pl-s1">config</span>):
        <span class="pl-en">super</span>().<span class="pl-en">__init__</span>(<span class="pl-s1">config</span>)

    <span class="pl-k">def</span> <span class="pl-en">build_model</span>(<span class="pl-s1">self</span>):
        <span class="pl-c"># model = resnet18()</span>
        <span class="pl-c"># model.fc = torch.nn.Linear(model.fc.in_features, 2)</span>
        <span class="pl-c"># return model</span>
        <span class="pl-k">return</span> <span class="pl-en">convnext_base</span>(<span class="pl-s1">pretrained</span><span class="pl-c1">=</span><span class="pl-c1">False</span>, <span class="pl-s1">num_classes</span><span class="pl-c1">=</span><span class="pl-c1">5</span>)
    
    <span class="pl-k">def</span> <span class="pl-en">build_criterion</span>(<span class="pl-s1">self</span>):
        <span class="pl-k">return</span> <span class="pl-s1">torch</span>.<span class="pl-s1">nn</span>.<span class="pl-v">CrossEntropyLoss</span>()
    
    <span class="pl-k">def</span> <span class="pl-en">build_scheduler</span>(<span class="pl-s1">self</span>):
        <span class="pl-k">return</span> <span class="pl-v">LinearWarmUpCosineAnnealingLR</span>(<span class="pl-s1">self</span>.<span class="pl-s1">optimizer</span>, <span class="pl-s1">warmup_iters</span><span class="pl-c1">=</span><span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'warmup_iters'</span>], <span class="pl-s1">max_learning_rate</span><span class="pl-c1">=</span><span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'max_learning_rate'</span>], <span class="pl-s1">min_lr</span><span class="pl-c1">=</span><span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'min_lr'</span>], <span class="pl-s1">lr_decay_iters</span><span class="pl-c1">=</span><span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'lr_decay_iters'</span>])
    
    <span class="pl-k">def</span> <span class="pl-en">build_dataloader</span>(<span class="pl-s1">self</span>, <span class="pl-s1">mode</span><span class="pl-c1">=</span><span class="pl-s">'train'</span>):
        <span class="pl-s1">dataset</span> <span class="pl-c1">=</span> <span class="pl-v">NaiLongDataset</span>(<span class="pl-s1">mode</span><span class="pl-c1">=</span><span class="pl-s">"train"</span>)
        <span class="pl-k">return</span> <span class="pl-v">DataLoader</span>(<span class="pl-s1">dataset</span>, <span class="pl-s1">batch_size</span><span class="pl-c1">=</span><span class="pl-s1">batch_size</span>, <span class="pl-s1">shuffle</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
    
    <span class="pl-k">def</span> <span class="pl-en">train_step</span>(<span class="pl-s1">self</span>, <span class="pl-s1">batch</span>):
        <span class="pl-s1">x</span>, <span class="pl-s1">y</span> <span class="pl-c1">=</span> <span class="pl-s1">batch</span>
        <span class="pl-s1">x</span>, <span class="pl-s1">y</span> <span class="pl-c1">=</span> <span class="pl-s1">x</span>.<span class="pl-en">to</span>(<span class="pl-s1">device</span>), <span class="pl-s1">y</span>.<span class="pl-en">to</span>(<span class="pl-s1">device</span>)
        <span class="pl-k">return</span> <span class="pl-s1">self</span>.<span class="pl-en">criterion</span>(<span class="pl-s1">self</span>.<span class="pl-en">model</span>(<span class="pl-s1">x</span>), <span class="pl-s1">y</span>)
    
    <span class="pl-k">def</span> <span class="pl-en">validate</span>(<span class="pl-s1">self</span>):
        <span class="pl-s1">self</span>.<span class="pl-s1">logger</span>.<span class="pl-en">info</span>(<span class="pl-s">"Validating..."</span>)
        <span class="pl-s1">self</span>.<span class="pl-s1">model</span>.<span class="pl-en">eval</span>()
        <span class="pl-s1">dataloader</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-en">build_dataloader</span>(<span class="pl-s1">mode</span><span class="pl-c1">=</span><span class="pl-s">'test'</span>)
        <span class="pl-s1">acc</span> <span class="pl-c1">=</span> []
        <span class="pl-s1">f1</span> <span class="pl-c1">=</span> [[], []]
        <span class="pl-k">with</span> <span class="pl-s1">torch</span>.<span class="pl-en">no_grad</span>(): 
            <span class="pl-k">for</span> <span class="pl-s1">i</span>, (<span class="pl-s1">x</span>, <span class="pl-s1">y</span>) <span class="pl-c1">in</span> <span class="pl-en">enumerate</span>(<span class="pl-s1">dataloader</span>):
                <span class="pl-s1">x</span>, <span class="pl-s1">y</span> <span class="pl-c1">=</span> <span class="pl-s1">x</span>.<span class="pl-en">to</span>(<span class="pl-s1">device</span>), <span class="pl-s1">y</span>.<span class="pl-en">to</span>(<span class="pl-s1">device</span>)
                <span class="pl-c"># print(f"Validation: {i}, {y}")</span>
                <span class="pl-s1">y_hat</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-en">model</span>(<span class="pl-s1">x</span>)
                <span class="pl-s1">acc</span>.<span class="pl-en">append</span>(<span class="pl-s1">torch</span>.<span class="pl-en">sum</span>(<span class="pl-s1">torch</span>.<span class="pl-en">argmax</span>(<span class="pl-s1">y_hat</span>, <span class="pl-s1">dim</span><span class="pl-c1">=</span><span class="pl-c1">1</span>) <span class="pl-c1">==</span> <span class="pl-s1">y</span>).<span class="pl-en">item</span>() <span class="pl-c1">/</span> <span class="pl-en">len</span>(<span class="pl-s1">y</span>))
                <span class="pl-s1">f1</span>[<span class="pl-c1">0</span>].<span class="pl-en">extend</span>(<span class="pl-s1">y</span>.<span class="pl-en">cpu</span>().<span class="pl-en">tolist</span>())
                <span class="pl-s1">f1</span>[<span class="pl-c1">1</span>].<span class="pl-en">extend</span>(<span class="pl-s1">torch</span>.<span class="pl-en">argmax</span>(<span class="pl-s1">y_hat</span>, <span class="pl-s1">dim</span><span class="pl-c1">=</span><span class="pl-c1">1</span>).<span class="pl-en">cpu</span>().<span class="pl-en">tolist</span>())
            <span class="pl-s1">f1_scores</span> <span class="pl-c1">=</span> <span class="pl-en">f1_score</span>(<span class="pl-s1">f1</span>[<span class="pl-c1">0</span>], <span class="pl-s1">f1</span>[<span class="pl-c1">1</span>], <span class="pl-s1">average</span><span class="pl-c1">=</span><span class="pl-s">'macro'</span>)
        <span class="pl-k">return</span> {<span class="pl-s">'acc'</span>: <span class="pl-en">sum</span>(<span class="pl-s1">acc</span>) <span class="pl-c1">/</span> <span class="pl-en">len</span>(<span class="pl-s1">acc</span>), <span class="pl-s">'f1'</span>: <span class="pl-s1">f1_scores</span>}


<span class="pl-k">def</span> <span class="pl-en">main</span>():
    <span class="pl-s1">config</span> <span class="pl-c1">=</span> {  <span class="pl-c"># test</span>
        <span class="pl-s">"model"</span>: <span class="pl-s">"convnext_tiny"</span>,
        <span class="pl-s">"checkpoint_dir"</span>: <span class="pl-s">"./checkpoints"</span>,
        <span class="pl-s">"tensorboard_dir"</span>: <span class="pl-s">"./tensorboard"</span>,
        <span class="pl-s">"device"</span>: <span class="pl-s">"cuda"</span>,
        <span class="pl-s">"enable_cudnn_benchmark"</span>: <span class="pl-c1">True</span>,
        <span class="pl-s">"enable_amp"</span>: <span class="pl-c1">False</span>,
        <span class="pl-s">"learning_rate"</span>: <span class="pl-c1">1</span>,  <span class="pl-c"># 启动lr_scheduler 这里必须是1</span>
        <span class="pl-s">"betas"</span>: [<span class="pl-c1">0.9</span>, <span class="pl-c1">0.999</span>],
        <span class="pl-s">"eps"</span>: <span class="pl-c1">1e-8</span>,
        <span class="pl-s">"enable_compile"</span>: <span class="pl-c1">False</span>,
        <span class="pl-s">"weight_decay"</span>: <span class="pl-c1">0.0</span>,
        <span class="pl-s">"max_steps"</span>: <span class="pl-c1">5000</span>,
        <span class="pl-s">"max_grad_norm"</span>: <span class="pl-c1">1.0</span>,
        <span class="pl-s">"save_every"</span>: <span class="pl-c1">500</span>,
        <span class="pl-s">"gradient_accumulation_steps"</span>: <span class="pl-c1">1</span>,
        <span class="pl-s">"warmup_iters"</span>: <span class="pl-c1">500</span>,
        <span class="pl-s">"max_learning_rate"</span>: <span class="pl-c1">1e-3</span>,
        <span class="pl-s">"min_lr"</span>: <span class="pl-c1">1e-4</span>,
        <span class="pl-s">'lr_decay_iters'</span>: <span class="pl-c1">1000</span>
    }
    <span class="pl-s1">os</span>.<span class="pl-en">makedirs</span>(<span class="pl-s1">config</span>[<span class="pl-s">'checkpoint_dir'</span>], <span class="pl-s1">exist_ok</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
    <span class="pl-s1">os</span>.<span class="pl-en">makedirs</span>(<span class="pl-s1">config</span>[<span class="pl-s">'tensorboard_dir'</span>], <span class="pl-s1">exist_ok</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
    <span class="pl-s1">trainer</span> <span class="pl-c1">=</span> <span class="pl-v">NaiLongTrainer</span>(<span class="pl-s1">config</span>)
    <span class="pl-s1">trainer</span>.<span class="pl-en">train</span>()

<span class="pl-k">if</span> <span class="pl-s1">__name__</span> <span class="pl-c1">==</span> <span class="pl-s">"__main__"</span>:
    <span class="pl-c"># 删除tensorboard下的文件, 但不删除文件夹</span>
    <span class="pl-k">for</span> <span class="pl-s1">i</span> <span class="pl-c1">in</span> <span class="pl-s1">os</span>.<span class="pl-en">listdir</span>(<span class="pl-s">'./tensorboard'</span>):
        <span class="pl-s1">os</span>.<span class="pl-en">remove</span>(<span class="pl-s1">os</span>.<span class="pl-s1">path</span>.<span class="pl-en">join</span>(<span class="pl-s">'./tensorboard'</span>, <span class="pl-s1">i</span>))
    <span class="pl-c"># 删除checkpoints下的文件</span>
    <span class="pl-k">for</span> <span class="pl-s1">i</span> <span class="pl-c1">in</span> <span class="pl-s1">os</span>.<span class="pl-en">listdir</span>(<span class="pl-s">'./checkpoints'</span>):
        <span class="pl-s1">os</span>.<span class="pl-en">remove</span>(<span class="pl-s1">os</span>.<span class="pl-s1">path</span>.<span class="pl-en">join</span>(<span class="pl-s">'./checkpoints'</span>, <span class="pl-s1">i</span>))
    <span class="pl-k">try</span>:
        <span class="pl-en">main</span>()
    <span class="pl-k">except</span> <span class="pl-v">KeyboardInterrupt</span>:
        <span class="pl-en">print</span>(<span class="pl-s">"KeyboardInterrupt"</span>)
        </pre></div>
<p></p>
</details> 
<h3>数据增广</h3>
<p>原始数据集只有两百多张图片, 这个时候无法避免的要做数据增广, 扩展 nailong 标签的数据, 这里因为是初版方案, 也没有非常精细的增广方案, 这里使用了以下几种方式(代码在如上train.py中):</p>
<ul>
<li>给图片添加颜色遮罩<br>
让模型不要将遇到黄色的就判定为奶龙</li>
<li>在负样本中嵌入正样本<br>
很经典的增广数据的手法</li>
<li>图片轴对称</li>
<li>取图像的一半镜像翻转</li>
</ul>
<h3>训练</h3>
<h4>参数搜索</h4>
<p>虽然是个人小项目, 简单的参数搜索不能少, 继续上面写的 <code class="notranslate">trainer.py</code>, 我也写了一个简单的 <code class="notranslate">hyperparameter_seacher.py</code> 来搜索超参</p>
<details><summary>hyperparameter_seacher.py</summary>
<p>
</p><div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">from</span> <span class="pl-s1">trainer</span> <span class="pl-k">import</span> <span class="pl-v">Trainer</span>
<span class="pl-k">import</span> <span class="pl-s1">optuna</span>


<span class="pl-s1">example_config</span> <span class="pl-c1">=</span> {
    <span class="pl-s">"model"</span>: <span class="pl-s">"convnext_tiny"</span>,
    <span class="pl-s">"checkpoint_dir"</span>: <span class="pl-s">"./checkpoints"</span>,
    <span class="pl-s">"tensorboard_dir"</span>: <span class="pl-s">"./tensorboard"</span>,
    <span class="pl-s">"device"</span>: <span class="pl-s">"cuda"</span>,
    <span class="pl-s">"enable_cudnn_benchmark"</span>: <span class="pl-c1">True</span>,
    <span class="pl-s">"enable_amp"</span>: <span class="pl-c1">False</span>,
    <span class="pl-s">"learning_rate"</span>: <span class="pl-c1">1e-4</span>,
    <span class="pl-s">"betas"</span>: [<span class="pl-c1">0.9</span>, <span class="pl-c1">0.999</span>],
    <span class="pl-s">"eps"</span>: <span class="pl-c1">1e-8</span>,
    <span class="pl-s">"enable_compile"</span>: <span class="pl-c1">False</span>,
    <span class="pl-s">"weight_decay"</span>: <span class="pl-c1">0.05</span>,
    <span class="pl-s">"max_steps"</span>: <span class="pl-c1">100</span>,
    <span class="pl-s">"max_grad_norm"</span>: <span class="pl-c1">1.0</span>,
    <span class="pl-s">"save_every"</span>: <span class="pl-c1">1000000</span>,  <span class="pl-c"># 不保存</span>
    <span class="pl-s">"gradient_accumulation_steps"</span>: <span class="pl-c1">4</span>
}

<span class="pl-s1">example_search_config</span> <span class="pl-c1">=</span> {
    <span class="pl-s">'params'</span>: {
        <span class="pl-s">"learning_rate"</span>: {
            <span class="pl-s">"type"</span>: <span class="pl-s">"float"</span>,
            <span class="pl-s">"range"</span>: [<span class="pl-c1">1e-5</span>, <span class="pl-c1">1e-2</span>],
            <span class="pl-s">"log"</span>: <span class="pl-c1">True</span>
        },
        <span class="pl-s">"gradient_accumulation_steps"</span>: {
            <span class="pl-s">"type"</span>: <span class="pl-s">"int"</span>,
            <span class="pl-s">"range"</span>: [<span class="pl-c1">1</span>, <span class="pl-c1">8</span>],
            <span class="pl-s">"log"</span>: <span class="pl-c1">False</span>
        }
    },
    <span class="pl-s">"if_save_info"</span>: <span class="pl-c1">False</span>,
    <span class="pl-s">"n_trials"</span>: <span class="pl-c1">10</span>
}

<span class="pl-k">class</span> <span class="pl-v">HyperparameterSearcher</span>:
    <span class="pl-k">def</span> <span class="pl-en">__init__</span>(<span class="pl-s1">self</span>, <span class="pl-s1">config</span>, <span class="pl-s1">trainer</span>):
        <span class="pl-k">assert</span> <span class="pl-en">isinstance</span>(<span class="pl-s1">trainer</span>, <span class="pl-v">Trainer</span>), <span class="pl-s">"trainer must be an instance of Trainer"</span>
        <span class="pl-s1">self</span>.<span class="pl-s1">config</span> <span class="pl-c1">=</span> <span class="pl-s1">config</span>
        <span class="pl-s1">self</span>.<span class="pl-s1">trainer</span> <span class="pl-c1">=</span> <span class="pl-s1">trainer</span>
        
    <span class="pl-k">def</span> <span class="pl-en">objective</span>(<span class="pl-s1">self</span>, <span class="pl-s1">trial</span>):
        <span class="pl-s1">search_params</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'params'</span>]
        
        <span class="pl-k">for</span> <span class="pl-s1">param_name</span>, <span class="pl-s1">param_config</span> <span class="pl-c1">in</span> <span class="pl-s1">search_params</span>.<span class="pl-en">items</span>():
            <span class="pl-k">if</span> <span class="pl-s1">param_config</span>[<span class="pl-s">"type"</span>] <span class="pl-c1">==</span> <span class="pl-s">"float"</span>:
                <span class="pl-s1">self</span>.<span class="pl-s1">trainer</span>.<span class="pl-s1">config</span>[<span class="pl-s1">param_name</span>] <span class="pl-c1">=</span> <span class="pl-s1">trial</span>.<span class="pl-en">suggest_float</span>(
                    <span class="pl-s1">param_name</span>,
                    <span class="pl-s1">param_config</span>[<span class="pl-s">"range"</span>][<span class="pl-c1">0</span>],
                    <span class="pl-s1">param_config</span>[<span class="pl-s">"range"</span>][<span class="pl-c1">1</span>],
                    <span class="pl-s1">log</span><span class="pl-c1">=</span><span class="pl-s1">param_config</span>.<span class="pl-en">get</span>(<span class="pl-s">"log"</span>, <span class="pl-c1">False</span>)
                )
            <span class="pl-k">elif</span> <span class="pl-s1">param_config</span>[<span class="pl-s">"type"</span>] <span class="pl-c1">==</span> <span class="pl-s">"int"</span>:
                <span class="pl-s1">self</span>.<span class="pl-s1">trainer</span>.<span class="pl-s1">config</span>[<span class="pl-s1">param_name</span>] <span class="pl-c1">=</span> <span class="pl-s1">trial</span>.<span class="pl-en">suggest_int</span>(
                    <span class="pl-s1">param_name</span>,
                    <span class="pl-s1">param_config</span>[<span class="pl-s">"range"</span>][<span class="pl-c1">0</span>],
                    <span class="pl-s1">param_config</span>[<span class="pl-s">"range"</span>][<span class="pl-c1">1</span>]
                )
            <span class="pl-k">elif</span> <span class="pl-s1">param_config</span>[<span class="pl-s">'type'</span>] <span class="pl-c1">==</span> <span class="pl-s">'list'</span>:
                <span class="pl-s1">self</span>.<span class="pl-s1">trainer</span>.<span class="pl-s1">config</span>[<span class="pl-s1">param_name</span>] <span class="pl-c1">=</span> <span class="pl-s1">trial</span>.<span class="pl-en">suggest_categorical</span>(
                    <span class="pl-s1">param_name</span>,
                    <span class="pl-s1">param_config</span>[<span class="pl-s">'range'</span>]
                )
            <span class="pl-k">else</span>:
                <span class="pl-k">raise</span> <span class="pl-v">ValueError</span>(<span class="pl-s">f"Unsupported parameter type: <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">param_config</span>[<span class="pl-s">'type'</span>]<span class="pl-kos">}</span></span>, only support float and int"</span>)
        
        <span class="pl-s1">self</span>.<span class="pl-s1">trainer</span>.<span class="pl-en">setup_training</span>()
        <span class="pl-s1">self</span>.<span class="pl-s1">trainer</span>.<span class="pl-en">train</span>()
        <span class="pl-s1">metric</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-s1">trainer</span>.<span class="pl-en">validate</span>()
        <span class="pl-k">if</span> <span class="pl-s">'acc'</span> <span class="pl-c1">not</span> <span class="pl-c1">in</span> <span class="pl-s1">metric</span>:
            <span class="pl-k">raise</span> <span class="pl-v">ValueError</span>(<span class="pl-s">"metric must contain 'acc'"</span>)
        <span class="pl-k">return</span> <span class="pl-c1">-</span><span class="pl-s1">metric</span>[<span class="pl-s">'acc'</span>]  <span class="pl-c"># only support maximizing acc</span>
    
    <span class="pl-k">def</span> <span class="pl-en">search</span>(<span class="pl-s1">self</span>):
        <span class="pl-s1">study</span> <span class="pl-c1">=</span> <span class="pl-s1">optuna</span>.<span class="pl-en">create_study</span>(<span class="pl-s1">direction</span><span class="pl-c1">=</span><span class="pl-s">"maximize"</span>)
        <span class="pl-s1">study</span>.<span class="pl-en">optimize</span>(<span class="pl-s1">self</span>.<span class="pl-s1">objective</span>, <span class="pl-s1">n_trials</span><span class="pl-c1">=</span><span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'n_trials'</span>])
        <span class="pl-en">print</span>(<span class="pl-s">"Best params:"</span>, <span class="pl-s1">study</span>.<span class="pl-s1">best_params</span>)
        <span class="pl-en">print</span>(<span class="pl-s">"Best value:"</span>, <span class="pl-c1">-</span><span class="pl-s1">study</span>.<span class="pl-s1">best_value</span>)
        <span class="pl-k">if</span> <span class="pl-s1">self</span>.<span class="pl-s1">config</span>[<span class="pl-s">'if_save_info'</span>]:
            <span class="pl-s1">study</span>.<span class="pl-en">trials_dataframe</span>().<span class="pl-en">to_csv</span>(<span class="pl-s">"./output/optuna_results.csv"</span>)
        <span class="pl-k">return</span> <span class="pl-s1">study</span>.<span class="pl-s1">best_params</span>
    
<span class="pl-k">def</span> <span class="pl-en">main</span>():
    
    <span class="pl-k">pass</span>

<span class="pl-k">if</span> <span class="pl-s1">__name__</span> <span class="pl-c1">==</span> <span class="pl-s">'__main__'</span>:
    <span class="pl-k">try</span>:
        <span class="pl-en">main</span>()
    <span class="pl-k">except</span> <span class="pl-v">KeyboardInterrupt</span>:
        <span class="pl-k">pass</span></pre></div>
<p></p>
</details> 
<p>超参搜索需要 trainer 的配合, 使用了与模型无关的 optuna 来跑给定范围的超参值, 这个时候可以给trainer一个比较容易训练的超参设置(短的epoch等), 同时关闭保存模式</p>
<p>我也写了个简单的超参搜索的例子</p>
<details><summary>hyperparameter_seacher使用案例</summary>
<p>
</p><p>from example_trainer import Cifer10Trainer<br>
from hyperparameter_seacher import HyperparameterSearcher</p>
<p>class Cifer10HyperparameterSearcher(HyperparameterSearcher):<br>
def <strong>init</strong>(self, config, trainer):<br>
super().<strong>init</strong>(config, trainer)</p>
<p>def main():<br>
search_config = {<br>
'params': {<br>
"learning_rate": {<br>
"type": "float",<br>
"range": [1e-5, 1e-2],<br>
"log": True<br>
},<br>
"gradient_accumulation_steps": {<br>
"type": "int",<br>
"range": [1, 8],<br>
"log": False<br>
}<br>
},<br>
"if_save_info": True,<br>
"n_trials": 10<br>
}</p>
<pre class="notranslate"><code class="notranslate">trainer_config = {
    "model": "resnet18",
    "checkpoint_dir": "./checkpoints",
    "tensorboard_dir": "./tensorboard",
    "device": "cuda",
    "enable_cudnn_benchmark": True,
    "enable_amp": False,
    "learning_rate": 1e-3,
    "betas": [0.9, 0.999],
    "eps": 1e-8,
    "enable_compile": False,
    "weight_decay": 0.05,
    "max_steps": 500,
    "max_grad_norm": 1.0,
    "save_every": 10000,  # large than max_steps, no save
    "gradient_accumulation_steps": 4,
    'batch_size': 32
}
trainer = Cifer10Trainer(trainer_config)
searcher = Cifer10HyperparameterSearcher(search_config, trainer)
best_params = searcher.search()
print(best_params)
</code></pre>
<p>if <strong>name</strong> == '<strong>main</strong>':<br>
main()</p>
<p></p>
</details> 
<blockquote>
<p>搜索上面那个例子中的合适的超参数</p>
</blockquote>
<p>在准备好这些后, 编写我们项目的超参搜索器</p>
<details><summary>之后补充</summary>
<p>
</p>
</details> 
<blockquote>
<p>搜索出来的最佳超参是一个很长的小数, 四舍五入合适的位数即可</p>
</blockquote>
<h4>第一次训练</h4>
<p>在准备好后, 开始第一次训练</p>
<p>在较新的GPU下, 训练以前较小的模型可谓降维打击, 不到一个小时训练完毕</p>
<p>然而, 第一个问题出来了</p>
<p>acc很高, f1很低</p>
<p>编写测试代码:</p>
<details><summary>test.py(非初版代码, 仅供参考)</summary>
<p>
</p><div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">from</span> <span class="pl-s1">sklearn</span>.<span class="pl-s1">metrics</span> <span class="pl-k">import</span> <span class="pl-s1">f1_score</span>
<span class="pl-k">import</span> <span class="pl-s1">torch</span>

<span class="pl-k">from</span> <span class="pl-s1">model</span> <span class="pl-k">import</span> <span class="pl-s1">convnext_base</span>
<span class="pl-k">from</span> <span class="pl-v">PIL</span> <span class="pl-k">import</span> <span class="pl-v">Image</span>
<span class="pl-k">import</span> <span class="pl-s1">numpy</span> <span class="pl-k">as</span> <span class="pl-s1">np</span>
<span class="pl-k">from</span> <span class="pl-s1">glob</span> <span class="pl-k">import</span> <span class="pl-s1">glob</span>
<span class="pl-k">from</span> <span class="pl-s1">torchvision</span> <span class="pl-k">import</span> <span class="pl-s1">transforms</span>

<span class="pl-s1">device</span> <span class="pl-c1">=</span> <span class="pl-s">"cuda"</span>
<span class="pl-s1">image_size</span> <span class="pl-c1">=</span> <span class="pl-c1">224</span>

<span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-en">convnext_base</span>(<span class="pl-s1">pretrained</span><span class="pl-c1">=</span><span class="pl-c1">False</span>, <span class="pl-s1">num_classes</span><span class="pl-c1">=</span><span class="pl-c1">5</span>).<span class="pl-en">to</span>(<span class="pl-s1">device</span>)
<span class="pl-c"># model = resnet18()</span>
<span class="pl-c"># model.fc = torch.nn.Linear(model.fc.in_features, 2)</span>
<span class="pl-s1">checkpoint</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-en">load</span>(<span class="pl-s">'./checkpoints/best.pt'</span>, <span class="pl-s1">map_location</span><span class="pl-c1">=</span><span class="pl-s1">device</span>)
<span class="pl-s1">model</span>.<span class="pl-en">load_state_dict</span>(<span class="pl-s1">checkpoint</span>[<span class="pl-s">'model'</span>])

<span class="pl-s1">transform</span> <span class="pl-c1">=</span> <span class="pl-s1">transforms</span>.<span class="pl-v">Compose</span>([
    <span class="pl-s1">transforms</span>.<span class="pl-v">Resize</span>((<span class="pl-s1">image_size</span>, <span class="pl-s1">image_size</span>)),
    <span class="pl-s1">transforms</span>.<span class="pl-v">ToTensor</span>(),
    <span class="pl-s1">transforms</span>.<span class="pl-v">Normalize</span>(<span class="pl-s1">mean</span><span class="pl-c1">=</span>[<span class="pl-c1">0.485</span>, <span class="pl-c1">0.456</span>, <span class="pl-c1">0.406</span>], <span class="pl-s1">std</span><span class="pl-c1">=</span>[<span class="pl-c1">0.229</span>, <span class="pl-c1">0.224</span>, <span class="pl-c1">0.225</span>]),
])

<span class="pl-k">def</span> <span class="pl-en">transform_img</span>(<span class="pl-s1">img</span>):
    <span class="pl-c"># 处理图片</span>
    <span class="pl-s1">img_np</span> <span class="pl-c1">=</span> <span class="pl-s1">np</span>.<span class="pl-en">array</span>(<span class="pl-s1">img</span>)
    <span class="pl-s1">img_tensor</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-en">from_numpy</span>(<span class="pl-s1">img_np</span>).<span class="pl-en">permute</span>(<span class="pl-c1">2</span>, <span class="pl-c1">0</span>, <span class="pl-c1">1</span>)  <span class="pl-c"># C, H, W</span>
    <span class="pl-s1">img_tensor</span> <span class="pl-c1">=</span> <span class="pl-s1">torch</span>.<span class="pl-s1">nn</span>.<span class="pl-s1">functional</span>.<span class="pl-en">interpolate</span>(<span class="pl-s1">img_tensor</span>.<span class="pl-en">unsqueeze</span>(<span class="pl-c1">0</span>), <span class="pl-s1">size</span><span class="pl-c1">=</span>(<span class="pl-s1">image_size</span>, <span class="pl-s1">image_size</span>), <span class="pl-s1">mode</span><span class="pl-c1">=</span><span class="pl-s">'bilinear'</span>, <span class="pl-s1">align_corners</span><span class="pl-c1">=</span><span class="pl-c1">False</span>).<span class="pl-en">squeeze</span>(<span class="pl-c1">0</span>)
    <span class="pl-c"># normalize</span>
    <span class="pl-s1">normalized_img</span> <span class="pl-c1">=</span> <span class="pl-s1">img_tensor</span>.<span class="pl-en">float</span>() <span class="pl-c1">/</span> <span class="pl-c1">255.0</span>
    <span class="pl-k">return</span> <span class="pl-s1">normalized_img</span>


<span class="pl-k">def</span> <span class="pl-en">get_input_images</span>(<span class="pl-s1">image_path</span>):
    <span class="pl-s1">img</span> <span class="pl-c1">=</span> <span class="pl-v">Image</span>.<span class="pl-en">open</span>(<span class="pl-s1">image_path</span>).<span class="pl-en">convert</span>(<span class="pl-s">"RGB"</span>)
    <span class="pl-s1">img</span> <span class="pl-c1">=</span> <span class="pl-en">transform</span>(<span class="pl-s1">img</span>)
    <span class="pl-k">return</span> <span class="pl-s1">torch</span>.<span class="pl-en">tensor</span>(<span class="pl-s1">img</span>).<span class="pl-en">to</span>(<span class="pl-s1">device</span>).<span class="pl-en">unsqueeze</span>(<span class="pl-c1">0</span>)

<span class="pl-s1">model</span>.<span class="pl-en">eval</span>()

<span class="pl-c"># 导出onnx</span>
<span class="pl-s1">input_names</span> <span class="pl-c1">=</span> [<span class="pl-s">"input"</span>]
<span class="pl-s1">output_names</span> <span class="pl-c1">=</span> [<span class="pl-s">"output"</span>]
<span class="pl-s1">dynamic_axes</span> <span class="pl-c1">=</span> {
    <span class="pl-s">"input"</span>: {<span class="pl-c1">0</span>: <span class="pl-s">"batch_size"</span>},  <span class="pl-c"># 输入的第一个维度是动态的</span>
    <span class="pl-s">"output"</span>: {<span class="pl-c1">0</span>: <span class="pl-s">"batch_size"</span>}  <span class="pl-c"># 输出的第一个维度是动态的</span>
}
<span class="pl-s1">torch</span>.<span class="pl-s1">onnx</span>.<span class="pl-en">export</span>(<span class="pl-s1">model</span>, <span class="pl-s1">torch</span>.<span class="pl-en">randn</span>(<span class="pl-c1">1</span>, <span class="pl-c1">3</span>, <span class="pl-c1">224</span>, <span class="pl-c1">224</span>).<span class="pl-en">to</span>(<span class="pl-s1">device</span>), <span class="pl-s">"model.onnx"</span>, <span class="pl-s1">input_names</span><span class="pl-c1">=</span><span class="pl-s1">input_names</span>, <span class="pl-s1">output_names</span><span class="pl-c1">=</span><span class="pl-s1">output_names</span>, <span class="pl-s1">dynamic_axes</span><span class="pl-c1">=</span><span class="pl-s1">dynamic_axes</span>, <span class="pl-s1">opset_version</span><span class="pl-c1">=</span><span class="pl-c1">11</span>)

<span class="pl-k">with</span> <span class="pl-s1">torch</span>.<span class="pl-en">no_grad</span>():
    <span class="pl-c"># image = torch.randn(1, 3, 256, 256)</span>
    <span class="pl-en">print</span>(<span class="pl-s1">torch</span>.<span class="pl-en">softmax</span>(<span class="pl-en">model</span>(<span class="pl-en">get_input_images</span>(<span class="pl-s">'1.jpg'</span>)), <span class="pl-s1">dim</span><span class="pl-c1">=</span><span class="pl-c1">1</span>))
    <span class="pl-en">input</span>()
    <span class="pl-en">print</span>(<span class="pl-s1">torch</span>.<span class="pl-en">softmax</span>(<span class="pl-en">model</span>(<span class="pl-en">get_input_images</span>(<span class="pl-s">'3.jpg'</span>)), <span class="pl-s1">dim</span><span class="pl-c1">=</span><span class="pl-c1">1</span>))
    <span class="pl-en">input</span>()
    <span class="pl-s1">acc</span> <span class="pl-c1">=</span> []
    <span class="pl-s1">f1</span> <span class="pl-c1">=</span> [[], []]
    
    
    <span class="pl-k">for</span> <span class="pl-s1">file</span> <span class="pl-c1">in</span> <span class="pl-en">glob</span>(<span class="pl-s">'./datasets/nailong/*'</span>):
        <span class="pl-s1">y_hat</span>, <span class="pl-s1">y</span> <span class="pl-c1">=</span> <span class="pl-en">model</span>(<span class="pl-en">get_input_images</span>(<span class="pl-s1">file</span>)), <span class="pl-s1">torch</span>.<span class="pl-en">tensor</span>([<span class="pl-c1">0</span>]).<span class="pl-en">to</span>(<span class="pl-s1">device</span>)
        <span class="pl-s1">acc</span>.<span class="pl-en">append</span>(<span class="pl-s1">torch</span>.<span class="pl-en">sum</span>(<span class="pl-s1">torch</span>.<span class="pl-en">argmax</span>(<span class="pl-s1">y_hat</span>, <span class="pl-s1">dim</span><span class="pl-c1">=</span><span class="pl-c1">1</span>) <span class="pl-c1">==</span> <span class="pl-s1">y</span>).<span class="pl-en">item</span>() <span class="pl-c1">/</span> <span class="pl-en">len</span>(<span class="pl-s1">y</span>))
        <span class="pl-s1">f1</span>[<span class="pl-c1">0</span>].<span class="pl-en">append</span>(<span class="pl-s1">y</span>.<span class="pl-en">cpu</span>().<span class="pl-en">tolist</span>()[<span class="pl-c1">0</span>])
        <span class="pl-s1">f1</span>[<span class="pl-c1">1</span>].<span class="pl-en">append</span>(<span class="pl-s1">torch</span>.<span class="pl-en">argmax</span>(<span class="pl-s1">y_hat</span>, <span class="pl-s1">dim</span><span class="pl-c1">=</span><span class="pl-c1">1</span>).<span class="pl-en">cpu</span>().<span class="pl-en">tolist</span>()[<span class="pl-c1">0</span>])
        

    <span class="pl-c"># for file in glob('./datasets/cifer10/*'):</span>
    <span class="pl-c">#     y_hat, y = model(get_input_images(file)), torch.tensor([3]).to(device)</span>
    <span class="pl-c">#     acc.append(torch.sum(torch.argmax(y_hat, dim=1) == y).item() / len(y))</span>
    <span class="pl-c">#     f1[0].append(y.cpu().tolist()[0])</span>
    <span class="pl-c">#     f1[1].append(torch.argmax(y_hat, dim=1).cpu().tolist()[0])</span>

<span class="pl-en">print</span>(<span class="pl-en">sum</span>(<span class="pl-s1">acc</span>) <span class="pl-c1">/</span> <span class="pl-en">len</span>(<span class="pl-s1">acc</span>))
<span class="pl-s1">f1_scores</span> <span class="pl-c1">=</span> <span class="pl-en">f1_score</span>(<span class="pl-s1">f1</span>[<span class="pl-c1">0</span>], <span class="pl-s1">f1</span>[<span class="pl-c1">1</span>], <span class="pl-s1">average</span><span class="pl-c1">=</span><span class="pl-s">'macro'</span>)
<span class="pl-en">print</span>(<span class="pl-s1">f1_scores</span>)</pre></div>
<p></p>
</details> 
<p>发现 acc 与 f1 的值非常低(百分之10到百分之20附近)</p>
<p>这个时候查看模型的输出, 发现模型的输出接近初始化的输出(softmax后), 模型没有怎么被训练</p>
<p>这个时候怀疑是训练代码出了问题, 然而 cifer10 的 训练并没有什么问题<br>
检查数据增广代码, 查看增广后的图片, 发现增广做的不是很好, 正样本内嵌负样本没嵌好</p>
<p>在经过修改后, 重启训练<br>
然而, 问题变成了, 模型的输出接近(0.5, 0.5)(二分类任务)</p>
<p>跟人讨论后, 认为是数据集难度太大, 检查表情包数据集, 都是一些分布与 nailong 数据差异很大的图片.</p>
<blockquote>
<p>非严格推理, 纯脑测<br>
模型发现, 给一张新的图片预测 nailong 类, 还是其他类, 都会导致loss上升, 于是干脆摆烂乱猜,  最终的概率分布会输出数据分布, 经过数据增广后的数据恰好是两类 1:1, 模型退化成统计数据集了</p>
</blockquote>
<p>导致这个的最直接原因是输入特征不够, 到图像分类就是模型找不到决定图片分类的模式</p>
<p>于是, 第一阶段的训练结束了</p>
<h4>第二次训练</h4>
<p>在数据集作者不断的努力下, nailong 数据集有了一些完善, 主要的完善点在于:</p>
<ol>
<li>新添更多 nailong</li>
<li>不是二分类了, 新增了表情包分类, 动画分类等五分类, 不过不同类别的数据数量差异很大(两个数量级)</li>
<li>加入了一些 corner case, 比如 藤田琴音等其他颜色为黄色的图像</li>
</ol>
<p>因为第一次训练代码已经写好了, 改起来也不是很麻烦, 只需要换个数据集定义与读取. 作者的数据集放在 huggingface 上, 于是我们使用 datasets 进行读取.</p>
<blockquote>
<p>我也不知道是不是我写的问题, datasets读起来很慢, dataloader 后, 会把 label 自动变成torch.tensor格式, 但是 n, c, h, w 格式的图片只会把 w 维度变成 torch.tensor 格式, 其他维度还是 List, 需要在 dataset 类定义的时候使用 <strong>getitem</strong>() 将数据提前变为 torch.tensor<br>
然后不支持多线程读取(会卡住), 单线程读取读起来很慢, gpu 的 cuda 呈现尖刺状<br>
然后, dataset 的读取<strong>要先</strong>读取 id 再读取 x 跟 label<br>
没怎么用过 dataset, 这次属实是学到了</p>
</blockquote>
<p>修改好后数据加载的代码后并注释掉先前的数据增广代码后(后续研究), 第二次训练开始了</p>
<p>这次结果好过头了<br>
模型的 loss 收敛到了 $1e^{-5}$, acc跟f1更是到达了 $100%$</p>
<p>使用测试代码简单测试, 发现在数据集的数据都能完美分类, 不在数据集的分类只要分不出是奶龙即可. 检查模型输出权重, 也没啥问题, 看起来是完美了?</p>
<p>然而 这张图还是给了模型一拳</p>
<details><summary>图</summary>
<p>
</p><p><a target="_blank" rel="noopener noreferrer" href="https://github.com/user-attachments/assets/3cb2b111-e01f-44dd-8e71-0509ab2bb6c0"><img src="https://github.com/user-attachments/assets/3cb2b111-e01f-44dd-8e71-0509ab2bb6c0" alt="22" style="max-width: 100%;"></a></p>
<p></p>
</details> 
<p>他会识别成 nailong, 不过我觉得问题不大(确实有人把他抽象的认成 nailong)</p>
<p>上面的 <code class="notranslate">test.py</code> 中 写了onnx导出的代码, 支持任意 batch 的输入(解锁了 n, c, h, w 的 n 维度)</p>
<p>简单编写onnx推理代码</p>
<details><summary>onnx_inference.py</summary>
<p>
</p><div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-c"># from torchvision import transforms</span>
<span class="pl-k">import</span> <span class="pl-s1">onnxruntime</span> <span class="pl-k">as</span> <span class="pl-s1">ort</span>
<span class="pl-k">from</span> <span class="pl-v">PIL</span> <span class="pl-k">import</span> <span class="pl-v">Image</span>
<span class="pl-k">import</span> <span class="pl-s1">numpy</span> <span class="pl-k">as</span> <span class="pl-s1">np</span>

<span class="pl-s1">img_size</span> <span class="pl-c1">=</span> <span class="pl-c1">224</span>

<span class="pl-c"># transform = transforms.Compose([</span>
<span class="pl-c">#     transforms.Resize((img_size, img_size)),</span>
<span class="pl-c">#     transforms.ToTensor(),</span>
<span class="pl-c">#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),</span>
<span class="pl-c"># ])</span>

<span class="pl-k">def</span> <span class="pl-en">transform_img</span>(<span class="pl-s1">img</span>: <span class="pl-v">Image</span>, <span class="pl-s1">image_size</span><span class="pl-c1">=</span><span class="pl-c1">224</span>, <span class="pl-s1">mean</span><span class="pl-c1">=</span>[<span class="pl-c1">0.485</span>, <span class="pl-c1">0.456</span>, <span class="pl-c1">0.406</span>], <span class="pl-s1">std</span><span class="pl-c1">=</span>[<span class="pl-c1">0.229</span>, <span class="pl-c1">0.224</span>, <span class="pl-c1">0.225</span>]):
    <span class="pl-s1">img</span> <span class="pl-c1">=</span> <span class="pl-s1">img</span>.<span class="pl-en">convert</span>(<span class="pl-s">"RGB"</span>).<span class="pl-en">resize</span>((<span class="pl-s1">image_size</span>, <span class="pl-s1">image_size</span>), <span class="pl-v">Image</span>.<span class="pl-v">Resampling</span>.<span class="pl-v">LANCZOS</span>)
    <span class="pl-s1">img</span> <span class="pl-c1">=</span> <span class="pl-s1">np</span>.<span class="pl-en">array</span>(<span class="pl-s1">img</span>)
    <span class="pl-s1">img</span> <span class="pl-c1">=</span> (<span class="pl-s1">img</span> <span class="pl-c1">/</span> <span class="pl-c1">255</span> <span class="pl-c1">-</span> <span class="pl-s1">mean</span>) <span class="pl-c1">/</span> <span class="pl-s1">std</span>
    <span class="pl-s1">img</span> <span class="pl-c1">=</span> <span class="pl-s1">img</span>.<span class="pl-en">transpose</span>((<span class="pl-c1">2</span>, <span class="pl-c1">0</span>, <span class="pl-c1">1</span>))
    <span class="pl-s1">img</span> <span class="pl-c1">=</span> <span class="pl-s1">np</span>.<span class="pl-en">expand_dims</span>(<span class="pl-s1">img</span>, <span class="pl-s1">axis</span><span class="pl-c1">=</span><span class="pl-c1">0</span>)
    <span class="pl-k">return</span> <span class="pl-s1">img</span>.<span class="pl-en">astype</span>(<span class="pl-s1">np</span>.<span class="pl-s1">float32</span>)


<span class="pl-s1">label_mapping</span> <span class="pl-c1">=</span> {
    <span class="pl-s">"nailong"</span>: <span class="pl-c1">0</span>,
    <span class="pl-s">"emoji"</span>: <span class="pl-c1">1</span>,
    <span class="pl-s">"anime"</span>: <span class="pl-c1">2</span>,
    <span class="pl-s">"others"</span>: <span class="pl-c1">3</span>,
    <span class="pl-s">"long"</span>: <span class="pl-c1">4</span>
}

<span class="pl-s1">reverse_label_mapping</span> <span class="pl-c1">=</span> {<span class="pl-s1">v</span>: <span class="pl-s1">k</span> <span class="pl-k">for</span> <span class="pl-s1">k</span>, <span class="pl-s1">v</span> <span class="pl-c1">in</span> <span class="pl-s1">label_mapping</span>.<span class="pl-en">items</span>()}

<span class="pl-s1">model_path</span> <span class="pl-c1">=</span> <span class="pl-s">'model.onnx'</span>
<span class="pl-s1">session</span> <span class="pl-c1">=</span> <span class="pl-s1">ort</span>.<span class="pl-v">InferenceSession</span>(<span class="pl-s1">model_path</span>)

<span class="pl-s1">image_path</span> <span class="pl-c1">=</span> <span class="pl-s">'3.jpg'</span>
<span class="pl-s1">image</span> <span class="pl-c1">=</span> <span class="pl-v">Image</span>.<span class="pl-en">open</span>(<span class="pl-s1">image_path</span>).<span class="pl-en">convert</span>(<span class="pl-s">"RGB"</span>)
<span class="pl-c"># image = transform(image).unsqueeze(0).numpy()</span>
<span class="pl-s1">image</span> <span class="pl-c1">=</span> <span class="pl-en">transform_img</span>(<span class="pl-s1">image</span>)

<span class="pl-c"># 运行推理</span>
<span class="pl-s1">input_name</span> <span class="pl-c1">=</span> <span class="pl-s1">session</span>.<span class="pl-en">get_inputs</span>()[<span class="pl-c1">0</span>].<span class="pl-s1">name</span>
<span class="pl-s1">output_name</span> <span class="pl-c1">=</span> <span class="pl-s1">session</span>.<span class="pl-en">get_outputs</span>()[<span class="pl-c1">0</span>].<span class="pl-s1">name</span>
<span class="pl-s1">outputs</span> <span class="pl-c1">=</span> <span class="pl-s1">session</span>.<span class="pl-en">run</span>([<span class="pl-s1">output_name</span>], {<span class="pl-s1">input_name</span>: <span class="pl-s1">image</span>})

<span class="pl-c"># 获取分类结果</span>
<span class="pl-s1">output</span> <span class="pl-c1">=</span> <span class="pl-s1">outputs</span>[<span class="pl-c1">0</span>]
<span class="pl-s1">predicted_class</span> <span class="pl-c1">=</span> <span class="pl-s1">np</span>.<span class="pl-en">argmax</span>(<span class="pl-s1">output</span>, <span class="pl-s1">axis</span><span class="pl-c1">=</span><span class="pl-c1">1</span>)
<span class="pl-s1">predicted_label</span> <span class="pl-c1">=</span> <span class="pl-s1">reverse_label_mapping</span>[<span class="pl-s1">predicted_class</span>[<span class="pl-c1">0</span>]]

<span class="pl-en">print</span>(<span class="pl-s">f"Predicted class: <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">predicted_label</span><span class="pl-kos">}</span></span>"</span>)</pre></div>
<p></p>
</details> 
<blockquote>
<p>训练的时候引入了 torchvision 的 transforms, 这里为了减少依赖, 选择手动实现, 有需要也可以自行取消注释并修改</p>
</blockquote>
<h3>部署</h3>
<p>TODO</p></div>
<div style="font-size:small;margin-top:8px;float:right;">转载无需注明出处</div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer"><div id="footer1">Copyright © <span id="copyrightYear"></span> <a href="https://FairyOwO.github.io">FairyOwO 的 Blog</a></div>
<div id="footer2">
    <span id="runday"></span><span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a></span>
</div>

<script>
var now=new Date();
document.getElementById("copyrightYear").innerHTML=now.getFullYear();

if("11/22/2024"!=""){
    var startSite=new Date("11/22/2024");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";
}
</script></div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z', 'copy': 'M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z', 'check': 'M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);



function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","FairyOwO/FairyOwO.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}

document.addEventListener('DOMContentLoaded', () => {
    const createClipboardHTML = (codeContent, additionalClasses = '') => `
        <pre class="notranslate"><code class="notranslate">${codeContent}</code></pre>
        <div class="clipboard-container position-absolute right-0 top-0 ${additionalClasses}">
            <clipboard-copy class="ClipboardButton btn m-2 p-0" role="button" style="display: inherit;">
                <svg height="16" width="16" class="octicon octicon-copy m-2"><path d="${IconList["copy"]}"></path></svg>
                <svg height="16" width="16" class="octicon octicon-check color-fg-success m-2 d-none"><path d="${IconList["check"]}"></path></svg>
            </clipboard-copy>
            <div class="copy-feedback">Copied!</div>
        </div>
    `;

    const handleCodeElements = (selector = '') => {
        document.querySelectorAll(selector).forEach(codeElement => {
            const codeContent = codeElement.innerHTML;
            const newStructure = document.createElement('div');
            newStructure.className = 'snippet-clipboard-content position-relative overflow-auto';
            newStructure.innerHTML = createClipboardHTML(codeContent);

            const parentElement = codeElement.parentElement;
            if (selector.includes('highlight')) {
                parentElement.insertBefore(newStructure, codeElement.nextSibling);
                parentElement.removeChild(codeElement);
            } else {
                parentElement.parentElement.replaceChild(newStructure, parentElement);
            }
        });
    };

    handleCodeElements('pre.notranslate > code.notranslate');
    handleCodeElements('div.highlight > pre.notranslate');

    let currentFeedback = null;
    document.querySelectorAll('clipboard-copy').forEach(copyButton => {
        copyButton.addEventListener('click', () => {
            const codeContent = copyButton.closest('.snippet-clipboard-content').innerText;
            const tempTextArea = document.createElement('textarea');
            tempTextArea.value = codeContent;
            document.body.appendChild(tempTextArea);
            tempTextArea.select();
            document.execCommand('copy');
            document.body.removeChild(tempTextArea);

            const copyIcon = copyButton.querySelector('.octicon-copy');
            const checkIcon = copyButton.querySelector('.octicon-check');
            const copyFeedback = copyButton.nextElementSibling;

            if (currentFeedback && currentFeedback !== copyFeedback) {currentFeedback.style.display = 'none';}
            currentFeedback = copyFeedback;

            copyIcon.classList.add('d-none');
            checkIcon.classList.remove('d-none');
            copyFeedback.style.display = 'block';
            copyButton.style.borderColor = 'var(--color-success-fg)';

            setTimeout(() => {
                copyIcon.classList.remove('d-none');
                checkIcon.classList.add('d-none');
                copyFeedback.style.display = 'none';
                copyButton.style.borderColor = '';
            }, 2000);
        });
    });
});

</script>
<script>MathJax = {tex: {inlineMath: [["$", "$"]]}};</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</html>
